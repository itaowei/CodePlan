<html><body><pre><hr><h1>server\models_server.py</h1><h2><span style="color:orangered;">Source</span> vs <span style="color:green;">Target</span></h2>index 7617896..2d7c2a3 100644<br><span style="color:blue;">@@ -43,7 +43,7 @@</span><br> warnings.filterwarnings("ignore")<br> <br> parser = argparse.ArgumentParser()<br><span style="color:orangered;">-parser.add_argument("--config", type=str, default="config.yaml")</span><br><span style="color:green;">+parser.add_argument("--config", type=str, default="configs/config.default.yaml")</span><br> args = parser.parse_args()<br> <br> logger = logging.getLogger(__name__)<br><h2><span style="color:orangered;">Source</span> vs <span style="color:green;">Predicted</span></h2>index 7617896..2d7c2a3 100644<br><span style="color:blue;">@@ -43,7 +43,7 @@</span><br> warnings.filterwarnings("ignore")<br> <br> parser = argparse.ArgumentParser()<br><span style="color:orangered;">-parser.add_argument("--config", type=str, default="config.yaml")</span><br><span style="color:green;">+parser.add_argument("--config", type=str, default="configs/config.default.yaml")</span><br> args = parser.parse_args()<br> <br> logger = logging.getLogger(__name__)<br><h2><span style="color:orangered;">Target</span> vs <span style="color:green;">Predicted</span></h2><br>No diff<br><hr><h1>server\get_token_ids.py</h1><h2><span style="color:orangered;">Source</span> vs <span style="color:green;">Target</span></h2><br>No diff<br><h2><span style="color:orangered;">Source</span> vs <span style="color:green;">Predicted</span></h2><br>No diff<br><h2><span style="color:orangered;">Target</span> vs <span style="color:green;">Predicted</span></h2><br>No diff<br><hr><h1>server\run_gradio_demo.py</h1><h2><span style="color:orangered;">Source</span> vs <span style="color:green;">Target</span></h2>index 404f67b..7dc40d3 100644<br><span style="color:blue;">@@ -75,7 +75,7 @@ def add_text(messages, message):</span><br> def bot(messages):<br>     if len(OPENAI_KEY) == 0 or not OPENAI_KEY.startswith("sk-"):<br>         return messages<br><span style="color:orangered;">-    message = chat_huggingface(all_messages, OPENAI_KEY, "openai")["message"]</span><br><span style="color:green;">+    message = chat_huggingface(all_messages, OPENAI_KEY, "openai", "https://api.openai.com/v1/completions")["message"]</span><br>     image_urls, audio_urls, video_urls = extract_medias(message)<br>     add_message(message, "assistant")<br>     messages[-1][1] = message<br><h2><span style="color:orangered;">Source</span> vs <span style="color:green;">Predicted</span></h2>index 404f67b..ba09a00 100644<br><span style="color:blue;">@@ -75,7 +75,7 @@ def add_text(messages, message):</span><br> def bot(messages):<br>     if len(OPENAI_KEY) == 0 or not OPENAI_KEY.startswith("sk-"):<br>         return messages<br><span style="color:orangered;">-    message = chat_huggingface(all_messages, OPENAI_KEY, "openai")["message"]</span><br><span style="color:green;">+    message = chat_huggingface(all_messages, OPENAI_KEY, "openai", API_ENDPOINT)["message"]</span><br>     image_urls, audio_urls, video_urls = extract_medias(message)<br>     add_message(message, "assistant")<br>     messages[-1][1] = message<br><h2><span style="color:orangered;">Target</span> vs <span style="color:green;">Predicted</span></h2>index 7dc40d3..ba09a00 100644<br><span style="color:blue;">@@ -75,7 +75,7 @@ def add_text(messages, message):</span><br> def bot(messages):<br>     if len(OPENAI_KEY) == 0 or not OPENAI_KEY.startswith("sk-"):<br>         return messages<br><span style="color:orangered;">-    message = chat_huggingface(all_messages, OPENAI_KEY, "openai", "https://api.openai.com/v1/completions")["message"]</span><br><span style="color:green;">+    message = chat_huggingface(all_messages, OPENAI_KEY, "openai", API_ENDPOINT)["message"]</span><br>     image_urls, audio_urls, video_urls = extract_medias(message)<br>     add_message(message, "assistant")<br>     messages[-1][1] = message<br><hr><h1>server\awesome_chat.py</h1><h2><span style="color:orangered;">Source</span> vs <span style="color:green;">Target</span></h2>index 695afce..9e94352 100644<br><span style="color:blue;">@@ -27,12 +27,12 @@</span><br> from huggingface_hub.inference_api import ALL_TASKS<br> <br> parser = argparse.ArgumentParser()<br><span style="color:orangered;">-parser.add_argument("--config", type=str, default="config.yaml")</span><br><span style="color:green;">+parser.add_argument("--config", type=str, default="configs/config.default.yaml")</span><br> parser.add_argument("--mode", type=str, default="cli")<br> args = parser.parse_args()<br> <br> if __name__ != "__main__":<br><span style="color:orangered;">-    args.config = "config.gradio.yaml"</span><br><span style="color:green;">+    args.config = "configs/config.gradio.yaml"</span><br>     args.mode = "gradio"<br> <br> config = yaml.load(open(args.config, "r"), Loader=yaml.FullLoader)<br><span style="color:blue;">@@ -88,25 +88,26 @@</span><br> elif "openai" in config:<br>     API_TYPE = "openai"<br> else:<br><span style="color:orangered;">-    logger.warning("No endpoint specified in config.yaml. The endpoint will be set dynamically according to the client.")</span><br><span style="color:green;">+    logger.warning(f"No endpoint specified in {args.config}. The endpoint will be set dynamically according to the client.")</span><br> <br> if args.mode in ["test", "cli"]:<br>     assert API_TYPE, "Only server mode supports dynamic endpoint."<br> <br> API_KEY = None<br><span style="color:green;">+API_ENDPOINT = None</span><br> if API_TYPE == "local":<br><span style="color:orangered;">-    endpoint = f"{config['local']['endpoint']}/v1/{api_name}"</span><br><span style="color:green;">+    API_ENDPOINT = f"{config['local']['endpoint']}/v1/{api_name}"</span><br> elif API_TYPE == "azure":<br><span style="color:orangered;">-    endpoint = f"{config['azure']['base_url']}/openai/deployments/{config['azure']['deployment_name']}/{api_name}?api-version={config['azure']['api_version']}"</span><br><span style="color:green;">+    API_ENDPOINT = f"{config['azure']['base_url']}/openai/deployments/{config['azure']['deployment_name']}/{api_name}?api-version={config['azure']['api_version']}"</span><br>     API_KEY = config["azure"]["api_key"]<br> elif API_TYPE == "openai":<br><span style="color:orangered;">-    endpoint = f"https://api.openai.com/v1/{api_name}"</span><br><span style="color:green;">+    API_ENDPOINT = f"https://api.openai.com/v1/{api_name}"</span><br>     if config["openai"]["api_key"].startswith("sk-"):  # Check for valid OpenAI key in config file<br>         API_KEY = config["openai"]["api_key"]<br>     elif "OPENAI_API_KEY" in os.environ and os.getenv("OPENAI_API_KEY").startswith("sk-"):  # Check for environment variable OPENAI_API_KEY<br>         API_KEY = os.getenv("OPENAI_API_KEY")<br>     else:<br><span style="color:orangered;">-        raise ValueError("Incrorrect OpenAI key. Please check your config.yaml file.")</span><br><span style="color:green;">+        raise ValueError(f"Incrorrect OpenAI key. Please check your {args.config} file.")</span><br> <br> PROXY = None<br> if config["proxy"]:<br><span style="color:blue;">@@ -120,7 +121,7 @@</span><br> Model_Server = None<br> if inference_mode!="huggingface":<br>     Model_Server = "http://" + config["local_inference_endpoint"]["host"] + ":" + str(config["local_inference_endpoint"]["port"])<br><span style="color:orangered;">-    message = "The server of local inference endpoints is not running, please start it first. (or using `inference_mode: huggingface` in config.yaml for a feature-limited experience)"</span><br><span style="color:green;">+    message = f"The server of local inference endpoints is not running, please start it first. (or using `inference_mode: huggingface` in {args.config} for a feature-limited experience)"</span><br>     try:<br>         r = requests.get(Model_Server + "/running")<br>         if r.status_code != 200:<br><span style="color:blue;">@@ -162,7 +163,7 @@</span><br>         "Authorization": f"Bearer {os.getenv('HUGGINGFACE_ACCESS_TOKEN')}",<br>     }<br> else:<br><span style="color:orangered;">-    raise ValueError("Incrorrect HuggingFace token. Please check your config.yaml file.")</span><br><span style="color:green;">+    raise ValueError(f"Incrorrect HuggingFace token. Please check your {args.config} file.")</span><br> <br> def convert_chat_to_completion(data):<br>     messages = data.pop('messages', [])<br><span style="color:blue;">@@ -188,6 +189,7 @@ def convert_chat_to_completion(data):</span><br> def send_request(data):<br>     api_key = data.pop("api_key")<br>     api_type = data.pop("api_type")<br><span style="color:green;">+    api_endpoint = data.pop("api_endpoint")</span><br>     if use_completion:<br>         data = convert_chat_to_completion(data)<br>     if api_type == "openai":<br><span style="color:blue;">@@ -201,7 +203,7 @@ def send_request(data):</span><br>         }<br>     else:<br>         HEADER = None<br><span style="color:orangered;">-    response = requests.post(endpoint, json=data, headers=HEADER, proxies=PROXY)</span><br><span style="color:green;">+    response = requests.post(api_endpoint, json=data, headers=HEADER, proxies=PROXY)</span><br>     if "error" in response.json():<br>         return response.json()<br>     logger.debug(response.text.strip())<br><span style="color:blue;">@@ -302,16 +304,17 @@ def unfold(tasks):</span><br>         <br>     return tasks<br> <br><span style="color:orangered;">-def chitchat(messages, api_key, api_type):</span><br><span style="color:green;">+def chitchat(messages, api_key, api_type, api_endpoint):</span><br>     data = {<br>         "model": LLM,<br>         "messages": messages,<br>         "api_key": api_key,<br><span style="color:orangered;">-        "api_type": api_type</span><br><span style="color:green;">+        "api_type": api_type,</span><br><span style="color:green;">+        "api_endpoint": api_endpoint</span><br>     }<br>     return send_request(data)<br> <br><span style="color:orangered;">-def parse_task(context, input, api_key, api_type):</span><br><span style="color:green;">+def parse_task(context, input, api_key, api_type, api_endpoint):</span><br>     demos_or_presteps = parse_task_demos_or_presteps<br>     messages = json.loads(demos_or_presteps)<br>     messages.insert(0, {"role": "system", "content": parse_task_tprompt})<br><span style="color:blue;">@@ -339,11 +342,12 @@ def parse_task(context, input, api_key, api_type):</span><br>         "temperature": 0,<br>         "logit_bias": {item: config["logit_bias"]["parse_task"] for item in task_parsing_highlight_ids},<br>         "api_key": api_key,<br><span style="color:orangered;">-        "api_type": api_type</span><br><span style="color:green;">+        "api_type": api_type,</span><br><span style="color:green;">+        "api_endpoint": api_endpoint</span><br>     }<br>     return send_request(data)<br> <br><span style="color:orangered;">-def choose_model(input, task, metas, api_key, api_type):</span><br><span style="color:green;">+def choose_model(input, task, metas, api_key, api_type, api_endpoint):</span><br>     prompt = replace_slot(choose_model_prompt, {<br>         "input": input,<br>         "task": task,<br><span style="color:blue;">@@ -364,12 +368,13 @@ def choose_model(input, task, metas, api_key, api_type):</span><br>         "temperature": 0,<br>         "logit_bias": {item: config["logit_bias"]["choose_model"] for item in choose_model_highlight_ids}, # 5<br>         "api_key": api_key,<br><span style="color:orangered;">-        "api_type": api_type</span><br><span style="color:green;">+        "api_type": api_type,</span><br><span style="color:green;">+        "api_endpoint": api_endpoint</span><br>     }<br>     return send_request(data)<br> <br> <br><span style="color:orangered;">-def response_results(input, results, api_key, api_type):</span><br><span style="color:green;">+def response_results(input, results, api_key, api_type, api_endpoint):</span><br>     results = [v for k, v in sorted(results.items(), key=lambda item: item[0])]<br>     prompt = replace_slot(response_results_prompt, {<br>         "input": input,<br><span style="color:blue;">@@ -387,7 +392,8 @@ def response_results(input, results, api_key, api_type):</span><br>         "messages": messages,<br>         "temperature": 0,<br>         "api_key": api_key,<br><span style="color:orangered;">-        "api_type": api_type</span><br><span style="color:green;">+        "api_type": api_type,</span><br><span style="color:green;">+        "api_endpoint": api_endpoint</span><br>     }<br>     return send_request(data)<br> <br><span style="color:blue;">@@ -711,7 +717,7 @@ def collect_result(command, choose, inference_result):</span><br>     return result<br> <br> <br><span style="color:orangered;">-def run_task(input, command, results, api_key, api_type):</span><br><span style="color:green;">+def run_task(input, command, results, api_key, api_type, api_endpoint):</span><br>     id = command["id"]<br>     args = command["args"]<br>     task = command["task"]<br><span style="color:blue;">@@ -814,7 +820,7 @@ def run_task(input, command, results, api_key, api_type):</span><br>             "role": "user",<br>             "content": f"[ {input} ] contains a task in JSON format {command}. Now you are a {command['task']} system, the arguments are {command['args']}. Just help me do {command['task']} and give me the result. The result must be in text form without any urls."<br>         }]<br><span style="color:orangered;">-        response = chitchat(messages, api_key, api_type)</span><br><span style="color:green;">+        response = chitchat(messages, api_key, api_type, api_endpoint)</span><br>         results[id] = collect_result(command, choose, {"response": response})<br>         return True<br>     else:<br><span style="color:blue;">@@ -859,7 +865,7 @@ def run_task(input, command, results, api_key, api_type):</span><br>                 if model["id"] in all_avaliable_model_ids<br>             ]<br> <br><span style="color:orangered;">-            choose_str = choose_model(input, command, cand_models_info, api_key, api_type)</span><br><span style="color:green;">+            choose_str = choose_model(input, command, cand_models_info, api_key, api_type, api_endpoint)</span><br>             logger.debug(f"chosen model: {choose_str}")<br>             try:<br>                 choose = json.loads(choose_str)<br><span style="color:blue;">@@ -882,14 +888,14 @@ def run_task(input, command, results, api_key, api_type):</span><br>     results[id] = collect_result(command, choose, inference_result)<br>     return True<br> <br><span style="color:orangered;">-def chat_huggingface(messages, api_key, api_type, return_planning = False, return_results = False):</span><br><span style="color:green;">+def chat_huggingface(messages, api_key, api_type, api_endpoint, return_planning = False, return_results = False):</span><br>     start = time.time()<br>     context = messages[:-1]<br>     input = messages[-1]["content"]<br>     logger.info("*"*80)<br>     logger.info(f"input: {input}")<br> <br><span style="color:orangered;">-    task_str = parse_task(context, input, api_key, api_type)</span><br><span style="color:green;">+    task_str = parse_task(context, input, api_key, api_type, api_endpoint)</span><br> <br>     if "error" in task_str:<br>         record_case(success=False, **{"input": input, "task": task_str, "reason": f"task parsing error: {task_str['error']['message']}", "op":"report message"})<br><span style="color:blue;">@@ -902,18 +908,18 @@ def chat_huggingface(messages, api_key, api_type, return_planning = False, retur</span><br>         tasks = json.loads(task_str)<br>     except Exception as e:<br>         logger.debug(e)<br><span style="color:orangered;">-        response = chitchat(messages, api_key, api_type)</span><br><span style="color:green;">+        response = chitchat(messages, api_key, api_type, api_endpoint)</span><br>         record_case(success=False, **{"input": input, "task": task_str, "reason": "task parsing fail", "op":"chitchat"})<br>         return {"message": response}<br>     <br>     if task_str == "[]":  # using LLM response for empty task<br>         record_case(success=False, **{"input": input, "task": [], "reason": "task parsing fail: empty", "op": "chitchat"})<br><span style="color:orangered;">-        response = chitchat(messages, api_key, api_type)</span><br><span style="color:green;">+        response = chitchat(messages, api_key, api_type, api_endpoint)</span><br>         return {"message": response}<br> <br>     if len(tasks) == 1 and tasks[0]["task"] in ["summarization", "translation", "conversational", "text-generation", "text2text-generation"]:<br>         record_case(success=True, **{"input": input, "task": tasks, "reason": "chitchat tasks", "op": "chitchat"})<br><span style="color:orangered;">-        response = chitchat(messages, api_key, api_type)</span><br><span style="color:green;">+        response = chitchat(messages, api_key, api_type, api_endpoint)</span><br>         return {"message": response}<br> <br>     tasks = unfold(tasks)<br><span style="color:blue;">@@ -939,7 +945,7 @@ def chat_huggingface(messages, api_key, api_type, return_planning = False, retur</span><br>             dep = task["dep"]<br>             if dep[0] == -1 or len(list(set(dep).intersection(d.keys()))) == len(dep):<br>                 tasks.remove(task)<br><span style="color:orangered;">-                thread = threading.Thread(target=run_task, args=(input, task, d, api_key, api_type))</span><br><span style="color:green;">+                thread = threading.Thread(target=run_task, args=(input, task, d, api_key, api_type, api_endpoint))</span><br>                 thread.start()<br>                 threads.append(thread)<br>         if num_thread == len(threads):<br><span style="color:blue;">@@ -959,7 +965,7 @@ def chat_huggingface(messages, api_key, api_type, return_planning = False, retur</span><br>     if return_results:<br>         return results<br>     <br><span style="color:orangered;">-    response = response_results(input, results, api_key, api_type).strip()</span><br><span style="color:green;">+    response = response_results(input, results, api_key, api_type, api_endpoint).strip()</span><br> <br>     end = time.time()<br>     during = end - start<br><span style="color:blue;">@@ -982,7 +988,7 @@ def test():</span><br>         <br>     for input in inputs:<br>         messages = [{"role": "user", "content": input}]<br><span style="color:orangered;">-        chat_huggingface(messages, API_KEY, API_TYPE, return_planning = False, return_results = False)</span><br><span style="color:green;">+        chat_huggingface(messages, API_KEY, API_TYPE, API_ENDPOINT, return_planning = False, return_results = False)</span><br>     <br>     # multi rounds example<br>     messages = [<br><span style="color:blue;">@@ -990,7 +996,7 @@ def test():</span><br>         {"role": "assistant", "content": """Sure. I understand your request. Based on the inference results of the models, I have generated a canny image for you. The workflow I used is as follows: First, I used the image-to-text model (nlpconnect/vit-gpt2-image-captioning) to convert the image /examples/f.jpg to text. The generated text is "a herd of giraffes and zebras grazing in a field". Second, I used the canny-control model (canny-control) to generate a canny image from the text. Unfortunately, the model failed to generate the canny image. Finally, I used the canny-text-to-image model (lllyasviel/sd-controlnet-canny) to generate a canny image from the text. The generated image is located at /images/f16d.png. I hope this answers your request. Is there anything else I can help you with?"""},<br>         {"role": "user", "content": """then based on the above canny image and a prompt "a photo of a zoo", generate a new image."""},<br>     ]<br><span style="color:orangered;">-    chat_huggingface(messages, API_KEY, API_TYPE, return_planning = False, return_results = False)</span><br><span style="color:green;">+    chat_huggingface(messages, API_KEY, API_TYPE, API_ENDPOINT, return_planning = False, return_results = False)</span><br> <br> def cli():<br>     messages = []<br><span style="color:blue;">@@ -1000,7 +1006,7 @@ def cli():</span><br>         if message == "exit":<br>             break<br>         messages.append({"role": "user", "content": message})<br><span style="color:orangered;">-        answer = chat_huggingface(messages, API_KEY, API_TYPE, return_planning=False, return_results=False)</span><br><span style="color:green;">+        answer = chat_huggingface(messages, API_KEY, API_TYPE, API_ENDPOINT, return_planning=False, return_results=False)</span><br>         print("[ Jarvis ]: ", answer["message"])<br>         messages.append({"role": "assistant", "content": answer["message"]})<br> <br><span style="color:blue;">@@ -1020,10 +1026,11 @@ def tasks():</span><br>         data = request.get_json()<br>         messages = data["messages"]<br>         api_key = data.get("api_key", API_KEY)<br><span style="color:orangered;">-        api_type = data.get("endpoint", API_TYPE)</span><br><span style="color:orangered;">-        if api_key is None and api_type is None:</span><br><span style="color:orangered;">-            return jsonify({"error": "Please provide api_key and api_type"}) </span><br><span style="color:orangered;">-        response = chat_huggingface(messages, api_key, api_type, return_planning=True)</span><br><span style="color:green;">+        api_endpoint = data.get("api_endpoint", API_ENDPOINT)</span><br><span style="color:green;">+        api_type = data.get("api_type", API_TYPE)</span><br><span style="color:green;">+        if api_key is None or api_type is None or api_endpoint is None:</span><br><span style="color:green;">+            return jsonify({"error": "Please provide api_key, api_type and api_endpoint"}) </span><br><span style="color:green;">+        response = chat_huggingface(messages, api_key, api_type, api_endpoint, return_planning=True)</span><br>         return jsonify(response)<br> <br>     @cross_origin()<br><span style="color:blue;">@@ -1032,10 +1039,11 @@ def results():</span><br>         data = request.get_json()<br>         messages = data["messages"]<br>         api_key = data.get("api_key", API_KEY)<br><span style="color:orangered;">-        api_type = data.get("endpoint", API_TYPE)</span><br><span style="color:orangered;">-        if api_key is None and api_type is None:</span><br><span style="color:orangered;">-            return jsonify({"error": "Please provide api_key and api_type"}) </span><br><span style="color:orangered;">-        response = chat_huggingface(messages, api_key, api_type, return_results=True)</span><br><span style="color:green;">+        api_endpoint = data.get("api_endpoint", API_ENDPOINT)</span><br><span style="color:green;">+        api_type = data.get("api_type", API_TYPE)</span><br><span style="color:green;">+        if api_key is None or api_type is None or api_endpoint is None:</span><br><span style="color:green;">+            return jsonify({"error": "Please provide api_key, api_type and api_endpoint"}) </span><br><span style="color:green;">+        response = chat_huggingface(messages, api_key, api_type, api_endpoint, return_results=True)</span><br>         return jsonify(response)<br> <br>     @cross_origin()<br><span style="color:blue;">@@ -1044,10 +1052,11 @@ def chat():</span><br>         data = request.get_json()<br>         messages = data["messages"]<br>         api_key = data.get("api_key", API_KEY)<br><span style="color:orangered;">-        api_type = data.get("endpoint", API_TYPE)</span><br><span style="color:orangered;">-        if api_key is None and api_type is None:</span><br><span style="color:orangered;">-            return jsonify({"error": "Please provide api_key and api_type"}) </span><br><span style="color:orangered;">-        response = chat_huggingface(messages, api_key, api_type)</span><br><span style="color:green;">+        api_endpoint = data.get("api_endpoint", API_ENDPOINT)</span><br><span style="color:green;">+        api_type = data.get("api_type", API_TYPE)</span><br><span style="color:green;">+        if api_key is None or api_type is None or api_endpoint is None:</span><br><span style="color:green;">+            return jsonify({"error": "Please provide api_key, api_type and api_endpoint"}) </span><br><span style="color:green;">+        response = chat_huggingface(messages, api_key, api_type, api_endpoint)</span><br>         return jsonify(response)<br>     print("server running...")<br>     waitress.serve(app, host=host, port=port)<br><h2><span style="color:orangered;">Source</span> vs <span style="color:green;">Predicted</span></h2>index 695afce..9758939 100644<br><span style="color:blue;">@@ -27,12 +27,12 @@</span><br> from huggingface_hub.inference_api import ALL_TASKS<br> <br> parser = argparse.ArgumentParser()<br><span style="color:orangered;">-parser.add_argument("--config", type=str, default="config.yaml")</span><br><span style="color:green;">+parser.add_argument("--config", type=str, default="configs/config.default.yaml")</span><br> parser.add_argument("--mode", type=str, default="cli")<br> args = parser.parse_args()<br> <br> if __name__ != "__main__":<br><span style="color:orangered;">-    args.config = "config.gradio.yaml"</span><br><span style="color:green;">+    args.config = "configs/config.gradio.yaml"</span><br>     args.mode = "gradio"<br> <br> config = yaml.load(open(args.config, "r"), Loader=yaml.FullLoader)<br><span style="color:blue;">@@ -88,25 +88,26 @@</span><br> elif "openai" in config:<br>     API_TYPE = "openai"<br> else:<br><span style="color:orangered;">-    logger.warning("No endpoint specified in config.yaml. The endpoint will be set dynamically according to the client.")</span><br><span style="color:green;">+    logger.warning(f"No endpoint specified in {args.config}. The endpoint will be set dynamically according to the client.")</span><br> <br> if args.mode in ["test", "cli"]:<br>     assert API_TYPE, "Only server mode supports dynamic endpoint."<br> <br> API_KEY = None<br><span style="color:green;">+API_ENDPOINT = None</span><br> if API_TYPE == "local":<br><span style="color:orangered;">-    endpoint = f"{config['local']['endpoint']}/v1/{api_name}"</span><br><span style="color:green;">+    API_ENDPOINT = f"{config['local']['endpoint']}/v1/{api_name}"</span><br> elif API_TYPE == "azure":<br><span style="color:orangered;">-    endpoint = f"{config['azure']['base_url']}/openai/deployments/{config['azure']['deployment_name']}/{api_name}?api-version={config['azure']['api_version']}"</span><br><span style="color:green;">+    API_ENDPOINT = f"{config['azure']['base_url']}/openai/deployments/{config['azure']['deployment_name']}/{api_name}?api-version={config['azure']['api_version']}"</span><br>     API_KEY = config["azure"]["api_key"]<br> elif API_TYPE == "openai":<br><span style="color:orangered;">-    endpoint = f"https://api.openai.com/v1/{api_name}"</span><br><span style="color:green;">+    API_ENDPOINT = f"https://api.openai.com/v1/{api_name}"</span><br>     if config["openai"]["api_key"].startswith("sk-"):  # Check for valid OpenAI key in config file<br>         API_KEY = config["openai"]["api_key"]<br>     elif "OPENAI_API_KEY" in os.environ and os.getenv("OPENAI_API_KEY").startswith("sk-"):  # Check for environment variable OPENAI_API_KEY<br>         API_KEY = os.getenv("OPENAI_API_KEY")<br>     else:<br><span style="color:orangered;">-        raise ValueError("Incrorrect OpenAI key. Please check your config.yaml file.")</span><br><span style="color:green;">+        raise ValueError(f"Incrorrect OpenAI key. Please check your {args.config} file.")</span><br> <br> PROXY = None<br> if config["proxy"]:<br><span style="color:blue;">@@ -120,7 +121,7 @@</span><br> Model_Server = None<br> if inference_mode!="huggingface":<br>     Model_Server = "http://" + config["local_inference_endpoint"]["host"] + ":" + str(config["local_inference_endpoint"]["port"])<br><span style="color:orangered;">-    message = "The server of local inference endpoints is not running, please start it first. (or using `inference_mode: huggingface` in config.yaml for a feature-limited experience)"</span><br><span style="color:green;">+    message = f"The server of local inference endpoints is not running, please start it first. (or using `inference_mode: huggingface` in {args.config} for a feature-limited experience)"</span><br>     try:<br>         r = requests.get(Model_Server + "/running")<br>         if r.status_code != 200:<br><span style="color:blue;">@@ -162,7 +163,7 @@</span><br>         "Authorization": f"Bearer {os.getenv('HUGGINGFACE_ACCESS_TOKEN')}",<br>     }<br> else:<br><span style="color:orangered;">-    raise ValueError("Incrorrect HuggingFace token. Please check your config.yaml file.")</span><br><span style="color:green;">+    raise ValueError(f"Incrorrect HuggingFace token. Please check your {args.config} file.")</span><br> <br> def convert_chat_to_completion(data):<br>     messages = data.pop('messages', [])<br><span style="color:blue;">@@ -188,6 +189,7 @@ def convert_chat_to_completion(data):</span><br> def send_request(data):<br>     api_key = data.pop("api_key")<br>     api_type = data.pop("api_type")<br><span style="color:green;">+    api_endpoint = data.pop("api_endpoint")</span><br>     if use_completion:<br>         data = convert_chat_to_completion(data)<br>     if api_type == "openai":<br><span style="color:blue;">@@ -201,7 +203,7 @@ def send_request(data):</span><br>         }<br>     else:<br>         HEADER = None<br><span style="color:orangered;">-    response = requests.post(endpoint, json=data, headers=HEADER, proxies=PROXY)</span><br><span style="color:green;">+    response = requests.post(api_endpoint, json=data, headers=HEADER, proxies=PROXY)</span><br>     if "error" in response.json():<br>         return response.json()<br>     logger.debug(response.text.strip())<br><span style="color:blue;">@@ -302,16 +304,17 @@ def unfold(tasks):</span><br>         <br>     return tasks<br> <br><span style="color:orangered;">-def chitchat(messages, api_key, api_type):</span><br><span style="color:green;">+def chitchat(messages, api_key, api_type, api_endpoint):</span><br>     data = {<br>         "model": LLM,<br>         "messages": messages,<br>         "api_key": api_key,<br><span style="color:orangered;">-        "api_type": api_type</span><br><span style="color:green;">+        "api_type": api_type,</span><br><span style="color:green;">+        "api_endpoint": api_endpoint</span><br>     }<br>     return send_request(data)<br> <br><span style="color:orangered;">-def parse_task(context, input, api_key, api_type):</span><br><span style="color:green;">+def parse_task(context, input, api_key, api_type, api_endpoint):</span><br>     demos_or_presteps = parse_task_demos_or_presteps<br>     messages = json.loads(demos_or_presteps)<br>     messages.insert(0, {"role": "system", "content": parse_task_tprompt})<br><span style="color:blue;">@@ -339,11 +342,12 @@ def parse_task(context, input, api_key, api_type):</span><br>         "temperature": 0,<br>         "logit_bias": {item: config["logit_bias"]["parse_task"] for item in task_parsing_highlight_ids},<br>         "api_key": api_key,<br><span style="color:orangered;">-        "api_type": api_type</span><br><span style="color:green;">+        "api_type": api_type,</span><br><span style="color:green;">+        "api_endpoint": api_endpoint</span><br>     }<br>     return send_request(data)<br> <br><span style="color:orangered;">-def choose_model(input, task, metas, api_key, api_type):</span><br><span style="color:green;">+def choose_model(input, task, metas, api_key, api_type, api_endpoint):</span><br>     prompt = replace_slot(choose_model_prompt, {<br>         "input": input,<br>         "task": task,<br><span style="color:blue;">@@ -364,12 +368,13 @@ def choose_model(input, task, metas, api_key, api_type):</span><br>         "temperature": 0,<br>         "logit_bias": {item: config["logit_bias"]["choose_model"] for item in choose_model_highlight_ids}, # 5<br>         "api_key": api_key,<br><span style="color:orangered;">-        "api_type": api_type</span><br><span style="color:green;">+        "api_type": api_type,</span><br><span style="color:green;">+        "api_endpoint": api_endpoint</span><br>     }<br>     return send_request(data)<br> <br> <br><span style="color:orangered;">-def response_results(input, results, api_key, api_type):</span><br><span style="color:green;">+def response_results(input, results, api_key, api_type, api_endpoint):</span><br>     results = [v for k, v in sorted(results.items(), key=lambda item: item[0])]<br>     prompt = replace_slot(response_results_prompt, {<br>         "input": input,<br><span style="color:blue;">@@ -387,7 +392,8 @@ def response_results(input, results, api_key, api_type):</span><br>         "messages": messages,<br>         "temperature": 0,<br>         "api_key": api_key,<br><span style="color:orangered;">-        "api_type": api_type</span><br><span style="color:green;">+        "api_type": api_type,</span><br><span style="color:green;">+        "api_endpoint": api_endpoint</span><br>     }<br>     return send_request(data)<br> <br><span style="color:blue;">@@ -711,7 +717,7 @@ def collect_result(command, choose, inference_result):</span><br>     return result<br> <br> <br><span style="color:orangered;">-def run_task(input, command, results, api_key, api_type):</span><br><span style="color:green;">+def run_task(input, command, results, api_key, api_type, api_endpoint):</span><br>     id = command["id"]<br>     args = command["args"]<br>     task = command["task"]<br><span style="color:blue;">@@ -814,7 +820,7 @@ def run_task(input, command, results, api_key, api_type):</span><br>             "role": "user",<br>             "content": f"[ {input} ] contains a task in JSON format {command}. Now you are a {command['task']} system, the arguments are {command['args']}. Just help me do {command['task']} and give me the result. The result must be in text form without any urls."<br>         }]<br><span style="color:orangered;">-        response = chitchat(messages, api_key, api_type)</span><br><span style="color:green;">+        response = chitchat(messages, api_key, api_type, api_endpoint)</span><br>         results[id] = collect_result(command, choose, {"response": response})<br>         return True<br>     else:<br><span style="color:blue;">@@ -859,7 +865,7 @@ def run_task(input, command, results, api_key, api_type):</span><br>                 if model["id"] in all_avaliable_model_ids<br>             ]<br> <br><span style="color:orangered;">-            choose_str = choose_model(input, command, cand_models_info, api_key, api_type)</span><br><span style="color:green;">+            choose_str = choose_model(input, command, cand_models_info, api_key, api_type, api_endpoint)</span><br>             logger.debug(f"chosen model: {choose_str}")<br>             try:<br>                 choose = json.loads(choose_str)<br><span style="color:blue;">@@ -882,14 +888,14 @@ def run_task(input, command, results, api_key, api_type):</span><br>     results[id] = collect_result(command, choose, inference_result)<br>     return True<br> <br><span style="color:orangered;">-def chat_huggingface(messages, api_key, api_type, return_planning = False, return_results = False):</span><br><span style="color:green;">+def chat_huggingface(messages, api_key, api_type, api_endpoint, return_planning = False, return_results = False):</span><br>     start = time.time()<br>     context = messages[:-1]<br>     input = messages[-1]["content"]<br>     logger.info("*"*80)<br>     logger.info(f"input: {input}")<br> <br><span style="color:orangered;">-    task_str = parse_task(context, input, api_key, api_type)</span><br><span style="color:green;">+    task_str = parse_task(context, input, api_key, api_type, api_endpoint)</span><br> <br>     if "error" in task_str:<br>         record_case(success=False, **{"input": input, "task": task_str, "reason": f"task parsing error: {task_str['error']['message']}", "op":"report message"})<br><span style="color:blue;">@@ -902,18 +908,18 @@ def chat_huggingface(messages, api_key, api_type, return_planning = False, retur</span><br>         tasks = json.loads(task_str)<br>     except Exception as e:<br>         logger.debug(e)<br><span style="color:orangered;">-        response = chitchat(messages, api_key, api_type)</span><br><span style="color:green;">+        response = chitchat(messages, api_key, api_type, api_endpoint)</span><br>         record_case(success=False, **{"input": input, "task": task_str, "reason": "task parsing fail", "op":"chitchat"})<br>         return {"message": response}<br>     <br>     if task_str == "[]":  # using LLM response for empty task<br>         record_case(success=False, **{"input": input, "task": [], "reason": "task parsing fail: empty", "op": "chitchat"})<br><span style="color:orangered;">-        response = chitchat(messages, api_key, api_type)</span><br><span style="color:green;">+        response = chitchat(messages, api_key, api_type, api_endpoint)</span><br>         return {"message": response}<br> <br>     if len(tasks) == 1 and tasks[0]["task"] in ["summarization", "translation", "conversational", "text-generation", "text2text-generation"]:<br>         record_case(success=True, **{"input": input, "task": tasks, "reason": "chitchat tasks", "op": "chitchat"})<br><span style="color:orangered;">-        response = chitchat(messages, api_key, api_type)</span><br><span style="color:green;">+        response = chitchat(messages, api_key, api_type, api_endpoint)</span><br>         return {"message": response}<br> <br>     tasks = unfold(tasks)<br><span style="color:blue;">@@ -939,7 +945,7 @@ def chat_huggingface(messages, api_key, api_type, return_planning = False, retur</span><br>             dep = task["dep"]<br>             if dep[0] == -1 or len(list(set(dep).intersection(d.keys()))) == len(dep):<br>                 tasks.remove(task)<br><span style="color:orangered;">-                thread = threading.Thread(target=run_task, args=(input, task, d, api_key, api_type))</span><br><span style="color:green;">+                thread = threading.Thread(target=run_task, args=(input, task, d, api_key, api_type, api_endpoint))</span><br>                 thread.start()<br>                 threads.append(thread)<br>         if num_thread == len(threads):<br><span style="color:blue;">@@ -959,7 +965,7 @@ def chat_huggingface(messages, api_key, api_type, return_planning = False, retur</span><br>     if return_results:<br>         return results<br>     <br><span style="color:orangered;">-    response = response_results(input, results, api_key, api_type).strip()</span><br><span style="color:green;">+    response = response_results(input, results, api_key, api_type, api_endpoint).strip()</span><br> <br>     end = time.time()<br>     during = end - start<br><span style="color:blue;">@@ -982,7 +988,7 @@ def test():</span><br>         <br>     for input in inputs:<br>         messages = [{"role": "user", "content": input}]<br><span style="color:orangered;">-        chat_huggingface(messages, API_KEY, API_TYPE, return_planning = False, return_results = False)</span><br><span style="color:green;">+        chat_huggingface(messages, API_KEY, API_TYPE, API_ENDPOINT, return_planning = False, return_results = False)</span><br>     <br>     # multi rounds example<br>     messages = [<br><span style="color:blue;">@@ -990,7 +996,7 @@ def test():</span><br>         {"role": "assistant", "content": """Sure. I understand your request. Based on the inference results of the models, I have generated a canny image for you. The workflow I used is as follows: First, I used the image-to-text model (nlpconnect/vit-gpt2-image-captioning) to convert the image /examples/f.jpg to text. The generated text is "a herd of giraffes and zebras grazing in a field". Second, I used the canny-control model (canny-control) to generate a canny image from the text. Unfortunately, the model failed to generate the canny image. Finally, I used the canny-text-to-image model (lllyasviel/sd-controlnet-canny) to generate a canny image from the text. The generated image is located at /images/f16d.png. I hope this answers your request. Is there anything else I can help you with?"""},<br>         {"role": "user", "content": """then based on the above canny image and a prompt "a photo of a zoo", generate a new image."""},<br>     ]<br><span style="color:orangered;">-    chat_huggingface(messages, API_KEY, API_TYPE, return_planning = False, return_results = False)</span><br><span style="color:green;">+    chat_huggingface(messages, API_KEY, API_TYPE, API_ENDPOINT, return_planning = False, return_results = False)</span><br> <br> def cli():<br>     messages = []<br><span style="color:blue;">@@ -1000,7 +1006,7 @@ def cli():</span><br>         if message == "exit":<br>             break<br>         messages.append({"role": "user", "content": message})<br><span style="color:orangered;">-        answer = chat_huggingface(messages, API_KEY, API_TYPE, return_planning=False, return_results=False)</span><br><span style="color:green;">+        answer = chat_huggingface(messages, API_KEY, API_TYPE, API_ENDPOINT, return_planning=False, return_results=False)</span><br>         print("[ Jarvis ]: ", answer["message"])<br>         messages.append({"role": "assistant", "content": answer["message"]})<br> <br><span style="color:blue;">@@ -1021,9 +1027,10 @@ def tasks():</span><br>         messages = data["messages"]<br>         api_key = data.get("api_key", API_KEY)<br>         api_type = data.get("endpoint", API_TYPE)<br><span style="color:green;">+        api_endpoint = data.get("api_endpoint", API_ENDPOINT)</span><br>         if api_key is None and api_type is None:<br>             return jsonify({"error": "Please provide api_key and api_type"}) <br><span style="color:orangered;">-        response = chat_huggingface(messages, api_key, api_type, return_planning=True)</span><br><span style="color:green;">+        response = chat_huggingface(messages, api_key, api_type, api_endpoint, return_planning=True)</span><br>         return jsonify(response)<br> <br>     @cross_origin()<br><span style="color:blue;">@@ -1033,9 +1040,10 @@ def results():</span><br>         messages = data["messages"]<br>         api_key = data.get("api_key", API_KEY)<br>         api_type = data.get("endpoint", API_TYPE)<br><span style="color:green;">+        api_endpoint = data.get("api_endpoint", API_ENDPOINT)</span><br>         if api_key is None and api_type is None:<br>             return jsonify({"error": "Please provide api_key and api_type"}) <br><span style="color:orangered;">-        response = chat_huggingface(messages, api_key, api_type, return_results=True)</span><br><span style="color:green;">+        response = chat_huggingface(messages, api_key, api_type, api_endpoint, return_results=True)</span><br>         return jsonify(response)<br> <br>     @cross_origin()<br><span style="color:blue;">@@ -1045,9 +1053,10 @@ def chat():</span><br>         messages = data["messages"]<br>         api_key = data.get("api_key", API_KEY)<br>         api_type = data.get("endpoint", API_TYPE)<br><span style="color:green;">+        api_endpoint = data.get("api_endpoint", API_ENDPOINT)</span><br>         if api_key is None and api_type is None:<br>             return jsonify({"error": "Please provide api_key and api_type"}) <br><span style="color:orangered;">-        response = chat_huggingface(messages, api_key, api_type)</span><br><span style="color:green;">+        response = chat_huggingface(messages, api_key, api_type, api_endpoint)</span><br>         return jsonify(response)<br>     print("server running...")<br>     waitress.serve(app, host=host, port=port)<br><h2><span style="color:orangered;">Target</span> vs <span style="color:green;">Predicted</span></h2>index 9e94352..9758939 100644<br><span style="color:blue;">@@ -1026,10 +1026,10 @@ def tasks():</span><br>         data = request.get_json()<br>         messages = data["messages"]<br>         api_key = data.get("api_key", API_KEY)<br><span style="color:green;">+        api_type = data.get("endpoint", API_TYPE)</span><br>         api_endpoint = data.get("api_endpoint", API_ENDPOINT)<br><span style="color:orangered;">-        api_type = data.get("api_type", API_TYPE)</span><br><span style="color:orangered;">-        if api_key is None or api_type is None or api_endpoint is None:</span><br><span style="color:orangered;">-            return jsonify({"error": "Please provide api_key, api_type and api_endpoint"}) </span><br><span style="color:green;">+        if api_key is None and api_type is None:</span><br><span style="color:green;">+            return jsonify({"error": "Please provide api_key and api_type"}) </span><br>         response = chat_huggingface(messages, api_key, api_type, api_endpoint, return_planning=True)<br>         return jsonify(response)<br> <br><span style="color:blue;">@@ -1039,10 +1039,10 @@ def results():</span><br>         data = request.get_json()<br>         messages = data["messages"]<br>         api_key = data.get("api_key", API_KEY)<br><span style="color:green;">+        api_type = data.get("endpoint", API_TYPE)</span><br>         api_endpoint = data.get("api_endpoint", API_ENDPOINT)<br><span style="color:orangered;">-        api_type = data.get("api_type", API_TYPE)</span><br><span style="color:orangered;">-        if api_key is None or api_type is None or api_endpoint is None:</span><br><span style="color:orangered;">-            return jsonify({"error": "Please provide api_key, api_type and api_endpoint"}) </span><br><span style="color:green;">+        if api_key is None and api_type is None:</span><br><span style="color:green;">+            return jsonify({"error": "Please provide api_key and api_type"}) </span><br>         response = chat_huggingface(messages, api_key, api_type, api_endpoint, return_results=True)<br>         return jsonify(response)<br> <br><span style="color:blue;">@@ -1052,10 +1052,10 @@ def chat():</span><br>         data = request.get_json()<br>         messages = data["messages"]<br>         api_key = data.get("api_key", API_KEY)<br><span style="color:green;">+        api_type = data.get("endpoint", API_TYPE)</span><br>         api_endpoint = data.get("api_endpoint", API_ENDPOINT)<br><span style="color:orangered;">-        api_type = data.get("api_type", API_TYPE)</span><br><span style="color:orangered;">-        if api_key is None or api_type is None or api_endpoint is None:</span><br><span style="color:orangered;">-            return jsonify({"error": "Please provide api_key, api_type and api_endpoint"}) </span><br><span style="color:green;">+        if api_key is None and api_type is None:</span><br><span style="color:green;">+            return jsonify({"error": "Please provide api_key and api_type"}) </span><br>         response = chat_huggingface(messages, api_key, api_type, api_endpoint)<br>         return jsonify(response)<br>     print("server running...")<br></pre></body></html>