facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, CompressionModel.decode_latent
def decode_latent(self, codes: torch.Tensor):
        """Decode from the discrete codes to continuous latent space."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingMultiheadAttention._load_from_state_dict
def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):
        if not self.custom:
            # Support compat with regular MHA
            keys = [n for n, _ in self.mha.named_parameters()]
            for key in keys:
                if prefix + key in state_dict:
                    state_dict[prefix + "mha." + key] = state_dict.pop(prefix + key)
        super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.build_model
def build_model(self):
        """Method to implement to initialize model."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\streaming.py
BlockTypes.METHOD, StreamingModule.streaming
def streaming(self):
        """Context manager to enter streaming mode. Reset streaming state on exit."""
        self._set_streaming(True)
        try:
            yield
        finally:
            self._set_streaming(False)
            self.reset_streaming()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, _do_predictions
def _do_predictions(texts, melodies, duration, progress=False, **gen_kwargs):
    MODEL.set_generation_params(duration=duration, **gen_kwargs)
    print("new batch", len(texts), texts, [None if m is None else (m[0], m[1].shape) for m in melodies])
    be = time.time()
    processed_melodies = []
    target_sr = 32000
    target_ac = 1
    for melody in melodies:
        if melody is None:
            processed_melodies.append(None)
        else:
            sr, melody = melody[0], torch.from_numpy(melody[1]).to(MODEL.device).float().t()
            if melody.dim() == 1:
                melody = melody[None]
            melody = melody[..., :int(sr * duration)]
            melody = convert_audio(melody, sr, target_sr, target_ac)
            processed_melodies.append(melody)

    if any(m is not None for m in processed_melodies):
        outputs = MODEL.generate_with_chroma(
            descriptions=texts,
            melody_wavs=processed_melodies,
            melody_sample_rate=target_sr,
            progress=progress,
            return_tokens=USE_DIFFUSION
        )
    else:
        outputs = MODEL.generate(texts, progress=progress, return_tokens=USE_DIFFUSION)
    if USE_DIFFUSION:
        outputs_diffusion = MBD.tokens_to_wav(outputs[1])
        outputs = torch.cat([outputs[0], outputs_diffusion], dim=0)
    outputs = outputs.detach().cpu().float()
    pending_videos = []
    out_wavs = []
    for output in outputs:
        with NamedTemporaryFile("wb", suffix=".wav", delete=False) as file:
            audio_write(
                file.name, output, MODEL.sample_rate, strategy="loudness",
                loudness_headroom_db=16, loudness_compressor=True, add_suffix=False)
            pending_videos.append(pool.submit(make_waveform, file.name))
            out_wavs.append(file.name)
            file_cleaner.add(file.name)
    out_videos = [pending_video.result() for pending_video in pending_videos]
    for video in out_videos:
        file_cleaner.add(video)
    print("batch finished", len(texts), time.time() - be)
    print("Tempfiles currently stored: ", len(file_cleaner.files))
    return out_videos, out_wavs
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, DummyQuantizer.__init__
def __init__(self):
        super().__init__()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\vq.py
BlockTypes.METHOD, ResidualVectorQuantizer.set_num_codebooks
def set_num_codebooks(self, n: int):
        assert n > 0 and n <= self.max_n_q
        self.n_q = n
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.evaluate
def evaluate(self):
        """Evaluate stage."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.initialize_ema
def initialize_ema(self):
        """Initialize exponential moving average with the registered sources.
        EMA object is created if the optim.ema.model.decay value is non-null.
        """
        from .builders import get_ema
        self.ema = get_ema(self._ema_sources, self.cfg.optim.ema)
        if self.ema is None:
            self.logger.info('No EMA on the model.')
        else:
            assert self.cfg.optim.ema.updates > 0
            self.logger.info(
                f'Initializing EMA on the model with decay = {self.ema.decay}'
                f' every {self.cfg.optim.ema.updates} updates'
            )
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\visqol.py
BlockTypes.METHOD, ViSQOL._collect_debug_data
def _collect_debug_data(self, debug_json_path: tp.Union[Path, str]) -> dict:
        # collect debug data for the visqol inference.
        with open(debug_json_path, "r") as f:
            data = json.load(f)
            return data
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen.sample_rate
def sample_rate(self) -> int:
        """Sample rate of the generated audio."""
        return self.compression_model.sample_rate
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.num_sequence_steps
def num_sequence_steps(self):
        return len(self.layout) - 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\losses\test_losses.py
BlockTypes.METHOD, test_mel_l1_loss
def test_mel_l1_loss():
    N, C, T = 2, 2, random.randrange(1000, 100_000)
    t1 = torch.randn(N, C, T)
    t2 = torch.randn(N, C, T)

    mel_l1 = MelSpectrogramL1Loss(sample_rate=22_050)
    loss = mel_l1(t1, t2)
    loss_same = mel_l1(t1, t1)

    assert isinstance(loss, torch.Tensor)
    assert isinstance(loss_same, torch.Tensor)
    assert loss_same.item() == 0.0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, MusicLMPattern.get_pattern
def get_pattern(self, timesteps: int) -> Pattern:
        out: PatternLayout = [[]]
        for offset in range(0, self.n_q, self.group_by):
            for t in range(timesteps):
                for q in range(offset, offset + self.group_by):
                    out.append([LayoutCoord(t, q)])
        return Pattern(out, n_q=self.n_q, timesteps=timesteps)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.generate
def generate(self):
        """Generate stage."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\specloss.py
BlockTypes.METHOD, MelSpectrogramWrapper.forward
def forward(self, x):
        p = int((self.n_fft - self.hop_length) // 2)
        if len(x.shape) == 2:
            x = x.unsqueeze(1)
        x = F.pad(x, (p, p), "reflect")
        # Make sure that all the frames are full.
        # The combination of `pad_for_conv1d` and the above padding
        # will make the output of size ceil(T / hop).
        x = pad_for_conv1d(x, self.n_fft, self.hop_length)
        self.mel_transform.to(x.device)
        mel_spec = self.mel_transform(x)
        B, C, freqs, frame = mel_spec.shape
        if self.log:
            mel_spec = torch.log10(self.floor_level + mel_spec)
        return mel_spec.reshape(B, C * freqs, frame)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, PasstKLDivergenceMetric.__init__
def __init__(self, pretrained_length: tp.Optional[float] = None):
        super().__init__()
        self._initialize_model(pretrained_length)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, BaseConditioner.__init__
def __init__(self, dim: int, output_dim: int):
        super().__init__()
        self.dim = dim
        self.output_dim = output_dim
        self.output_proj = nn.Linear(dim, output_dim)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_text_consistency
def get_text_consistency(cfg: omegaconf.DictConfig) -> metrics.TextConsistencyMetric:
    """Instantiate Text Consistency metric from config."""
    text_consistency_metrics = {
        'clap': metrics.CLAPTextConsistencyMetric
    }
    klass = text_consistency_metrics[cfg.model]
    kwargs = dict_from_config(cfg.get(cfg.model))
    return klass(**kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\balancer.py
BlockTypes.METHOD, Balancer.__init__
def __init__(self, weights: tp.Dict[str, float], balance_grads: bool = True, total_norm: float = 1.,
                 ema_decay: float = 0.999, per_batch_item: bool = True, epsilon: float = 1e-12,
                 monitor: bool = False):
        self.weights = weights
        self.per_batch_item = per_batch_item
        self.total_norm = total_norm or 1.
        self.averager = flashy.averager(ema_decay or 1.)
        self.epsilon = epsilon
        self.monitor = monitor
        self.balance_grads = balance_grads
        self._metrics: tp.Dict[str, tp.Any] = {}
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, _patch_passt_stft.__init__
def __init__(self):
        self.old_stft = torch.stft
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.run_one_stage
def run_one_stage(self, stage_name: str):
        """Run only the specified stage.
        This method is useful to only generate samples from a trained experiment
        or rerun the validation or evaluation stages.
        """
        fn = {
            'generate': with_rank_rng()(self.generate),
            'evaluate': self.evaluate,
            'valid': self.valid,
        }
        if stage_name not in fn:
            raise ValueError(f'Trying to run stage {stage_name} is not supported.')
        assert len(self.state_dict()) > 0
        self._start_epoch()
        with torch.no_grad(), self.swap_best_state():
            self.run_stage(stage_name, fn[stage_name])
        if not self.cfg.execute_inplace:
            self.commit(save_checkpoints=False)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\notebook.py
BlockTypes.METHOD, display_audio
def display_audio(samples: torch.Tensor, sample_rate: int):
    """Renders an audio player for the given audio samples.

    Args:
        samples (torch.Tensor): a Tensor of decoded audio samples
            with shapes [B, C, T] or [C, T]
        sample_rate (int): sample rate audio should be displayed with.
    """
    assert samples.dim() == 2 or samples.dim() == 3

    samples = samples.detach().cpu()
    if samples.dim() == 2:
        samples = samples[None, ...]

    for audio in samples:
        ipd.display(ipd.Audio(audio, rate=sample_rate))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingTransformer._apply_layer
def _apply_layer(self, layer, *args, **kwargs):
        method = self.checkpointing
        if method == 'none':
            return layer(*args, **kwargs)
        elif method == 'torch':
            return torch_checkpoint(layer, *args, use_reentrant=False, **kwargs)
        elif method.startswith('xformers'):
            from xformers.checkpoint_fairinternal import checkpoint, _get_default_policy
            if method == 'xformers_default':
                # those operations will be saved, and not recomputed.
                # According to Francisco we can get smarter policies but this is a good start.
                allow_list = [
                    "xformers.efficient_attention_forward_cutlass.default",
                    "xformers_flash.flash_fwd.default",
                    "aten.addmm.default",
                    "aten.mm.default",
                ]
            elif method == 'xformers_mm':
                # those operations will be saved, and not recomputed.
                # According to Francisco we can get smarter policies but this is a good start.
                allow_list = [
                    "aten.addmm.default",
                    "aten.mm.default",
                ]
            else:
                raise ValueError(f"xformers checkpointing xformers policy {method} is not known.")
            policy_fn = _get_default_policy(allow_list)
            return checkpoint(layer, *args, policy_fn=policy_fn, **kwargs)
        else:
            raise ValueError(f"Checkpointing method {method} is unknown.")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, get_loader
def get_loader(dataset, num_samples: tp.Optional[int], batch_size: int,
               num_workers: int, seed: int, **kwargs) -> torch.utils.data.DataLoader:
    """Convenience function to load dataset into a dataloader with optional subset sampling.

    Args:
        dataset: Dataset to load.
        num_samples (Optional[int]): Number of samples to limit subset size.
        batch_size (int): Batch size.
        num_workers (int): Number of workers for data loading.
        seed (int): Random seed.
    """
    if num_samples is not None:
        dataset = random_subset(dataset, num_samples, seed)

    dataloader = flashy.distrib.loader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        **kwargs
    )
    return dataloader
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.generate_unconditional
def generate_unconditional(self, num_samples: int, progress: bool = False,
                               return_tokens: bool = False) -> tp.Union[torch.Tensor,
                                                                        tp.Tuple[torch.Tensor, torch.Tensor]]:
        """Generate samples in an unconditional manner.

        Args:
            num_samples (int): Number of samples to be generated.
            progress (bool, optional): Flag to display progress of the generation process. Defaults to False.
        """
        descriptions: tp.List[tp.Optional[str]] = [None] * num_samples
        attributes, prompt_tokens = self._prepare_tokens_and_attributes(descriptions, None)
        tokens = self._generate_tokens(attributes, prompt_tokens, progress)
        if return_tokens:
            return self.generate_audio(tokens), tokens
        return self.generate_audio(tokens)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.max_delay
def max_delay(self):
        max_t_in_seq_coords = 0
        for seq_coords in self.layout[1:]:
            for coords in seq_coords:
                max_t_in_seq_coords = max(max_t_in_seq_coords, coords.t + 1)
        return max_t_in_seq_coords - self.timesteps
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.revert_pattern_logits
def revert_pattern_logits(self, logits: torch.Tensor, special_token: float, keep_only_valid_steps: bool = False):
        """Revert model logits obtained on a sequence built from the pattern
        back to a tensor matching the original sequence.

        This method is similar to ``revert_pattern_sequence`` with the following specificities:
        1. It is designed to work with the extra cardinality dimension
        2. We return the logits for the first sequence item that matches the special_token and
        which matching target in the original sequence is the first item of the sequence,
        while we skip the last logits as there is no matching target
        """
        B, card, K, S = logits.shape
        indexes, mask = self._build_reverted_sequence_scatter_indexes(
            S, K, keep_only_valid_steps, is_model_output=True, device=logits.device
        )
        logits = logits.reshape(B, card, -1)
        # we append the special token as the last index of our flattened z tensor
        logits = torch.cat([logits, torch.zeros_like(logits[:, :, :1]) + special_token], dim=-1)  # [B, card, K x S]
        values = logits[:, :, indexes.view(-1)]
        values = values.view(B, card, K, indexes.shape[-1])
        return values, indexes, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, DummyQuantizer.forward
def forward(self, x: torch.Tensor, frame_rate: int):
        q = x.unsqueeze(1)
        return QuantizedResult(x, q, torch.tensor(q.numel() * 32 * frame_rate / 1000 / len(x)).to(x))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditionFuser.forward
def forward(
        self,
        input: torch.Tensor,
        conditions: tp.Dict[str, ConditionType]
    ) -> tp.Tuple[torch.Tensor, tp.Optional[torch.Tensor]]:
        """Fuse the conditions to the provided model input.

        Args:
            input (torch.Tensor): Transformer input.
            conditions (dict[str, ConditionType]): Dict of conditions.
        Returns:
            tuple[torch.Tensor, torch.Tensor]: The first tensor is the transformer input
                after the conditions have been fused. The second output tensor is the tensor
                used for cross-attention or None if no cross attention inputs exist.
        """
        B, T, _ = input.shape

        if 'offsets' in self._streaming_state:
            first_step = False
            offsets = self._streaming_state['offsets']
        else:
            first_step = True
            offsets = torch.zeros(input.shape[0], dtype=torch.long, device=input.device)

        assert set(conditions.keys()).issubset(set(self.cond2fuse.keys())), \
            f"given conditions contain unknown attributes for fuser, " \
            f"expected {self.cond2fuse.keys()}, got {conditions.keys()}"
        cross_attention_output = None
        for cond_type, (cond, cond_mask) in conditions.items():
            op = self.cond2fuse[cond_type]
            if op == 'sum':
                input += cond
            elif op == 'input_interpolate':
                cond = einops.rearrange(cond, "b t d -> b d t")
                cond = F.interpolate(cond, size=input.shape[1])
                input += einops.rearrange(cond, "b d t -> b t d")
            elif op == 'prepend':
                if first_step:
                    input = torch.cat([cond, input], dim=1)
            elif op == 'cross':
                if cross_attention_output is not None:
                    cross_attention_output = torch.cat([cross_attention_output, cond], dim=1)
                else:
                    cross_attention_output = cond
            else:
                raise ValueError(f"unknown op ({op})")

        if self.cross_attention_pos_emb and cross_attention_output is not None:
            positions = torch.arange(
                cross_attention_output.shape[1],
                device=cross_attention_output.device
            ).view(1, -1, 1)
            pos_emb = create_sin_embedding(positions, cross_attention_output.shape[-1])
            cross_attention_output = cross_attention_output + self.cross_attention_pos_emb_scale * pos_emb

        if self._is_streaming:
            self._streaming_state['offsets'] = offsets + T

        return input, cross_attention_output
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.expire_codes_
def expire_codes_(self, batch_samples):
        if self.threshold_ema_dead_code == 0:
            return

        expired_codes = self.cluster_size < self.threshold_ema_dead_code
        if not torch.any(expired_codes):
            return

        batch_samples = rearrange(batch_samples, "... d -> (...) d")
        self.replace_(batch_samples, mask=expired_codes)
        flashy.distrib.broadcast_tensors(self.buffers())
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.load_from_pretrained
def load_from_pretrained(self, name: str):
        # TODO: support native HF versions of MusicGen.
        lm_pkg = models.loaders.load_lm_model_ckpt(name)
        state: dict = {
            'best_state': {
                'model': lm_pkg['best_state'],
            },
        }
        return state
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\loaders.py
BlockTypes.METHOD, load_lm_model
def load_lm_model(file_or_url_or_id: tp.Union[Path, str], device='cpu', cache_dir: tp.Optional[str] = None):
    pkg = load_lm_model_ckpt(file_or_url_or_id, cache_dir=cache_dir)
    cfg = OmegaConf.create(pkg['xp.cfg'])
    cfg.device = str(device)
    if cfg.device == 'cpu':
        cfg.dtype = 'float32'
    else:
        cfg.dtype = 'float16'
    _delete_param(cfg, 'conditioners.self_wav.chroma_stem.cache_path')
    _delete_param(cfg, 'conditioners.args.merge_text_conditions_p')
    _delete_param(cfg, 'conditioners.args.drop_desc_p')
    model = builders.get_lm_model(cfg)
    model.load_state_dict(pkg['best_state'])
    model.eval()
    model.cfg = cfg
    return model
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, SoundDataset.collater
def collater(self, samples):
        # when training, audio mixing is performed in the collate function
        wav, sound_info = super().collater(samples)  # SoundDataset always returns infos
        if self.aug_p > 0:
            wav, sound_info = mix_samples(wav, sound_info, self.aug_p, self.mix_p,
                                          snr_low=self.mix_snr_low, snr_high=self.mix_snr_high,
                                          min_overlap=self.mix_min_overlap)
        return wav, sound_info
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestGeneratorAdversarialLoss.test_mse_generator_adv_loss
def test_mse_generator_adv_loss(self):
        adv_loss = get_adv_criterion(loss_type='mse')

        t0 = torch.randn(1, 2, 0)
        t1 = torch.FloatTensor([1.0, 1.0, 1.0])
        t2 = torch.FloatTensor([2.0, 5.0, 5.0])

        assert adv_loss(t0).item() == 0.0
        assert adv_loss(t1).item() == 0.0
        assert adv_loss(t2).item() == 11.0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_rope.py
BlockTypes.METHOD, test_transformer_with_rope
def test_transformer_with_rope():
    set_efficient_attention_backend('xformers')
    torch.manual_seed(1234)
    for pos in ['rope', 'sin_rope']:
        tr = StreamingTransformer(
            16, 4, 2, custom=True, dropout=0., layer_scale=0.1,
            positional_embedding=pos)
        tr.eval()
        steps = 12
        x = torch.randn(3, steps, 16)

        out = tr(x)
        assert list(out.shape) == list(x.shape)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\cosine_lr_scheduler.py
BlockTypes.METHOD, CosineLRScheduler._get_sched_lr
def _get_sched_lr(self, lr: float, step: int):
        if step < self.warmup_steps:
            lr_ratio = step / self.warmup_steps
            lr = lr_ratio * lr
        elif step <= self.total_steps:
            s = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            lr_ratio = self.lr_min_ratio + 0.5 * (1 - self.lr_min_ratio) * \
                (1. + math.cos(math.pi * s / self.cycle_length))
            lr = lr_ratio * lr
        else:
            lr_ratio = self.lr_min_ratio
            lr = lr_ratio * lr
        return lr
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DataProcess.process_data
def process_data(self, x, metric=False):
        if x is None:
            return None
        if self.boost:
            x /= torch.clamp(x.std(dim=(1, 2), keepdim=True), min=1e-4)
            x * 0.22
        if self.use_filter and not metric:
            x = self.filter(x)[self.idx_band]
        if self.use_resampling:
            x = julius.resample_frac(x, old_sr=self.initial_sr, new_sr=self.target_sr)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\rope.py
BlockTypes.METHOD, RotaryEmbedding.rotate
def rotate(self, x: torch.Tensor, start: int = 0, invert_decay: bool = False):
        """Apply rope rotation to query or key tensor."""
        T = x.shape[1]
        rotation = self.get_rotation(start, start + T).unsqueeze(0).unsqueeze(2)

        if self.xpos:
            decay = self.xpos.get_decay(start, start + T).unsqueeze(0).unsqueeze(2)
        else:
            decay = 1.0

        if invert_decay:
            decay = decay ** -1

        x_complex = torch.view_as_complex(x.to(self.dtype).reshape(*x.shape[:-1], -1, 2))
        scaled_rotation = (rotation * decay) * self.scale + (1.0 - self.scale)
        x_out = torch.view_as_real(x_complex * scaled_rotation).flatten(-2)

        return x_out.type_as(x)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, _patch_passt_stft.__enter__
def __enter__(self):
        # return_complex is a mandatory parameter in latest torch versions
        # torch is throwing RuntimeErrors when not set
        torch.stft = partial(torch.stft, return_complex=False)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen.audio_channels
def audio_channels(self) -> int:
        """Audio channels of the generated audio."""
        return self.compression_model.channels
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\seanet.py
BlockTypes.METHOD, SEANetDecoder.__init__
def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 3,
                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},
                 final_activation: tp.Optional[str] = None, final_activation_params: tp.Optional[dict] = None,
                 norm: str = 'none', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,
                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,
                 pad_mode: str = 'reflect', true_skip: bool = True, compress: int = 2, lstm: int = 0,
                 disable_norm_outer_blocks: int = 0, trim_right_ratio: float = 1.0):
        super().__init__()
        self.dimension = dimension
        self.channels = channels
        self.n_filters = n_filters
        self.ratios = ratios
        del ratios
        self.n_residual_layers = n_residual_layers
        self.hop_length = np.prod(self.ratios)
        self.n_blocks = len(self.ratios) + 2  # first and last conv + residual blocks
        self.disable_norm_outer_blocks = disable_norm_outer_blocks
        assert self.disable_norm_outer_blocks >= 0 and self.disable_norm_outer_blocks <= self.n_blocks, \
            "Number of blocks for which to disable norm is invalid." \
            "It should be lower or equal to the actual number of blocks in the network and greater or equal to 0."

        act = getattr(nn, activation)
        mult = int(2 ** len(self.ratios))
        model: tp.List[nn.Module] = [
            StreamableConv1d(dimension, mult * n_filters, kernel_size,
                             norm='none' if self.disable_norm_outer_blocks == self.n_blocks else norm,
                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)
        ]

        if lstm:
            model += [StreamableLSTM(mult * n_filters, num_layers=lstm)]

        # Upsample to raw audio scale
        for i, ratio in enumerate(self.ratios):
            block_norm = 'none' if self.disable_norm_outer_blocks >= self.n_blocks - (i + 1) else norm
            # Add upsampling layers
            model += [
                act(**activation_params),
                StreamableConvTranspose1d(mult * n_filters, mult * n_filters // 2,
                                          kernel_size=ratio * 2, stride=ratio,
                                          norm=block_norm, norm_kwargs=norm_params,
                                          causal=causal, trim_right_ratio=trim_right_ratio),
            ]
            # Add residual layers
            for j in range(n_residual_layers):
                model += [
                    SEANetResnetBlock(mult * n_filters // 2, kernel_sizes=[residual_kernel_size, 1],
                                      dilations=[dilation_base ** j, 1],
                                      activation=activation, activation_params=activation_params,
                                      norm=block_norm, norm_params=norm_params, causal=causal,
                                      pad_mode=pad_mode, compress=compress, true_skip=true_skip)]

            mult //= 2

        # Add final layers
        model += [
            act(**activation_params),
            StreamableConv1d(n_filters, channels, last_kernel_size,
                             norm='none' if self.disable_norm_outer_blocks >= 1 else norm,
                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)
        ]
        # Add optional final activation to decoder (eg. tanh)
        if final_activation is not None:
            final_act = getattr(nn, final_activation)
            final_activation_params = final_activation_params or {}
            model += [
                final_act(**final_activation_params)
            ]
        self.model = nn.Sequential(*model)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\info_audio_dataset.py
BlockTypes.METHOD, _clusterify_meta
def _clusterify_meta(meta: AudioMeta) -> AudioMeta:
    """Monkey-patch meta to match cluster specificities."""
    meta.path = AudioCraftEnvironment.apply_dataset_mappers(meta.path)
    if meta.info_path is not None:
        meta.info_path.zip_path = AudioCraftEnvironment.apply_dataset_mappers(meta.info_path.zip_path)
    return meta
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\activations.py
BlockTypes.METHOD, CustomGLU.__init__
def __init__(self, activation: nn.Module, dim: int = -1):
        super(CustomGLU, self).__init__()
        self.dim = dim
        self.activation = activation
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\autocast.py
BlockTypes.METHOD, TorchAutocast.__init__
def __init__(self, enabled: bool, *args, **kwargs):
        self.autocast = torch.autocast(*args, **kwargs) if enabled else None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, PasstKLDivergenceMetric._initialize_model
def _initialize_model(self, pretrained_length: tp.Optional[float] = None):
        """Initialize underlying PaSST audio classifier."""
        model, sr, max_frames, min_frames = self._load_base_model(pretrained_length)
        self.min_input_frames = min_frames
        self.max_input_frames = max_frames
        self.model_sample_rate = sr
        self.model = model
        self.model.eval()
        self.model.to(self.device)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.update_best_state_from_stage
def update_best_state_from_stage(self, stage_name: str = 'valid'):
        """Update latest best state based on pending metrics of a given stage. This method relies
        on the `BestStateDictManager.update` method to update the best state_dict with latest weights
        if the registered states happen to match to the best performing setup.
        """
        if self.best_metric_name is None:
            # when no best metric is defined, the last state is always the best
            self._new_best_state = True
            self.logger.info("Updating best state with current state.")
        else:
            assert stage_name in self._pending_metrics, f"Metrics for stage {stage_name} not found."
            assert self.best_metric_name in self._pending_metrics[stage_name], \
                f"Best metric not found in {stage_name} metrics. Cannot register best state"
            current_score = self._pending_metrics[stage_name][self.best_metric_name]
            all_best_metric_scores = [
                past_metrics[stage_name][self.best_metric_name]
                for past_metrics in self.history
            ]
            all_best_metric_scores.append(current_score)
            best_score = min(all_best_metric_scores)
            self._new_best_state = current_score == best_score
            if self._new_best_state:
                old_best = min(all_best_metric_scores[:-1] + [float('inf')])
                self.logger.info(
                    f"New best state with {self.best_metric_name}={current_score:.3f} (was {old_best:.3f})")

        if self._new_best_state:
            if self.cfg.fsdp.use:
                # this will give an empty state dict on all ranks but the rank 0
                # which will have a copy in memory of the full model.
                with fsdp.switch_to_full_state_dict(self._fsdp_modules):
                    for name in self.best_state.states.keys():
                        state_source = self._get_state_source(name)
                        self.best_state.update(name, state_source)
                    # we save to a different dict.
                    self.fsdp_best_state.update(self.best_state.state_dict())
                # We cannot efficiently load fsdp_best_state when using FSDP,
                # so we have do do a second pass, with the local shards.
            for name in self.best_state.states.keys():
                state_source = self._get_state_source(name)
                self.best_state.update(name, state_source)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_discriminators.py
BlockTypes.METHOD, TestMultiScaleDiscriminator.test_msd_discriminator
def test_msd_discriminator(self):
        N, C, T = 2, 2, random.randrange(1, 100_000)
        t0 = torch.randn(N, C, T)

        scale_norms = ['weight_norm', 'weight_norm']
        msd = MultiScaleDiscriminator(scale_norms=scale_norms, in_channels=C)
        logits, fmaps = msd(t0)

        assert len(logits) == len(scale_norms)
        assert len(fmaps) == len(scale_norms)
        assert all([logit.shape[0] == N and len(logit.shape) == 3 for logit in logits])
        assert all([feature.shape[0] == N for fmap in fmaps for feature in fmap])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestConvertAudioChannels.test_convert_audio_channels_downmix
def test_convert_audio_channels_downmix(self):
        b, c, t = 2, 3, 100
        audio = get_batch_white_noise(b, c, t)
        mixed = convert_audio_channels(audio, channels=2)
        assert list(mixed.shape) == [b, 2, t]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, CompressionModel.set_num_codebooks
def set_num_codebooks(self, n: int):
        """Set the active number of codebooks used by the quantizer."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._get_wav_embedding
def _get_wav_embedding(self, x: JointEmbedCondition) -> torch.Tensor:
        """Get CLAP embedding from a batch of audio tensors (and corresponding sample rates)."""
        no_undefined_paths = all(p is not None for p in x.path)
        no_nullified_cond = x.wav.shape[-1] > 1  # we don't want to read from cache when condition dropout
        if self.wav_cache is not None and no_undefined_paths and no_nullified_cond:
            paths = [Path(p) for p in x.path if p is not None]
            embed = self.wav_cache.get_embed_from_cache(paths, x)
        else:
            embed = self._compute_wav_embedding(x.wav, x.length, x.sample_rate, reduce_mean=True)
        if self.normalize:
            embed = torch.nn.functional.normalize(embed, p=2.0, dim=-1)
        return embed
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cluster.py
BlockTypes.METHOD, get_cluster_type
def get_cluster_type(
    cluster_type: tp.Optional[ClusterType] = None,
) -> tp.Optional[ClusterType]:
    if cluster_type is None:
        return _guess_cluster_type()

    return cluster_type
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager.__init__
def __init__(self, xp: dora.XP, map_reference_to_sample_id: bool = False):
        self.xp = xp
        self.base_folder: Path = xp.folder / xp.cfg.generate.path
        self.reference_folder = self.base_folder / 'reference'
        self.map_reference_to_sample_id = map_reference_to_sample_id
        self.samples: tp.List[Sample] = []
        self._load_samples()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\clap_consistency.py
BlockTypes.METHOD, TextConsistencyMetric.update
def update(self, audio: torch.Tensor, text: tp.List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) -> None:
        raise NotImplementedError("implement how to update the metric from the audio and text pairs.")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, CompressionModel.get_pretrained
def get_pretrained(
            name: str, device: tp.Union[torch.device, str] = 'cpu'
            ) -> 'CompressionModel':
        """Instantiate a CompressionModel from a given pretrained model.

        Args:
            name (Path or str): name of the pretrained model. See after.
            device (torch.device or str): Device on which the model is loaded.

        Pretrained models:
            - dac_44khz (https://github.com/descriptinc/descript-audio-codec)
            - dac_24khz (same)
            - facebook/encodec_24khz (https://huggingface.co/facebook/encodec_24khz)
            - facebook/encodec_32khz (https://huggingface.co/facebook/encodec_32khz)
            - your own model on HugginFace. Export instructions to come...
        """

        from . import builders, loaders
        model: CompressionModel
        if name in ['dac_44khz', 'dac_24khz']:
            model_type = name.split('_')[1]
            logger.info("Getting pretrained compression model from DAC %s", model_type)
            model = DAC(model_type)
        elif name in ['debug_compression_model']:
            logger.info("Getting pretrained compression model for debug")
            model = builders.get_debug_compression_model()
        elif Path(name).exists():
            # We assume here if the paths exist that it is in fact an AC checkpoint
            # that was exported using `audiocraft.utils.export` functions.
            model = loaders.load_compression_model(name, device=device)
        else:
            logger.info("Getting pretrained compression model from HF %s", name)
            hf_model = HFEncodecModel.from_pretrained(name)
            model = HFEncodecCompressionModel(hf_model).to(device)
        return model.to(device).eval()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, DummyQuantizer.encode
def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode a given input tensor with the specified sample rate at the given bandwidth.
        In the case of the DummyQuantizer, the codes are actually identical
        to the input and resulting quantized representation as no quantization is done.
        """
        return x.unsqueeze(1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver._compute_cross_entropy
def _compute_cross_entropy(
        self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor
    ) -> tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:
        """Compute cross entropy between multi-codebook targets and model's logits.
        The cross entropy is computed per codebook to provide codebook-level cross entropy.
        Valid timesteps for each of the codebook are pulled from the mask, where invalid
        timesteps are set to 0.

        Args:
            logits (torch.Tensor): Model's logits of shape [B, K, T, card].
            targets (torch.Tensor): Target codes, of shape [B, K, T].
            mask (torch.Tensor): Mask for valid target codes, of shape [B, K, T].
        Returns:
            ce (torch.Tensor): Cross entropy averaged over the codebooks
            ce_per_codebook (list of torch.Tensor): Cross entropy per codebook (detached).
        """
        B, K, T = targets.shape
        assert logits.shape[:-1] == targets.shape
        assert mask.shape == targets.shape
        ce = torch.zeros([], device=targets.device)
        ce_per_codebook: tp.List[torch.Tensor] = []
        for k in range(K):
            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]
            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]
            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]
            ce_targets = targets_k[mask_k]
            ce_logits = logits_k[mask_k]
            q_ce = F.cross_entropy(ce_logits, ce_targets)
            ce += q_ce
            ce_per_codebook.append(q_ce.detach())
        # average cross entropy across codebooks
        ce = ce / K
        return ce, ce_per_codebook
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, length_to_mask
def length_to_mask(lengths: torch.Tensor, max_len: tp.Optional[int] = None) -> torch.Tensor:
    """Utility function to convert a tensor of sequence lengths to a mask (useful when working on padded sequences).
    For example: [3, 5] => [[1, 1, 1, 0, 0], [1, 1, 1, 1, 1]]

    Args:
        lengths (torch.Tensor): tensor with lengths
        max_len (int): can set the max length manually. Defaults to None.
    Returns:
        torch.Tensor: mask with 0s where there is pad tokens else 1s
    """
    assert len(lengths.shape) == 1, "Length shape should be 1 dimensional."
    final_length = lengths.max().item() if not max_len else max_len
    final_length = max(final_length, 1)  # if all seqs are of len zero we don't want a zero-size tensor
    return torch.arange(final_length)[None, :].to(lengths.device) < lengths[:, None]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\balancer.py
BlockTypes.METHOD, Balancer.metrics
def metrics(self):
        return self._metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, _patch_passt_stft.__exit__
def __exit__(self, *exc):
        torch.stft = self.old_stft
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, DecoderLayer.forward
def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.res_blocks(x)
        x = self.norm(x)
        x = self.activation(x)
        x = self.convtr(x)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\info_audio_dataset.py
BlockTypes.METHOD, get_keyword_or_keyword_list
def get_keyword_or_keyword_list(value: tp.Optional[str]) -> tp.Union[tp.Optional[str], tp.Optional[tp.List[str]]]:
    """Preprocess a single keyword or possible a list of keywords."""
    if isinstance(value, list):
        return get_keyword_list(value)
    else:
        return get_keyword(value)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\autocast.py
BlockTypes.METHOD, TorchAutocast.__enter__
def __enter__(self):
        if self.autocast is None:
            return
        try:
            self.autocast.__enter__()
        except RuntimeError:
            device = self.autocast.device
            dtype = self.autocast.fast_dtype
            raise RuntimeError(
                f"There was an error autocasting with dtype={dtype} device={device}\n"
                "If you are on the FAIR Cluster, you might need to use autocast_dtype=float16"
            )
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_rope.py
BlockTypes.METHOD, test_rope_memory_efficient
def test_rope_memory_efficient():
    set_efficient_attention_backend('xformers')
    torch.manual_seed(1234)
    tr = StreamingTransformer(
        16, 4, 2, custom=True, dropout=0., layer_scale=0.1,
        positional_embedding='rope')
    tr_mem_efficient = StreamingTransformer(
        16, 4, 2, dropout=0., memory_efficient=True, layer_scale=0.1,
        positional_embedding='rope')
    tr_mem_efficient.load_state_dict(tr.state_dict())
    tr.eval()
    steps = 12
    x = torch.randn(3, steps, 16)

    with torch.no_grad():
        y = tr(x)
        y2 = tr_mem_efficient(x)
        # Check at float precision b/c this is the rope default.
        assert torch.allclose(y, y2, atol=1e-7), (y - y2).norm()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\visqol.py
BlockTypes.METHOD, ViSQOL._run_visqol
def _run_visqol(
        self,
        input_csv_path: tp.Union[Path, str],
        results_csv_path: tp.Union[Path, str],
        debug_csv_path: tp.Optional[tp.Union[Path, str]],
    ):
        input_csv_path = str(input_csv_path)
        results_csv_path = str(results_csv_path)
        debug_csv_path = str(debug_csv_path)
        cmd = [
            f'{self.visqol_bin}/bazel-bin/visqol',
            '--batch_input_csv', f'{input_csv_path}',
            '--results_csv', f'{results_csv_path}'
        ]
        if debug_csv_path is not None:
            cmd += ['--output_debug', f'{debug_csv_path}']
        if self.visqol_mode == "speech":
            cmd += ['--use_speech_mode']
        cmd += ['--similarity_to_quality_model', f'{self.visqol_model}']
        result = subprocess.run(cmd, capture_output=True)
        if result.returncode:
            logger.error("Error with visqol: \n %s \n %s", result.stdout.decode(), result.stderr.decode())
            raise RuntimeError("Error while executing visqol")
        result.check_returncode()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen.generate_continuation
def generate_continuation(self, prompt: torch.Tensor, prompt_sample_rate: int,
                              descriptions: tp.Optional[tp.List[tp.Optional[str]]] = None,
                              progress: bool = False) -> torch.Tensor:
        """Generate samples conditioned on audio prompts.

        Args:
            prompt (torch.Tensor): A batch of waveforms used for continuation.
                Prompt should be [B, C, T], or [C, T] if only one sample is generated.
            prompt_sample_rate (int): Sampling rate of the given audio waveforms.
            descriptions (list of str, optional): A list of strings used as text conditioning. Defaults to None.
            progress (bool, optional): Flag to display progress of the generation process. Defaults to False.
        """
        if prompt.dim() == 2:
            prompt = prompt[None]
        if prompt.dim() != 3:
            raise ValueError("prompt should have 3 dimensions: [B, C, T] (C = 1).")
        prompt = convert_audio(prompt, prompt_sample_rate, self.sample_rate, self.audio_channels)
        if descriptions is None:
            descriptions = [None] * len(prompt)
        attributes, prompt_tokens = self._prepare_tokens_and_attributes(descriptions, prompt)
        assert prompt_tokens is not None
        return self._generate_tokens(attributes, prompt_tokens, progress)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingTransformer.forward
def forward(self, x: torch.Tensor, *args, **kwargs):
        B, T, C = x.shape

        if 'offsets' in self._streaming_state:
            offsets = self._streaming_state['offsets']
        else:
            offsets = torch.zeros(B, dtype=torch.long, device=x.device)

        if self.positional_embedding in ['sin', 'sin_rope']:
            positions = torch.arange(T, device=x.device).view(1, -1, 1)
            positions = positions + offsets.view(-1, 1, 1)
            pos_emb = create_sin_embedding(positions, C, max_period=self.max_period, dtype=x.dtype)
            x = x + self.positional_scale * pos_emb

        for layer in self.layers:
            x = self._apply_layer(layer, x, *args, **kwargs)

        if self._is_streaming:
            self._streaming_state['offsets'] = offsets + T

        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.forward
def forward(self, x: torch.Tensor) -> qt.QuantizedResult:
        # We don't support training with this.
        raise NotImplementedError("Forward and training with DAC not supported.")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, BaseConditioner.tokenize
def tokenize(self, *args, **kwargs) -> tp.Any:
        """Should be any part of the processing that will lead to a synchronization
        point, e.g. BPE tokenization with transfer to the GPU.

        The returned value will be saved and return later when calling forward().
        """
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen.get_pretrained
def get_pretrained(name: str = 'facebook/audiogen-medium', device=None):
        """Return pretrained model, we provide a single model for now:
        - facebook/audiogen-medium (1.5B), text to sound,
          # see: https://huggingface.co/facebook/audiogen-medium
        """
        if device is None:
            if torch.cuda.device_count():
                device = 'cuda'
            else:
                device = 'cpu'

        if name == 'debug':
            # used only for unit tests
            compression_model = get_debug_compression_model(device, sample_rate=16000)
            lm = get_debug_lm_model(device)
            return AudioGen(name, compression_model, lm, max_duration=10)

        compression_model = load_compression_model(name, device=device)
        lm = load_lm_model(name, device=device)
        assert 'self_wav' not in lm.condition_provider.conditioners, \
            "AudioGen do not support waveform conditioning for now"
        return AudioGen(name, compression_model, lm)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, betas_from_alpha_bar
def betas_from_alpha_bar(alpha_bar):
    alphas = torch.cat([torch.Tensor([alpha_bar[0]]), alpha_bar[1:]/alpha_bar[:-1]])
    return 1 - alphas
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\streaming.py
BlockTypes.METHOD, StreamingModule.reset_streaming
def reset_streaming(self):
        """Reset the streaming state."""
        def _reset(name: str, module: StreamingModule):
            module._streaming_state.clear()

        self._apply_named_streaming(_reset)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\clap_consistency.py
BlockTypes.METHOD, TextConsistencyMetric.compute
def compute(self):
        raise NotImplementedError("implement how to compute the final metric score.")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.decode
def decode(self, codes: torch.Tensor, scale: tp.Optional[torch.Tensor] = None):
        if scale is None:
            scales = [None]  # type: ignore
        else:
            scales = scale  # type: ignore
        res = self.model.decode(codes[None], scales)
        return res[0]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_rope.py
BlockTypes.METHOD, test_rope_streaming
def test_rope_streaming():
    set_efficient_attention_backend('xformers')
    torch.manual_seed(1234)
    tr = StreamingTransformer(
        16, 4, 2, causal=True, dropout=0.,
        custom=True, positional_embedding='rope')
    tr.eval()
    steps = 12
    x = torch.randn(3, steps, 16)

    ref = tr(x)

    with tr.streaming():
        outs = []
        frame_sizes = [1] * steps

        for frame_size in frame_sizes:
            frame = x[:, :frame_size]
            x = x[:, frame_size:]
            outs.append(tr(frame))

    out = torch.cat(outs, dim=1)
    assert list(out.shape) == [3, steps, 16]
    delta = torch.norm(out - ref) / torch.norm(out)
    assert delta < 1e-6, delta
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager.add_sample
def add_sample(self, sample_wav: torch.Tensor, epoch: int, index: int = 0,
                   conditions: tp.Optional[tp.Dict[str, str]] = None, prompt_wav: tp.Optional[torch.Tensor] = None,
                   ground_truth_wav: tp.Optional[torch.Tensor] = None,
                   generation_args: tp.Optional[tp.Dict[str, tp.Any]] = None) -> Sample:
        """Adds a single sample.
        The sample is stored in the XP's sample output directory, under a corresponding epoch folder.
        Each sample is assigned an id which is computed using the input data. In addition to the
        sample itself, a json file containing associated metadata is stored next to it.

        Args:
            sample_wav (torch.Tensor): sample audio to store. Tensor of shape [channels, shape].
            epoch (int): current training epoch.
            index (int): helpful to differentiate samples from the same batch.
            conditions (dict[str, str], optional): conditioning used during generation.
            prompt_wav (torch.Tensor, optional): prompt used during generation. Tensor of shape [channels, shape].
            ground_truth_wav (torch.Tensor, optional): reference audio where prompt was extracted from.
                Tensor of shape [channels, shape].
            generation_args (dict[str, any], optional): dictionary of other arguments used during generation.
        Returns:
            Sample: The saved sample.
        """
        sample_id = self._get_sample_id(index, prompt_wav, conditions)
        reuse_id = self.map_reference_to_sample_id
        prompt, ground_truth = None, None
        if prompt_wav is not None:
            prompt_id = sample_id if reuse_id else self._get_tensor_id(prompt_wav.sum(0, keepdim=True))
            prompt_duration = prompt_wav.shape[-1] / self.xp.cfg.sample_rate
            prompt_path = self._store_audio(prompt_wav, self.base_folder / str(epoch) / 'prompt' / prompt_id)
            prompt = ReferenceSample(prompt_id, str(prompt_path), prompt_duration)
        if ground_truth_wav is not None:
            ground_truth_id = sample_id if reuse_id else self._get_tensor_id(ground_truth_wav.sum(0, keepdim=True))
            ground_truth_duration = ground_truth_wav.shape[-1] / self.xp.cfg.sample_rate
            ground_truth_path = self._store_audio(ground_truth_wav, self.base_folder / 'reference' / ground_truth_id)
            ground_truth = ReferenceSample(ground_truth_id, str(ground_truth_path), ground_truth_duration)
        sample_path = self._store_audio(sample_wav, self.base_folder / str(epoch) / sample_id, overwrite=True)
        duration = sample_wav.shape[-1] / self.xp.cfg.sample_rate
        sample = Sample(sample_id, str(sample_path), epoch, duration, conditions, prompt, ground_truth, generation_args)
        self.samples.append(sample)
        with open(sample_path.with_suffix('.json'), 'w') as f:
            json.dump(asdict(sample), f, indent=2)
        return sample
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_chroma_cosine_similarity
def get_chroma_cosine_similarity(cfg: omegaconf.DictConfig) -> metrics.ChromaCosineSimilarityMetric:
    """Instantiate Chroma Cosine Similarity metric from config."""
    assert cfg.model == 'chroma_base', "Only support 'chroma_base' method for chroma cosine similarity metric"
    kwargs = dict_from_config(cfg.get(cfg.model))
    return metrics.ChromaCosineSimilarityMetric(**kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, MultiBandProcessor.return_sample
def return_sample(self, x: torch.Tensor):
        assert x.dim() == 3
        bands = self.split_bands(x)
        rescale = (self.std / self.target_std) ** self.power_std
        bands = bands * rescale.view(-1, 1, 1, 1) + self.mean.view(-1, 1, 1, 1)
        return bands.sum(dim=0)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.run_epoch
def run_epoch(self):
        """Run a single epoch with all stages.

        Metrics for a given stage are stored in _pending_metrics and committed by the solver afterwards.
        Children solvers can extend this method with custom behavior, e.g.:

            def run_epoch(self):
                ... # custom code
                super().run_epoch()
                ... # custom code
        """
        self.run_stage('train', self.train)
        with torch.no_grad():
            with self.swap_ema_state():
                self.run_stage('valid', self.valid)
                # the best state is updated with EMA states if available
                self.update_best_state_from_stage('valid')
            with self.swap_best_state():
                if self.should_run_stage('evaluate'):
                    self.run_stage('evaluate', self.evaluate)
                if self.should_run_stage('generate'):
                    self.run_stage('generate', with_rank_rng()(self.generate))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_musicgen.py
BlockTypes.METHOD, TestMusicGenModel.test_generate
def test_generate(self):
        mg = self.get_musicgen()
        wav = mg.generate(
            ['youpi', 'lapin dort'])
        assert list(wav.shape) == [2, 1, 64000]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_diffusion_model
def get_diffusion_model(cfg: omegaconf.DictConfig):
    # TODO Find a way to infer the channels from dset
    channels = cfg.channels
    num_steps = cfg.schedule.num_steps
    return DiffusionUnet(
            chin=channels, num_steps=num_steps, **cfg.diffusion_unet)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.get_eval_solver_from_sig
def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                                 device: tp.Optional[str] = None, autocast: bool = True,
                                 batch_size: tp.Optional[int] = None,
                                 override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                                 **kwargs):
        """Mostly a convenience function around audiocraft.train.get_solver_from_sig,
        populating all the proper param, deactivating EMA, FSDP, loading the best state,
        basically all you need to get a solver ready to "play" with in single GPU mode
        and with minimal memory overhead.

        Args:
            sig (str): signature to load.
            dtype (str or None): potential dtype, as a string, i.e. 'float16'.
            device (str or None): potential device, as a string, i.e. 'cuda'.
            override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. 'cuda'.
        """
        from audiocraft import train
        our_override_cfg: tp.Dict[str, tp.Any] = {'optim': {'ema': {'use': False}}}
        our_override_cfg['autocast'] = autocast
        if dtype is not None:
            our_override_cfg['dtype'] = dtype
        if device is not None:
            our_override_cfg['device'] = device
        if batch_size is not None:
            our_override_cfg['dataset'] = {'batch_size': batch_size}
        if override_cfg is None:
            override_cfg = {}
        override_cfg = omegaconf.OmegaConf.merge(
            omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
        solver = train.get_solver_from_sig(
            sig, override_cfg=override_cfg,
            load_best=True, disable_fsdp=True,
            ignore_state_keys=['optimizer', 'ema'], **kwargs)
        solver.model.eval()
        return solver
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.__init__
def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__(cfg)
        # easier access to sampling parameters
        self.generation_params = {
            'use_sampling': self.cfg.generate.lm.use_sampling,
            'temp': self.cfg.generate.lm.temp,
            'top_k': self.cfg.generate.lm.top_k,
            'top_p': self.cfg.generate.lm.top_p,
        }
        self._best_metric_name: tp.Optional[str] = 'ce'

        self._cached_batch_writer = None
        self._cached_batch_loader = None
        if cfg.cache.path:
            if cfg.cache.write:
                self._cached_batch_writer = CachedBatchWriter(Path(cfg.cache.path))
                if self.cfg.cache.write_num_shards:
                    self.logger.warning("Multiple shard cache, best_metric_name will be set to None.")
                    self._best_metric_name = None
            else:
                self._cached_batch_loader = CachedBatchLoader(
                    Path(cfg.cache.path), cfg.dataset.batch_size, cfg.dataset.num_workers,
                    min_length=self.cfg.optim.updates_per_epoch or 1)
                self.dataloaders['original_train'] = self.dataloaders['train']
                self.dataloaders['train'] = self._cached_batch_loader  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\train.py
BlockTypes.METHOD, init_seed_and_system
def init_seed_and_system(cfg):
    import numpy as np
    import torch
    import random
    from audiocraft.modules.transformer import set_efficient_attention_backend

    multiprocessing.set_start_method(cfg.mp_start_method)
    logger.debug('Setting mp start method to %s', cfg.mp_start_method)
    random.seed(cfg.seed)
    np.random.seed(cfg.seed)
    # torch also initialize cuda seed if available
    torch.manual_seed(cfg.seed)
    torch.set_num_threads(cfg.num_threads)
    os.environ['MKL_NUM_THREADS'] = str(cfg.num_threads)
    os.environ['OMP_NUM_THREADS'] = str(cfg.num_threads)
    logger.debug('Setting num threads to %d', cfg.num_threads)
    set_efficient_attention_backend(cfg.efficient_attention_backend)
    logger.debug('Setting efficient attention backend to %s', cfg.efficient_attention_backend)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, PasstKLDivergenceMetric._load_base_model
def _load_base_model(self, pretrained_length: tp.Optional[float]):
        """Load pretrained model from PaSST."""
        try:
            if pretrained_length == 30:
                from hear21passt.base30sec import get_basic_model  # type: ignore
                max_duration = 30
            elif pretrained_length == 20:
                from hear21passt.base20sec import get_basic_model  # type: ignore
                max_duration = 20
            else:
                from hear21passt.base import get_basic_model  # type: ignore
                # Original PASST was trained on AudioSet with 10s-long audio samples
                max_duration = 10
            min_duration = 0.15
            min_duration = 0.15
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "Please install hear21passt to compute KL divergence: ",
                "pip install 'git+https://github.com/kkoutini/passt_hear21@0.0.19#egg=hear21passt'"
            )
        model_sample_rate = 32_000
        max_input_frames = int(max_duration * model_sample_rate)
        min_input_frames = int(min_duration * model_sample_rate)
        with open(os.devnull, 'w') as f, contextlib.redirect_stdout(f):
            model = get_basic_model(mode='logits')
        return model, model_sample_rate, max_input_frames, min_input_frames
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\rope.py
BlockTypes.METHOD, XPos.__init__
def __init__(self, dim: int, smoothing: float = 0.4, base_scale: int = 512,
                 device=None, dtype: torch.dtype = torch.float32):
        super().__init__()
        assert dim % 2 == 0
        assert dtype in [torch.float64, torch.float32]
        self.dtype = dtype
        self.base_scale = base_scale

        half_dim = dim // 2
        adim = torch.arange(half_dim, device=device, dtype=dtype)
        decay_rates = (adim / half_dim + smoothing) / (1.0 + smoothing)
        self.register_buffer("decay_rates", decay_rates)
        self.decay: tp.Optional[torch.Tensor] = None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, DummyQuantizer.decode
def decode(self, codes: torch.Tensor) -> torch.Tensor:
        """Decode the given codes to the quantized representation.
        In the case of the DummyQuantizer, the codes are actually identical
        to the input and resulting quantized representation as no quantization is done.
        """
        return codes.squeeze(1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\activations.py
BlockTypes.METHOD, CustomGLU.forward
def forward(self, x: Tensor):
        assert x.shape[self.dim] % 2 == 0  # M = N / 2
        a, b = torch.chunk(x, 2, dim=self.dim)
        return a * self.activation(b)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.build_dataloaders
def build_dataloaders(self):
        """Method to implement to initialize dataloaders."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.build_dataloaders
def build_dataloaders(self):
        """Build audio dataloaders for each stage."""
        self.dataloaders = builders.get_audio_datasets(self.cfg)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, get_dataset_from_loader
def get_dataset_from_loader(dataloader):
    dataset = dataloader.dataset
    if isinstance(dataset, torch.utils.data.Subset):
        return dataset.dataset
    else:
        return dataset
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.encode
def encode(self, x: torch.Tensor) -> tp.Tuple[torch.Tensor, tp.Optional[torch.Tensor]]:
        codes = self.model.encode(x, self.n_quantizers)[1]
        return codes, None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.generate
def generate(self, descriptions: tp.List[str], progress: bool = False, return_tokens: bool = False) \
            -> tp.Union[torch.Tensor, tp.Tuple[torch.Tensor, torch.Tensor]]:
        """Generate samples conditioned on text.

        Args:
            descriptions (list of str): A list of strings used as text conditioning.
            progress (bool, optional): Flag to display progress of the generation process. Defaults to False.
        """
        attributes, prompt_tokens = self._prepare_tokens_and_attributes(descriptions, None)
        assert prompt_tokens is None
        tokens = self._generate_tokens(attributes, prompt_tokens, progress)
        if return_tokens:
            return self.generate_audio(tokens), tokens
        return self.generate_audio(tokens)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioMeta.test_save_audio_meta
def test_save_audio_meta(self):
        audio_meta = [
            AudioMeta("mypath1", 1., 16_000, None, None, PathInZip('/foo/bar.zip:/relative/file1.json')),
            AudioMeta("mypath2", 2., 16_000, None, None, PathInZip('/foo/bar.zip:/relative/file2.json'))
            ]
        empty_audio_meta = []
        for idx, meta in enumerate([audio_meta, empty_audio_meta]):
            path = self.get_temp_path(f'data_{idx}_save.jsonl')
            save_audio_meta(path, meta)
            with open(path, 'r') as f:
                lines = f.readlines()
                read_meta = [AudioMeta.from_dict(json.loads(line)) for line in lines]
                assert len(read_meta) == len(meta)
                for m, read_m in zip(meta, read_meta):
                    assert m == read_m
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, snr_mix
def snr_mix(src: torch.Tensor, dst: torch.Tensor, snr_low: int, snr_high: int, min_overlap: float):
    if snr_low == snr_high:
        snr = snr_low
    else:
        snr = np.random.randint(snr_low, snr_high)
    mix = snr_mixer(src, dst, snr, min_overlap)
    return mix
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, STFTLoss.__init__
def __init__(self, n_fft: int = 1024, hop_length: int = 120, win_length: int = 600,
                 window: str = "hann_window", normalized: bool = False,
                 factor_sc: float = 0.1, factor_mag: float = 0.1,
                 epsilon: float = torch.finfo(torch.float32).eps):
        super().__init__()
        self.loss = STFTLosses(n_fft, hop_length, win_length, window, normalized, epsilon)
        self.factor_sc = factor_sc
        self.factor_mag = factor_mag
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, SegmentWithAttributes.to_condition_attributes
def to_condition_attributes(self) -> ConditioningAttributes:
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.show
def show(self):
        """Method to log any information without running the job."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, kl_divergence
def kl_divergence(pred_probs: torch.Tensor, target_probs: torch.Tensor, epsilon: float = 1e-6) -> torch.Tensor:
    """Computes the elementwise KL-Divergence loss between probability distributions
    from generated samples and target samples.

    Args:
        pred_probs (torch.Tensor): Probabilities for each label obtained
            from a classifier on generated audio. Expected shape is [B, num_classes].
        target_probs (torch.Tensor): Probabilities for each label obtained
            from a classifier on target audio. Expected shape is [B, num_classes].
        epsilon (float): Epsilon value.
    Returns:
        kld (torch.Tensor): KLD loss between each generated sample and target pair.
    """
    kl_div = torch.nn.functional.kl_div((pred_probs + epsilon).log(), target_probs, reduction="none")
    return kl_div.sum(-1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\chroma.py
BlockTypes.METHOD, ChromaExtractor.forward
def forward(self, wav: torch.Tensor) -> torch.Tensor:
        T = wav.shape[-1]
        # in case we are getting a wav that was dropped out (nullified)
        # from the conditioner, make sure wav length is no less that nfft
        if T < self.nfft:
            pad = self.nfft - T
            r = 0 if pad % 2 == 0 else 1
            wav = F.pad(wav, (pad // 2, pad // 2 + r), 'constant', 0)
            assert wav.shape[-1] == self.nfft, f"expected len {self.nfft} but got {wav.shape[-1]}"

        spec = self.spec(wav).squeeze(1)
        raw_chroma = torch.einsum('cf,...ft->...ct', self.fbanks, spec)
        norm_chroma = torch.nn.functional.normalize(raw_chroma, p=self.norm, dim=-2, eps=1e-6)
        norm_chroma = rearrange(norm_chroma, 'b d t -> b t d')

        if self.argmax:
            idx = norm_chroma.argmax(-1, keepdim=True)
            norm_chroma[:] = 0
            norm_chroma.scatter_(dim=-1, index=idx, value=1)

        return norm_chroma
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset._filter_duration
def _filter_duration(self, meta: tp.List[AudioMeta]) -> tp.List[AudioMeta]:
        """Filters out audio files with audio durations that will not allow to sample examples from them."""
        orig_len = len(meta)

        # Filter data that is too short.
        if self.min_audio_duration is not None:
            meta = [m for m in meta if m.duration >= self.min_audio_duration]

        # Filter data that is too long.
        if self.max_audio_duration is not None:
            meta = [m for m in meta if m.duration <= self.max_audio_duration]

        filtered_len = len(meta)
        removed_percentage = 100*(1-float(filtered_len)/orig_len)
        msg = 'Removed %.2f percent of the data because it was too short or too long.' % removed_percentage
        if removed_percentage < 10:
            logging.debug(msg)
        else:
            logging.warning(msg)
        return meta
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\_base_explorers.py
BlockTypes.METHOD, BaseExplorer.get_grid_metrics
def get_grid_metrics(self):
        """Return the metrics that should be displayed in the tracking table.
        """
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\cosine_lr_scheduler.py
BlockTypes.METHOD, CosineLRScheduler.get_lr
def get_lr(self):
        return [self._get_sched_lr(lr, self.last_epoch) for lr in self.base_lrs]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.log_updates
def log_updates(self):
        # convenient access to log updates
        return self._log_updates
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, BaseConditioner.forward
def forward(self, inputs: tp.Any) -> ConditionType:
        """Gets input that should be used as conditioning (e.g, genre, description or a waveform).
        Outputs a ConditionType, after the input data was embedded as a dense vector.

        Returns:
            ConditionType:
                - A tensor of size [B, T, D] where B is the batch size, T is the length of the
                  output embedding and D is the dimension of the embedding.
                - And a mask indicating where the padding tokens.
        """
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, JointEmbeddingConditioner.__init__
def __init__(self, dim: int, output_dim: int, device: str, attribute: str,
                 autocast_dtype: tp.Optional[str] = 'float32', quantize: bool = True,
                 n_q: int = 12, bins: int = 1024, **kwargs):
        super().__init__(dim=dim, output_dim=output_dim)
        self.device = device
        self.attribute = attribute
        if autocast_dtype is None or device == 'cpu':
            self.autocast = TorchAutocast(enabled=False)
            logger.warning("JointEmbeddingConditioner has no autocast, this might lead to NaN.")
        else:
            dtype = getattr(torch, autocast_dtype)
            assert isinstance(dtype, torch.dtype)
            logger.info(f"JointEmbeddingConditioner will be evaluated with autocast as {autocast_dtype}.")
            self.autocast = TorchAutocast(enabled=True, device_type=self.device, dtype=dtype)
        # residual vector quantizer to discretize the conditioned embedding
        self.quantizer: tp.Optional[ResidualVectorQuantizer] = None
        if quantize:
            self.quantizer = ResidualVectorQuantizer(dim, n_q=n_q, bins=bins, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\vq.py
BlockTypes.METHOD, ResidualVectorQuantizer.__init__
def __init__(
        self,
        dimension: int = 256,
        n_q: int = 8,
        q_dropout: bool = False,
        bins: int = 1024,
        decay: float = 0.99,
        kmeans_init: bool = True,
        kmeans_iters: int = 10,
        threshold_ema_dead_code: int = 2,
        orthogonal_reg_weight: float = 0.0,
        orthogonal_reg_active_codes_only: bool = False,
        orthogonal_reg_max_codes: tp.Optional[int] = None,
    ):
        super().__init__()
        self.max_n_q = n_q
        self.n_q = n_q
        self.q_dropout = q_dropout
        self.dimension = dimension
        self.bins = bins
        self.decay = decay
        self.kmeans_init = kmeans_init
        self.kmeans_iters = kmeans_iters
        self.threshold_ema_dead_code = threshold_ema_dead_code
        self.orthogonal_reg_weight = orthogonal_reg_weight
        self.orthogonal_reg_active_codes_only = orthogonal_reg_active_codes_only
        self.orthogonal_reg_max_codes = orthogonal_reg_max_codes
        self.vq = ResidualVectorQuantization(
            dim=self.dimension,
            codebook_size=self.bins,
            num_quantizers=self.n_q,
            decay=self.decay,
            kmeans_init=self.kmeans_init,
            kmeans_iters=self.kmeans_iters,
            threshold_ema_dead_code=self.threshold_ema_dead_code,
            orthogonal_reg_weight=self.orthogonal_reg_weight,
            orthogonal_reg_active_codes_only=self.orthogonal_reg_active_codes_only,
            orthogonal_reg_max_codes=self.orthogonal_reg_max_codes,
            channels_last=False
        )
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestAvRead.test_avread_seek_outofbound
def test_avread_seek_outofbound(self):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 1.
        for sample_rate, ch in product(sample_rates, channels):
            n_frames = int(sample_rate * duration)
            wav = get_white_noise(ch, n_frames)
            path = self.get_temp_path(f'reference_c_{sample_rate}_{ch}.wav')
            save_wav(path, wav, sample_rate)
            seek_time = 1.5
            read_wav, read_sr = _av_read(path, seek_time, 1.)
            assert read_sr == sample_rate
            assert read_wav.shape[0] == wav.shape[0]
            assert read_wav.shape[-1] == 0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset.__init__
def __init__(self,
                 meta: tp.List[AudioMeta],
                 segment_duration: tp.Optional[float] = None,
                 shuffle: bool = True,
                 num_samples: int = 10_000,
                 sample_rate: int = 48_000,
                 channels: int = 2,
                 pad: bool = True,
                 sample_on_duration: bool = True,
                 sample_on_weight: bool = True,
                 min_segment_ratio: float = 0.5,
                 max_read_retry: int = 10,
                 return_info: bool = False,
                 min_audio_duration: tp.Optional[float] = None,
                 max_audio_duration: tp.Optional[float] = None,
                 shuffle_seed: int = 0,
                 load_wav: bool = True,
                 permutation_on_files: bool = False,
                 ):
        assert len(meta) > 0, "No audio meta provided to AudioDataset. Please check loading of audio meta."
        assert segment_duration is None or segment_duration > 0
        assert segment_duration is None or min_segment_ratio >= 0
        self.segment_duration = segment_duration
        self.min_segment_ratio = min_segment_ratio
        self.max_audio_duration = max_audio_duration
        self.min_audio_duration = min_audio_duration
        if self.min_audio_duration is not None and self.max_audio_duration is not None:
            assert self.min_audio_duration <= self.max_audio_duration
        self.meta: tp.List[AudioMeta] = self._filter_duration(meta)
        assert len(self.meta)  # Fail fast if all data has been filtered.
        self.total_duration = sum(d.duration for d in self.meta)

        if segment_duration is None:
            num_samples = len(self.meta)
        self.num_samples = num_samples
        self.shuffle = shuffle
        self.sample_rate = sample_rate
        self.channels = channels
        self.pad = pad
        self.sample_on_weight = sample_on_weight
        self.sample_on_duration = sample_on_duration
        self.sampling_probabilities = self._get_sampling_probabilities()
        self.max_read_retry = max_read_retry
        self.return_info = return_info
        self.shuffle_seed = shuffle_seed
        self.current_epoch: tp.Optional[int] = None
        self.load_wav = load_wav
        if not load_wav:
            assert segment_duration is not None
        self.permutation_on_files = permutation_on_files
        if permutation_on_files:
            assert not self.sample_on_duration
            assert not self.sample_on_weight
            assert self.shuffle
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric._sequential_create_embedding_beams
def _sequential_create_embedding_beams(self):
        logger.info("Creating embeddings beams in a sequential manner")
        tests_beams_process, tests_beams_log_file = self._create_embedding_beams(is_background=False)
        tests_beams_code = tests_beams_process.wait()
        self._log_process_result(tests_beams_code, tests_beams_log_file, is_background=False)
        bg_beams_process, bg_beams_log_file = self._create_embedding_beams(is_background=True)
        bg_beams_code = bg_beams_process.wait()
        self._log_process_result(bg_beams_code, bg_beams_log_file, is_background=True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, AdversarialLoss.forward
def forward(self, fake: torch.Tensor, real: torch.Tensor) -> tp.Tuple[torch.Tensor, torch.Tensor]:
        """Return the loss for the generator, i.e. trying to fool the adversary,
        and feature matching loss if provided.
        """
        adv = torch.tensor(0., device=fake.device)
        feat = torch.tensor(0., device=fake.device)
        with flashy.utils.readonly(self.adversary):
            all_logits_fake_is_fake, all_fmap_fake = self.get_adversary_pred(fake)
            all_logits_real_is_fake, all_fmap_real = self.get_adversary_pred(real)
            n_sub_adversaries = len(all_logits_fake_is_fake)
            for logit_fake_is_fake in all_logits_fake_is_fake:
                adv += self.loss(logit_fake_is_fake)
            if self.loss_feat:
                for fmap_fake, fmap_real in zip(all_fmap_fake, all_fmap_real):
                    feat += self.loss_feat(fmap_fake, fmap_real)

        if self.normalize:
            adv /= n_sub_adversaries
            feat /= n_sub_adversaries

        return adv, feat
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric._create_embedding_beams
def _create_embedding_beams(self, is_background: bool, gpu_index: tp.Optional[int] = None):
        if is_background:
            input_samples_dir = self.samples_background_dir
            input_filename = self.manifest_background
            stats_name = self.stats_background_dir
        else:
            input_samples_dir = self.samples_tests_dir
            input_filename = self.manifest_tests
            stats_name = self.stats_tests_dir
        beams_name = self._get_samples_name(is_background)
        log_file = self.tmp_dir / f'fad_logs_create_beams_{beams_name}.log'

        logger.info(f"Scanning samples folder to fetch list of files: {input_samples_dir}")
        with open(input_filename, "w") as fout:
            for path in Path(input_samples_dir).glob(f"*.{self.format}"):
                fout.write(f"{str(path)}\n")

        cmd = [
            self.python_path, "-m",
            "frechet_audio_distance.create_embeddings_main",
            "--model_ckpt", f"{self.model_path}",
            "--input_files", f"{str(input_filename)}",
            "--stats", f"{str(stats_name)}",
        ]
        if self.batch_size is not None:
            cmd += ["--batch_size", str(self.batch_size)]
        logger.info(f"Launching frechet_audio_distance embeddings main method: {' '.join(cmd)} on {beams_name}")
        env = os.environ
        if gpu_index is not None:
            env["CUDA_VISIBLE_DEVICES"] = str(gpu_index)
        process = subprocess.Popen(
            cmd, stdout=open(log_file, "w"), env={**env, **self.tf_env}, stderr=subprocess.STDOUT)
        return process, log_file
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, _get_audio_meta
def _get_audio_meta(file_path: str, minimal: bool = True) -> AudioMeta:
    """AudioMeta from a path to an audio file.

    Args:
        file_path (str): Resolved path of valid audio file.
        minimal (bool): Whether to only load the minimal set of metadata (takes longer if not).
    Returns:
        AudioMeta: Audio file path and its metadata.
    """
    info = audio_info(file_path)
    amplitude: tp.Optional[float] = None
    if not minimal:
        wav, sr = audio_read(file_path)
        amplitude = wav.abs().max().item()
    return AudioMeta(file_path, info.duration, info.sample_rate, amplitude)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\mpd.py
BlockTypes.METHOD, PeriodDiscriminator.forward
def forward(self, x: torch.Tensor):
        fmap = []
        # 1d to 2d
        b, c, t = x.shape
        if t % self.period != 0:  # pad first
            n_pad = self.period - (t % self.period)
            x = F.pad(x, (0, n_pad), 'reflect')
            t = t + n_pad
        x = x.view(b, c, t // self.period, self.period)

        for conv in self.convs:
            x = conv(x)
            x = self.activation(x)
            fmap.append(x)
        x = self.conv_post(x)
        fmap.append(x)
        # x = torch.flatten(x, 1, -1)

        return x, fmap
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, SampleProcessor.project_sample
def project_sample(self, x: torch.Tensor):
        """Project the original sample to the 'space' where the diffusion will happen."""
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, MultiBandDiffusion.re_eq
def re_eq(self, wav: torch.Tensor, ref: torch.Tensor, n_bands: int = 32, strictness: float = 1):
        """match the eq to the encodec output by matching the standard deviation of some frequency bands
        Args:
            wav (torch.Tensor): audio to equalize
            ref (torch.Tensor):refenrence audio from which we match the spectrogram.
            n_bands (int): number of bands of the eq
            strictness (float): how strict the the matching. 0 is no matching, 1 is exact matching.
        """
        split = julius.SplitBands(n_bands=n_bands, sample_rate=self.codec_model.sample_rate).to(wav.device)
        bands = split(wav)
        bands_ref = split(ref)
        out = torch.zeros_like(ref)
        for i in range(n_bands):
            out += bands[i] * (bands_ref[i].std() / bands[i].std()) ** strictness
        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.valid_layout
def valid_layout(self):
        valid_step = len(self.layout) - self.max_delay
        return self.layout[:valid_step]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingMultiheadAttention._get_mask
def _get_mask(self, current_steps: int, device: torch.device, dtype: torch.dtype):
        # Return a causal mask, accounting for potentially stored past keys/values
        # We actually return a bias for the attention score, as this has the same
        # convention both in the builtin MHA in Pytorch, and Xformers functions.
        time_dim = _get_attention_time_dimension()
        if self.memory_efficient:
            from xformers.ops import LowerTriangularMask
            if current_steps == 1:
                # If we only have one step, then we do not need a mask.
                return None
            elif 'past_keys' in self._streaming_state:
                raise RuntimeError("Not supported at the moment")
            else:
                # Then we can safely use a lower triangular mask
                return LowerTriangularMask()
        if self._streaming_state:
            past_keys = self._streaming_state['past_keys']
            past_steps = past_keys.shape[time_dim]
        else:
            past_steps = 0

        queries_pos = torch.arange(
            past_steps, current_steps + past_steps, device=device).view(-1, 1)
        keys_pos = torch.arange(past_steps + current_steps, device=device).view(1, -1)
        delta = queries_pos - keys_pos
        valid = delta >= 0
        if self.past_context is not None:
            valid &= (delta <= self.past_context)
        return torch.where(
            valid,
            torch.zeros([], device=device, dtype=dtype),
            torch.full([], float('-inf'), device=device, dtype=dtype))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\_explorers.py
BlockTypes.METHOD, LMExplorer.stages
def stages(self) -> tp.List[str]:
        return ['train', 'valid']
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\clap_consistency.py
BlockTypes.METHOD, CLAPTextConsistencyMetric.__init__
def __init__(self, model_path: tp.Union[str, Path], model_arch: str = 'HTSAT-tiny', enable_fusion: bool = False):
        super().__init__()
        if laion_clap is None:
            raise ImportError("Please install CLAP to compute text consistency: 'pip install laion_clap'")
        self.add_state("cosine_sum", default=torch.tensor(0.), dist_reduce_fx="sum")
        self.add_state("weight", default=torch.tensor(0.), dist_reduce_fx="sum")
        self._initialize_model(model_path, model_arch, enable_fusion)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, DummyQuantizer.total_codebooks
def total_codebooks(self):
        """Total number of codebooks."""
        return 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.show
def show(self):
        # TODO
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner.reset_eval_wavs
def reset_eval_wavs(self, eval_wavs: tp.Optional[torch.Tensor]) -> None:
        self.eval_wavs = eval_wavs
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.checkpoint_path
def checkpoint_path(self, **kwargs):
        kwargs.setdefault('use_fsdp', self.cfg.fsdp.use)
        return self.folder / checkpoint.checkpoint_name(**kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_musicgen.py
BlockTypes.METHOD, TestMusicGenModel.test_generate_long
def test_generate_long(self):
        mg = self.get_musicgen()
        mg.max_duration = 3.
        mg.set_generation_params(duration=4., extend_stride=2.)
        wav = mg.generate(
            ['youpi', 'lapin dort'])
        assert list(wav.shape) == [2, 1, 32000 * 4]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_utils.py
BlockTypes.METHOD, convert_audio_channels
def convert_audio_channels(wav: torch.Tensor, channels: int = 2) -> torch.Tensor:
    """Convert audio to the given number of channels.

    Args:
        wav (torch.Tensor): Audio wave of shape [B, C, T].
        channels (int): Expected number of channels as output.
    Returns:
        torch.Tensor: Downmixed or unchanged audio wave [B, C, T].
    """
    *shape, src_channels, length = wav.shape
    if src_channels == channels:
        pass
    elif channels == 1:
        # Case 1:
        # The caller asked 1-channel audio, and the stream has multiple
        # channels, downmix all channels.
        wav = wav.mean(dim=-2, keepdim=True)
    elif src_channels == 1:
        # Case 2:
        # The caller asked for multiple channels, but the input file has
        # a single channel, replicate the audio over all channels.
        wav = wav.expand(*shape, channels, length)
    elif src_channels >= channels:
        # Case 3:
        # The caller asked for multiple channels, and the input file has
        # more channels than requested. In that case return the first channels.
        wav = wav[..., :channels, :]
    else:
        # Case 4: What is a reasonable choice here?
        raise ValueError('The audio file has less channels than requested but is not mono.')
    return wav
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\info_audio_dataset.py
BlockTypes.METHOD, get_string
def get_string(value: tp.Optional[str]) -> tp.Optional[str]:
    """Preprocess a single keyword."""
    if value is None or (not isinstance(value, str)) or len(value) == 0 or value == 'None':
        return None
    else:
        return value.strip()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.decode
def decode(self, codes: torch.Tensor, scale: tp.Optional[torch.Tensor] = None):
        assert scale is None
        z_q = self.decode_latent(codes)
        return self.model.decode(z_q)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, nullify_condition
def nullify_condition(condition: ConditionType, dim: int = 1):
    """Transform an input condition to a null condition.
    The way it is done by converting it to a single zero vector similarly
    to how it is done inside WhiteSpaceTokenizer and NoopTokenizer.

    Args:
        condition (ConditionType): A tuple of condition and mask (tuple[torch.Tensor, torch.Tensor])
        dim (int): The dimension that will be truncated (should be the time dimension)
        WARNING!: dim should not be the batch dimension!
    Returns:
        ConditionType: A tuple of null condition and mask
    """
    assert dim != 0, "dim cannot be the batch dimension!"
    assert isinstance(condition, tuple) and \
        isinstance(condition[0], torch.Tensor) and \
        isinstance(condition[1], torch.Tensor), "'nullify_condition' got an unexpected input type!"
    cond, mask = condition
    B = cond.shape[0]
    last_dim = cond.dim() - 1
    out = cond.transpose(dim, last_dim)
    out = 0. * out[..., :1]
    out = out.transpose(dim, last_dim)
    mask = torch.zeros((B, 1), device=out.device).int()
    assert cond.dim() == out.dim()
    return out, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, NoiseSchedule.generate
def generate(self, model: torch.nn.Module, initial: tp.Optional[torch.Tensor] = None,
                 condition: tp.Optional[torch.Tensor] = None, return_list: bool = False):
        """Full ddpm reverse process.

        Args:
            model (nn.Module): Diffusion model.
            initial (tensor): Initial Noise.
            condition (tensor): Input conditionning Tensor (e.g. encodec compressed representation).
            return_list (bool): Whether to return the whole process or only the sampled point.
        """
        alpha_bar = self.get_alpha_bar(step=self.num_steps - 1)
        current = initial
        iterates = [initial]
        for step in range(self.num_steps)[::-1]:
            with torch.no_grad():
                estimate = model(current, step, condition=condition).sample
            alpha = 1 - self.betas[step]
            previous = (current - (1 - alpha) / (1 - alpha_bar).sqrt() * estimate) / alpha.sqrt()
            previous_alpha_bar = self.get_alpha_bar(step=step - 1)
            if step == 0:
                sigma2 = 0
            elif self.variance == 'beta':
                sigma2 = 1 - alpha
            elif self.variance == 'beta_tilde':
                sigma2 = (1 - previous_alpha_bar) / (1 - alpha_bar) * (1 - alpha)
            elif self.variance == 'none':
                sigma2 = 0
            else:
                raise ValueError(f'Invalid variance type {self.variance}')

            if sigma2 > 0:
                previous += sigma2**0.5 * torch.randn_like(previous) * self.noise_scale
            if self.clip:
                previous = previous.clamp(-self.clip, self.clip)
            current = previous
            alpha_bar = previous_alpha_bar
            if step == 0:
                previous *= self.rescale
            if return_list:
                iterates.append(previous.cpu())

        if return_list:
            return iterates
        else:
            return self.sample_processor.return_sample(previous)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\streaming.py
BlockTypes.METHOD, StreamingModule.get_streaming_state
def get_streaming_state(self) -> State:
        """Return the streaming state, including that of sub-modules."""
        state: State = {}

        def _add(name: str, module: StreamingModule):
            if name:
                name += "."
            for key, value in module._streaming_state.items():
                state[name + key] = value

        self._apply_named_streaming(_add)
        return state
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, rms_f
def rms_f(x: torch.Tensor) -> torch.Tensor:
    return (x ** 2).mean(1).pow(0.5)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_processor
def get_processor(cfg, sample_rate: int = 24000):
    sample_processor = SampleProcessor()
    if cfg.use:
        kw = dict(cfg)
        kw.pop('use')
        kw.pop('name')
        if cfg.name == "multi_band_processor":
            sample_processor = MultiBandProcessor(sample_rate=sample_rate, **kw)
    return sample_processor
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager.latest_epoch
def latest_epoch(self):
        """Latest epoch across all samples."""
        return max(self.samples, key=lambda x: x.epoch).epoch if self.samples else 0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\balancer.py
BlockTypes.METHOD, Balancer.backward
def backward(self, losses: tp.Dict[str, torch.Tensor], input: torch.Tensor) -> torch.Tensor:
        """Compute the backward and return the effective train loss, e.g. the loss obtained from
        computing the effective weights. If `balance_grads` is True, the effective weights
        are the one that needs to be applied to each gradient to respect the desired relative
        scale of gradients coming from each loss.

        Args:
            losses (Dict[str, torch.Tensor]): dictionary with the same keys as `self.weights`.
            input (torch.Tensor): the input of the losses, typically the output of the model.
                This should be the single point of dependence between the losses
                and the model being trained.
        """
        norms = {}
        grads = {}
        for name, loss in losses.items():
            # Compute partial derivative of the less with respect to the input.
            grad, = autograd.grad(loss, [input], retain_graph=True)
            if self.per_batch_item:
                # We do not average the gradient over the batch dimension.
                dims = tuple(range(1, grad.dim()))
                norm = grad.norm(dim=dims, p=2).mean()
            else:
                norm = grad.norm(p=2)
            norms[name] = norm
            grads[name] = grad

        count = 1
        if self.per_batch_item:
            count = len(grad)
        # Average norms across workers. Theoretically we should average the
        # squared norm, then take the sqrt, but it worked fine like that.
        avg_norms = flashy.distrib.average_metrics(self.averager(norms), count)
        # We approximate the total norm of the gradient as the sums of the norms.
        # Obviously this can be very incorrect if all gradients are aligned, but it works fine.
        total = sum(avg_norms.values())

        self._metrics = {}
        if self.monitor:
            # Store the ratio of the total gradient represented by each loss.
            for k, v in avg_norms.items():
                self._metrics[f'ratio_{k}'] = v / total

        total_weights = sum([self.weights[k] for k in avg_norms])
        assert total_weights > 0.
        desired_ratios = {k: w / total_weights for k, w in self.weights.items()}

        out_grad = torch.zeros_like(input)
        effective_loss = torch.tensor(0., device=input.device, dtype=input.dtype)
        for name, avg_norm in avg_norms.items():
            if self.balance_grads:
                # g_balanced = g / avg(||g||) * total_norm * desired_ratio
                scale = desired_ratios[name] * self.total_norm / (self.epsilon + avg_norm)
            else:
                # We just do regular weighted sum of the gradients.
                scale = self.weights[name]
            out_grad.add_(grads[name], alpha=scale)
            effective_loss += scale * losses[name].detach()
        # Send the computed partial derivative with respect to the output of the model to the model.
        input.backward(out_grad)
        return effective_loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.get_sequence_coords_with_timestep
def get_sequence_coords_with_timestep(self, t: int, q: tp.Optional[int] = None):
        """Get codebook coordinates in the layout that corresponds to the specified timestep t
        and optionally to the codebook q. Coordinates are returned as a tuple with the sequence step
        and the actual codebook coordinates.
        """
        assert t <= self.timesteps, "provided timesteps is greater than the pattern's number of timesteps"
        if q is not None:
            assert q <= self.n_q, "provided number of codebooks is greater than the pattern's number of codebooks"
        coords = []
        for s, seq_codes in enumerate(self.layout):
            for code in seq_codes:
                if code.t == t and (q is None or code.q == q):
                    coords.append((s, code))
        return coords
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, hash_trick
def hash_trick(word: str, vocab_size: int) -> int:
    """Hash trick to pair each word with an index

    Args:
        word (str): word we wish to convert to an index
        vocab_size (int): size of the vocabulary
    Returns:
        int: index of the word in the embedding LUT
    """
    hash = int(hashlib.sha256(word.encode("utf-8")).hexdigest(), 16)
    return hash % vocab_size
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\_explorers.py
BlockTypes.METHOD, LMExplorer.get_grid_metrics
def get_grid_metrics(self):
        """Return the metrics that should be displayed in the tracking table."""
        return [
            tt.group(
                'train',
                [
                    tt.leaf('epoch'),
                    tt.leaf('duration', '.1f'),  # duration in minutes
                    tt.leaf('ping'),
                    tt.leaf('ce', '.4f'),  # cross entropy
                    tt.leaf("ppl", '.3f'),  # perplexity
                ],
                align='>',
            ),
            tt.group(
                'valid',
                [
                    tt.leaf('ce', '.4f'),
                    tt.leaf('ppl', '.3f'),
                    tt.leaf('best_ppl', '.3f'),
                ],
                align='>',
            ),
        ]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner.has_eval_wavs
def has_eval_wavs(self) -> bool:
        return self.eval_wavs is not None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.run_step
def run_step(self, idx: int, batch: torch.Tensor, metrics: dict):
        """Perform one training or valid step on a given batch."""
        x = batch.to(self.device)
        loss_fun = F.mse_loss if self.cfg.loss.kind == 'mse' else F.l1_loss

        condition = self.get_condition(x)  # [bs, 128, T/hop, n_emb]
        sample = self.data_processor.process_data(x)

        input_, target, step = self.schedule.get_training_item(sample,
                                                               tensor_step=self.cfg.schedule.variable_step_batch)
        out = self.model(input_, step, condition=condition).sample

        base_loss = loss_fun(out, target, reduction='none').mean(dim=(1, 2))
        reference_loss = loss_fun(input_, target, reduction='none').mean(dim=(1, 2))
        loss = base_loss / reference_loss ** self.cfg.loss.norm_power

        if self.is_training:
            loss.mean().backward()
            flashy.distrib.sync_model(self.model)
            self.optimizer.step()
            self.optimizer.zero_grad()
        metrics = {
            'loss': loss.mean(), 'normed_loss': (base_loss / reference_loss).mean(),
            }
        metrics.update(self.per_stage({'loss': loss, 'normed_loss': base_loss / reference_loss}, step))
        metrics.update({
            'std_in': input_.std(), 'std_out': out.std()})
        return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, normalize
def normalize(audio: torch.Tensor, target_level: int = -25) -> torch.Tensor:
    """Normalize the signal to the target level."""
    rms = rms_f(audio)
    scalar = 10 ** (target_level / 20) / (rms + EPS)
    audio = audio * scalar.unsqueeze(1)
    return audio
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, EncoderLayer.__init__
def __init__(self, chin: int, chout: int, kernel: int = 4, stride: int = 2,
                 norm_groups: int = 4, res_blocks: int = 1, activation: tp.Type[nn.Module] = nn.ReLU,
                 dropout: float = 0.):
        super().__init__()
        padding = (kernel - stride) // 2
        Conv = nn.Conv1d
        self.conv = Conv(chin, chout, kernel, stride, padding, bias=False)
        self.norm = nn.GroupNorm(norm_groups, chout)
        self.activation = activation()
        self.res_blocks = nn.Sequential(
            *[ResBlock(chout, norm_groups=norm_groups, dilation=2**idx, dropout=dropout)
              for idx in range(res_blocks)])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, SampleProcessor.return_sample
def return_sample(self, z: torch.Tensor):
        """Project back from diffusion space to the actual sample space."""
        return z
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\autocast.py
BlockTypes.METHOD, TorchAutocast.__exit__
def __exit__(self, *args, **kwargs):
        if self.autocast is None:
            return
        self.autocast.__exit__(*args, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, multinomial
def multinomial(input: torch.Tensor, num_samples: int, replacement=False, *, generator=None):
    """torch.multinomial with arbitrary number of dimensions, and number of candidates on the last dimension.

    Args:
        input (torch.Tensor): The input tensor containing probabilities.
        num_samples (int): Number of samples to draw.
        replacement (bool): Whether to draw with replacement or not.
    Keywords args:
        generator (torch.Generator): A pseudorandom number generator for sampling.
    Returns:
        torch.Tensor: Last dimension contains num_samples indices
            sampled from the multinomial probability distribution
            located in the last dimension of tensor input.
    """
    input_ = input.reshape(-1, input.shape[-1])
    output_ = torch.multinomial(input_, num_samples=num_samples, replacement=replacement, generator=generator)
    output = output_.reshape(*list(input.shape[:-1]), -1)
    return output
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, STFTLoss.forward
def forward(self, x: torch.Tensor, y: torch.Tensor) -> tp.Tuple[torch.Tensor, torch.Tensor]:
        """Calculate forward propagation.

        Args:
            x (torch.Tensor): Predicted signal (B, T).
            y (torch.Tensor): Groundtruth signal (B, T).
        Returns:
            torch.Tensor: Single resolution STFT loss.
        """
        sc_loss, mag_loss = self.loss(x, y)
        return self.factor_sc * sc_loss + self.factor_mag * mag_loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, DummyQuantizer.num_codebooks
def num_codebooks(self):
        """Total number of codebooks."""
        return self.total_codebooks
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.epoch_checkpoint_path
def epoch_checkpoint_path(self, epoch: int, **kwargs):
        kwargs.setdefault('use_fsdp', self.cfg.fsdp.use)
        return self.folder / checkpoint.checkpoint_name(str(epoch), **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, KLDivergenceMetric.__init__
def __init__(self):
        super().__init__()
        self.add_state("kld_pq_sum", default=torch.tensor(0.), dist_reduce_fx="sum")
        self.add_state("kld_qp_sum", default=torch.tensor(0.), dist_reduce_fx="sum")
        self.add_state("kld_all_sum", default=torch.tensor(0.), dist_reduce_fx="sum")
        self.add_state("weight", default=torch.tensor(0), dist_reduce_fx="sum")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, MultiBandDiffusion.regenerate
def regenerate(self, wav: torch.Tensor, sample_rate: int):
        """Regenerate a wavform through compression and diffusion regeneration.
        Args:
            wav (torch.Tensor): Original 'ground truth' audio
            sample_rate (int): sample rate of the input (and output) wav
        """
        if sample_rate != self.codec_model.sample_rate:
            wav = julius.resample_frac(wav, sample_rate, self.codec_model.sample_rate)
        emb = self.get_condition(wav, sample_rate=self.codec_model.sample_rate)
        size = wav.size()
        out = self.generate(emb, size=size)
        if sample_rate != self.codec_model.sample_rate:
            out = julius.resample_frac(out, self.codec_model.sample_rate, sample_rate)
        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.preprocess
def preprocess(self, x):
        x = rearrange(x, "... d -> (...) d")
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestConvertAudioChannels.test_convert_audio_channels_upmix
def test_convert_audio_channels_upmix(self):
        b, c, t = 2, 1, 100
        audio = get_batch_white_noise(b, c, t)
        mixed = convert_audio_channels(audio, channels=3)
        assert list(mixed.shape) == [b, 3, t]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.decode_latent
def decode_latent(self, codes: torch.Tensor):
        """Decode from the discrete codes to continuous latent space."""
        return self.model.quantizer.decode(codes.transpose(0, 1))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, mix_text
def mix_text(src_text: str, dst_text: str):
    """Mix text from different sources by concatenating them."""
    if src_text == dst_text:
        return src_text
    return src_text + " " + dst_text
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._sample_eval_wavs
def _sample_eval_wavs(self, num_samples: int) -> torch.Tensor:
        """Sample wavs from a predefined list."""
        assert self.eval_wavs is not None, "Cannot sample eval wavs as no eval wavs provided."
        total_eval_wavs = len(self.eval_wavs)
        out = self.eval_wavs
        if num_samples > total_eval_wavs:
            out = self.eval_wavs.repeat(num_samples // total_eval_wavs + 1, 1, 1)
        return out[torch.randperm(len(out))][:num_samples]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_audio_datasets
def get_audio_datasets(cfg: omegaconf.DictConfig,
                       dataset_type: DatasetType = DatasetType.AUDIO) -> tp.Dict[str, torch.utils.data.DataLoader]:
    """Build AudioDataset from configuration.

    Args:
        cfg (omegaconf.DictConfig): Configuration.
        dataset_type: The type of dataset to create.
    Returns:
        dict[str, torch.utils.data.DataLoader]: Map of dataloader for each data split.
    """
    dataloaders: dict = {}

    sample_rate = cfg.sample_rate
    channels = cfg.channels
    seed = cfg.seed
    max_sample_rate = cfg.datasource.max_sample_rate
    max_channels = cfg.datasource.max_channels

    assert cfg.dataset is not None, "Could not find dataset definition in config"

    dataset_cfg = dict_from_config(cfg.dataset)
    splits_cfg: dict = {}
    splits_cfg['train'] = dataset_cfg.pop('train')
    splits_cfg['valid'] = dataset_cfg.pop('valid')
    splits_cfg['evaluate'] = dataset_cfg.pop('evaluate')
    splits_cfg['generate'] = dataset_cfg.pop('generate')
    execute_only_stage = cfg.get('execute_only', None)

    for split, path in cfg.datasource.items():
        if not isinstance(path, str):
            continue  # skipping this as not a path
        if execute_only_stage is not None and split != execute_only_stage:
            continue
        logger.info(f"Loading audio data split {split}: {str(path)}")
        assert (
            cfg.sample_rate <= max_sample_rate
        ), f"Expecting a max sample rate of {max_sample_rate} for datasource but {sample_rate} found."
        assert (
            cfg.channels <= max_channels
        ), f"Expecting a max number of channels of {max_channels} for datasource but {channels} found."

        split_cfg = splits_cfg[split]
        split_kwargs = {k: v for k, v in split_cfg.items()}
        kwargs = {**dataset_cfg, **split_kwargs}  # split kwargs overrides default dataset_cfg
        kwargs['sample_rate'] = sample_rate
        kwargs['channels'] = channels

        if kwargs.get('permutation_on_files') and cfg.optim.updates_per_epoch:
            kwargs['num_samples'] = (
                flashy.distrib.world_size() * cfg.dataset.batch_size * cfg.optim.updates_per_epoch)

        num_samples = kwargs['num_samples']
        shuffle = kwargs['shuffle']

        return_info = kwargs.pop('return_info')
        batch_size = kwargs.pop('batch_size', None)
        num_workers = kwargs.pop('num_workers')

        if dataset_type == DatasetType.MUSIC:
            dataset = data.music_dataset.MusicDataset.from_meta(path, **kwargs)
        elif dataset_type == DatasetType.SOUND:
            dataset = data.sound_dataset.SoundDataset.from_meta(path, **kwargs)
        elif dataset_type == DatasetType.AUDIO:
            dataset = data.info_audio_dataset.InfoAudioDataset.from_meta(path, return_info=return_info, **kwargs)
        else:
            raise ValueError(f"Dataset type is unsupported: {dataset_type}")

        loader = get_loader(
            dataset,
            num_samples,
            batch_size=batch_size,
            num_workers=num_workers,
            seed=seed,
            collate_fn=dataset.collater if return_info else None,
            shuffle=shuffle,
        )
        dataloaders[split] = loader

    return dataloaders
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_multibanddiffusion.py
BlockTypes.METHOD, TestMBD._create_mbd
def _create_mbd(self,
                    sample_rate: int,
                    channels: int,
                    n_filters: int = 3,
                    n_residual_layers: int = 1,
                    ratios: list = [5, 4, 3, 2],
                    num_steps: int = 1000,
                    codec_dim: int = 128,
                    **kwargs):
        frame_rate = np.prod(ratios)
        encoder = SEANetEncoder(channels=channels, dimension=codec_dim, n_filters=n_filters,
                                n_residual_layers=n_residual_layers, ratios=ratios)
        decoder = SEANetDecoder(channels=channels, dimension=codec_dim, n_filters=n_filters,
                                n_residual_layers=n_residual_layers, ratios=ratios)
        quantizer = DummyQuantizer()
        compression_model = EncodecModel(encoder, decoder, quantizer, frame_rate=frame_rate,
                                         sample_rate=sample_rate, channels=channels, **kwargs)
        diffusion_model = DiffusionUnet(chin=channels, num_steps=num_steps, codec_dim=codec_dim)
        schedule = NoiseSchedule(device='cpu', num_steps=num_steps)
        DP = DiffusionProcess(model=diffusion_model, noise_schedule=schedule)
        mbd = MultiBandDiffusion(DPs=[DP], codec_model=compression_model)
        return mbd
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\diffusion\_explorers.py
BlockTypes.METHOD, DiffusionExplorer.stages
def stages(self):
        return ["train", "valid", "valid_ema", "evaluate", "evaluate_ema"]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestDiscriminatorAdversarialLoss._disc_loss
def _disc_loss(self, loss_type: str, fake: torch.Tensor, real: torch.Tensor):
        disc_loss_real = get_real_criterion(loss_type)
        disc_loss_fake = get_fake_criterion(loss_type)

        loss = disc_loss_fake(fake) + disc_loss_real(real)
        return loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager._load_samples
def _load_samples(self):
        """Scan the sample folder and load existing samples."""
        jsons = self.base_folder.glob('**/*.json')
        with ThreadPoolExecutor(6) as pool:
            self.samples = list(pool.map(self._load_sample, jsons))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_discriminators.py
BlockTypes.METHOD, TestMultiScaleStftDiscriminator.test_msstftd_discriminator
def test_msstftd_discriminator(self):
        N, C, T = 2, 2, random.randrange(1, 100_000)
        t0 = torch.randn(N, C, T)

        n_filters = 4
        n_ffts = [128, 256, 64]
        hop_lengths = [32, 64, 16]
        win_lengths = [128, 256, 64]

        msstftd = MultiScaleSTFTDiscriminator(filters=n_filters, n_ffts=n_ffts, hop_lengths=hop_lengths,
                                              win_lengths=win_lengths, in_channels=C)
        logits, fmaps = msstftd(t0)

        assert len(logits) == len(n_ffts)
        assert len(fmaps) == len(n_ffts)
        assert all([logit.shape[0] == N and len(logit.shape) == 4 for logit in logits])
        assert all([feature.shape[0] == N for fmap in fmaps for feature in fmap])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestRead.test_read_partial_wav
def test_read_partial_wav(self):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 1.
        read_duration = torch.rand(1).item()
        for sample_rate, ch in product(sample_rates, channels):
            n_frames = int(sample_rate * duration)
            read_frames = int(sample_rate * read_duration)
            wav = get_white_noise(ch, n_frames).clamp(-0.99, 0.99)
            path = self.get_temp_path('sample_wav.wav')
            save_wav(path, wav, sample_rate)
            read_wav, read_sr = audio_read(path, 0, read_duration)
            assert read_sr == sample_rate
            assert read_wav.shape[0] == wav.shape[0]
            assert read_wav.shape[1] == read_frames
            assert torch.allclose(read_wav[..., 0:read_frames], wav[..., 0:read_frames], rtol=1e-03, atol=1e-04)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.decode_latent
def decode_latent(self, codes: torch.Tensor):
        """Decode from the discrete codes to continuous latent space."""
        return self.model.quantizer.from_codes(codes)[0]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.generate
def generate(self):
        """Generate stage."""
        self.model.eval()
        sample_manager = SampleManager(self.xp, map_reference_to_sample_id=True)
        generate_stage_name = str(self.current_stage)

        loader = self.dataloaders['generate']
        updates = len(loader)
        lp = self.log_progress(generate_stage_name, loader, total=updates, updates=self.log_updates)

        for batch in lp:
            reference, _ = batch
            reference = reference.to(self.device)
            with torch.no_grad():
                qres = self.model(reference)
            assert isinstance(qres, quantization.QuantizedResult)

            reference = reference.cpu()
            estimate = qres.x.cpu()
            sample_manager.add_samples(estimate, self.epoch, ground_truth_wavs=reference)

        flashy.distrib.barrier()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\quantization\test_vq.py
BlockTypes.METHOD, TestResidualVectorQuantizer.test_rvq
def test_rvq(self):
        x = torch.randn(1, 16, 2048)
        vq = ResidualVectorQuantizer(n_q=8, dimension=16, bins=8)
        res = vq(x, 1.)
        assert res.x.shape == torch.Size([1, 16, 2048])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\_explorers.py
BlockTypes.METHOD, LMExplorer.process_sheep
def process_sheep(self, sheep, history):
        parts = super().process_sheep(sheep, history)

        track_by = {'ppl': 'lower'}  # values should be in ['lower', 'higher']
        best_metrics = {k: (1 if v == 'lower' else -1) * float('inf') for k, v in track_by.items()}

        def comparator(mode, a, b):
            return a < b if mode == 'lower' else a > b

        for metrics in history:
            for key, sub in metrics.items():
                for metric in track_by:
                    # for the validation set, keep track of best metrics (ppl in this example)
                    # this is so we can conveniently compare metrics between runs in the grid
                    if key == 'valid' and metric in sub and comparator(
                        track_by[metric], sub[metric], best_metrics[metric]
                    ):
                        best_metrics[metric] = sub[metric]

        if 'valid' in parts:
            parts['valid'].update({f'best_{k}': v for k, v in best_metrics.items()})
        return parts
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cluster.py
BlockTypes.METHOD, get_slurm_parameters
def get_slurm_parameters(
    cfg: omegaconf.DictConfig, cluster_type: tp.Optional[ClusterType] = None
) -> omegaconf.DictConfig:
    """Update SLURM parameters in configuration based on cluster type.
    If the cluster type is not specify, it infers it automatically.
    """
    from ..environment import AudioCraftEnvironment
    cluster_type = get_cluster_type(cluster_type)
    # apply cluster-specific adjustments
    if cluster_type == ClusterType.AWS:
        cfg["mem_per_gpu"] = None
        cfg["constraint"] = None
        cfg["setup"] = []
    elif cluster_type == ClusterType.RSC:
        cfg["mem_per_gpu"] = None
        cfg["setup"] = []
        cfg["constraint"] = None
        cfg["partition"] = "learn"
    slurm_exclude = AudioCraftEnvironment.get_slurm_exclude()
    if slurm_exclude is not None:
        cfg["exclude"] = slurm_exclude
    return cfg
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_discriminators.py
BlockTypes.METHOD, TestMultiPeriodDiscriminator.test_mpd_discriminator
def test_mpd_discriminator(self):
        N, C, T = 2, 2, random.randrange(1, 100_000)
        t0 = torch.randn(N, C, T)
        periods = [1, 2, 3]
        mpd = MultiPeriodDiscriminator(periods=periods, in_channels=C)
        logits, fmaps = mpd(t0)

        assert len(logits) == len(periods)
        assert len(fmaps) == len(periods)
        assert all([logit.shape[0] == N and len(logit.shape) == 4 for logit in logits])
        assert all([feature.shape[0] == N for fmap in fmaps for feature in fmap])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_seanet.py
BlockTypes.METHOD, TestSEANetModel.test_base
def test_base(self):
        encoder = SEANetEncoder()
        decoder = SEANetDecoder()

        x = torch.randn(1, 1, 24000)
        z = encoder(x)
        assert list(z.shape) == [1, 128, 75], z.shape
        y = decoder(z)
        assert y.shape == x.shape, (x.shape, y.shape)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\activations.py
BlockTypes.METHOD, SwiGLU.__init__
def __init__(self, dim: int = -1):
        super(SwiGLU, self).__init__(nn.SiLU(), dim)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchLoader.__init__
def __init__(self, cache_folder: Path, batch_size: int,
                 num_workers: int = 10, min_length: int = 1):
        self.cache_folder = cache_folder
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.min_length = min_length
        self._current_epoch: tp.Optional[int] = None
        self.sampler = None  # for compatibility with the regular DataLoader
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\info_audio_dataset.py
BlockTypes.METHOD, get_keyword
def get_keyword(value: tp.Optional[str]) -> tp.Optional[str]:
    """Preprocess a single keyword."""
    if value is None or (not isinstance(value, str)) or len(value) == 0 or value == 'None':
        return None
    else:
        return value.strip().lower()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, DummyQuantizer.set_num_codebooks
def set_num_codebooks(self, n: int):
        """Set the number of active codebooks."""
        raise AttributeError("Cannot override the number of codebooks for the dummy quantizer")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.checkpoint_path_with_name
def checkpoint_path_with_name(self, name: str, **kwargs):
        kwargs.setdefault('use_fsdp', self.cfg.fsdp.use)
        return self.folder / checkpoint.checkpoint_name(name=name, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, with_rank_rng
def with_rank_rng(base_seed: int = 1234):
    """Decorator for a function so that the function will use a Random Number Generator
    whose state depend on the GPU rank. The original RNG state is restored upon returning.

    Args:
        base_seed (int): Random seed.
    """
    def _decorator(fun: tp.Callable):
        @wraps(fun)
        def _decorated(*args, **kwargs):
            state = torch.get_rng_state()
            seed = base_seed ^ flashy.distrib.rank()
            torch.manual_seed(seed)
            logger.debug('Rank dependent seed set to %d', seed)
            try:
                return fun(*args, **kwargs)
            finally:
                torch.set_rng_state(state)
                logger.debug('RNG state restored.')
        return _decorated
    return _decorator
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\diffusion\_explorers.py
BlockTypes.METHOD, DiffusionExplorer.get_grid_meta
def get_grid_meta(self):
        """Returns the list of Meta information to display for each XP/job.
        """
        return [
            tt.leaf("index", align=">"),
            tt.leaf("name", wrap=140),
            tt.leaf("state"),
            tt.leaf("sig", align=">"),
        ]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.get_eval_solver_from_sig
def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                                 device: tp.Optional[str] = None, autocast: bool = True,
                                 batch_size: tp.Optional[int] = None,
                                 override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                                 **kwargs):
        """Mostly a convenience function around magma.train.get_solver_from_sig,
        populating all the proper param, deactivating EMA, FSDP, loading the best state,
        basically all you need to get a solver ready to "play" with in single GPU mode
        and with minimal memory overhead.

        Args:
            sig (str): signature to load.
            dtype (str or None): potential dtype, as a string, i.e. 'float16'.
            device (str or None): potential device, as a string, i.e. 'cuda'.
            override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. 'cuda'.
        """
        from audiocraft import train
        our_override_cfg: tp.Dict[str, tp.Any] = {'optim': {'ema': {'use': False}}}
        our_override_cfg['autocast'] = autocast
        if dtype is not None:
            our_override_cfg['dtype'] = dtype
        if device is not None:
            our_override_cfg['device'] = device
        if batch_size is not None:
            our_override_cfg['dataset'] = {'batch_size': batch_size}
        if override_cfg is None:
            override_cfg = {}
        override_cfg = omegaconf.OmegaConf.merge(
            omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
        solver = train.get_solver_from_sig(
            sig, override_cfg=override_cfg,
            load_best=True, disable_fsdp=True,
            ignore_state_keys=['optimizer', 'ema'], **kwargs)
        solver.model.eval()
        return solver
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\clap_consistency.py
BlockTypes.METHOD, CLAPTextConsistencyMetric._initialize_model
def _initialize_model(self, model_path: tp.Union[str, Path], model_arch: str, enable_fusion: bool):
        model_path = AudioCraftEnvironment.resolve_reference_path(model_path)
        self.tokenize = RobertaTokenizer.from_pretrained('roberta-base')
        self.model = laion_clap.CLAP_Module(enable_fusion=enable_fusion, amodel=model_arch)
        self.model_sample_rate = 48_000
        load_clap_state_dict(self.model, model_path)
        self.model.eval()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.quantize
def quantize(self, x):
        embed = self.embed.t()
        dist = -(
            x.pow(2).sum(1, keepdim=True)
            - 2 * x @ embed
            + embed.pow(2).sum(0, keepdim=True)
        )
        embed_ind = dist.max(dim=-1).indices
        return embed_ind
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, _resolve_audio_meta
def _resolve_audio_meta(m: AudioMeta, fast: bool = True) -> AudioMeta:
    """If Dora is available as a dependency, try to resolve potential relative paths
    in list of AudioMeta. This method is expected to be used when loading meta from file.

    Args:
        m (AudioMeta): Audio meta to resolve.
        fast (bool): If True, uses a really fast check for determining if a file
            is already absolute or not. Only valid on Linux/Mac.
    Returns:
        AudioMeta: Audio meta with resolved path.
    """
    def is_abs(m):
        if fast:
            return str(m)[0] == '/'
        else:
            os.path.isabs(str(m))

    if not dora:
        return m

    if not is_abs(m.path):
        m.path = dora.git_save.to_absolute_path(m.path)
    if m.info_path is not None and not is_abs(m.info_path.zip_path):
        m.info_path.zip_path = dora.git_save.to_absolute_path(m.path)
    return m
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.channels
def channels(self) -> int:
        return self.model.config.audio_channels
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\checkpoint.py
BlockTypes.METHOD, check_sharded_checkpoint
def check_sharded_checkpoint(checkpoint_path: Path, rank0_checkpoint_path: Path) -> None:
    """Check sharded checkpoint state, ensuring the checkpoints are not corrupted."""
    # Finish the work of a previous run that got interrupted while dumping.
    old_path = Path(str(checkpoint_path) + '.old')
    if old_path.exists():
        raise RuntimeError(
            f"Old checkpoint {old_path} from previous version of this code exist, cannot safely proceed.")
    token = Path(str(rank0_checkpoint_path) + '.tmp.done')
    tmp_path = Path(str(checkpoint_path) + '.tmp')
    if token.exists():
        if tmp_path.exists():
            tmp_path.rename(checkpoint_path)
    flashy.distrib.barrier()
    if flashy.distrib.is_rank_zero() and token.exists():
        token.unlink()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, TestStreamableConv1d.get_streamable_conv1d_output_length
def get_streamable_conv1d_output_length(self, length, kernel_size, stride, dilation):
        # StreamableConv1d internally pads to make sure that the last window is full
        padding_total = (kernel_size - 1) * dilation - (stride - 1)
        n_frames = (length - kernel_size + padding_total) / stride + 1
        ideal_length = (math.ceil(n_frames) - 1) * stride + (kernel_size - padding_total)
        return ideal_length // stride
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, LUTConditioner.__init__
def __init__(self, n_bins: int, dim: int, output_dim: int, tokenizer: str, pad_idx: int = 0):
        super().__init__(dim, output_dim)
        self.embed = nn.Embedding(n_bins, dim)
        self.tokenizer: Tokenizer
        if tokenizer == 'whitespace':
            self.tokenizer = WhiteSpaceTokenizer(n_bins, pad_idx=pad_idx)
        elif tokenizer == 'noop':
            self.tokenizer = NoopTokenizer(n_bins, pad_idx=pad_idx)
        else:
            raise ValueError(f"unrecognized tokenizer `{tokenizer}`.")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\rope.py
BlockTypes.METHOD, RotaryEmbedding.rotate_qk
def rotate_qk(self, query: torch.Tensor, key: torch.Tensor, start: int = 0):
        """ Apply rope rotation to both query and key tensors.
        Supports streaming mode, in which query and key are not expected to have the same shape.
        In streaming mode, key will be of length [P + C] with P the cached past timesteps, but
        query will be [C] (typically C == 1).

        Args:
            query (torch.Tensor): Query to rotate.
            key (torch.Tensor): Key to rotate.
            start (int): Start index of the sequence for time offset.
        """
        query_timesteps = query.shape[1]
        key_timesteps = key.shape[1]
        streaming_offset = key_timesteps - query_timesteps

        query_out = self.rotate(query, start + streaming_offset)
        key_out = self.rotate(key, start, invert_decay=True)

        return query_out, key_out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.generate_with_chroma
def generate_with_chroma(self, descriptions: tp.List[str], melody_wavs: MelodyType,
                             melody_sample_rate: int, progress: bool = False,
                             return_tokens: bool = False) -> tp.Union[torch.Tensor,
                                                                      tp.Tuple[torch.Tensor, torch.Tensor]]:
        """Generate samples conditioned on text and melody.

        Args:
            descriptions (list of str): A list of strings used as text conditioning.
            melody_wavs: (torch.Tensor or list of Tensor): A batch of waveforms used as
                melody conditioning. Should have shape [B, C, T] with B matching the description length,
                C=1 or 2. It can be [C, T] if there is a single description. It can also be
                a list of [C, T] tensors.
            melody_sample_rate: (int): Sample rate of the melody waveforms.
            progress (bool, optional): Flag to display progress of the generation process. Defaults to False.
        """
        if isinstance(melody_wavs, torch.Tensor):
            if melody_wavs.dim() == 2:
                melody_wavs = melody_wavs[None]
            if melody_wavs.dim() != 3:
                raise ValueError("Melody wavs should have a shape [B, C, T].")
            melody_wavs = list(melody_wavs)
        else:
            for melody in melody_wavs:
                if melody is not None:
                    assert melody.dim() == 2, "One melody in the list has the wrong number of dims."

        melody_wavs = [
            convert_audio(wav, melody_sample_rate, self.sample_rate, self.audio_channels)
            if wav is not None else None
            for wav in melody_wavs]
        attributes, prompt_tokens = self._prepare_tokens_and_attributes(descriptions=descriptions, prompt=None,
                                                                        melody_wavs=melody_wavs)
        assert prompt_tokens is None
        tokens = self._generate_tokens(attributes, prompt_tokens, progress)
        if return_tokens:
            return self.generate_audio(tokens), tokens
        return self.generate_audio(tokens)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, KLDivergenceMetric._get_label_distribution
def _get_label_distribution(self, x: torch.Tensor, sizes: torch.Tensor,
                                sample_rates: torch.Tensor) -> tp.Optional[torch.Tensor]:
        """Get model output given provided input tensor.

        Args:
            x (torch.Tensor): Input audio tensor of shape [B, C, T].
            sizes (torch.Tensor): Actual audio sample length, of shape [B].
            sample_rates (torch.Tensor): Actual audio sample rate, of shape [B].
        Returns:
            probs (torch.Tensor): Probabilities over labels, of shape [B, num_classes].
        """
        raise NotImplementedError("implement method to extract label distributions from the model.")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, NoiseSchedule.__init__
def __init__(self, beta_t0: float = 1e-4, beta_t1: float = 0.02, num_steps: int = 1000, variance: str = 'beta',
                 clip: float = 5., rescale: float = 1., device='cuda', beta_exp: float = 1,
                 repartition: str = "power", alpha_sigmoid: dict = {}, n_bands: tp.Optional[int] = None,
                 sample_processor: SampleProcessor = SampleProcessor(), noise_scale: float = 1.0, **kwargs):

        self.beta_t0 = beta_t0
        self.beta_t1 = beta_t1
        self.variance = variance
        self.num_steps = num_steps
        self.clip = clip
        self.sample_processor = sample_processor
        self.rescale = rescale
        self.n_bands = n_bands
        self.noise_scale = noise_scale
        assert n_bands is None
        if repartition == "power":
            self.betas = torch.linspace(beta_t0 ** (1 / beta_exp), beta_t1 ** (1 / beta_exp), num_steps,
                                        device=device, dtype=torch.float) ** beta_exp
        else:
            raise RuntimeError('Not implemented')
        self.rng = random.Random(1234)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.channels
def channels(self) -> int:
        return 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, MultiBandProcessor.__init__
def __init__(self, n_bands: int = 8, sample_rate: float = 24_000,
                 num_samples: int = 10_000, power_std: tp.Union[float, tp.List[float], torch.Tensor] = 1.):
        super().__init__()
        self.n_bands = n_bands
        self.split_bands = julius.SplitBands(sample_rate, n_bands=n_bands)
        self.num_samples = num_samples
        self.power_std = power_std
        if isinstance(power_std, list):
            assert len(power_std) == n_bands
            power_std = torch.tensor(power_std)
        self.register_buffer('counts', torch.zeros(1))
        self.register_buffer('sum_x', torch.zeros(n_bands))
        self.register_buffer('sum_x2', torch.zeros(n_bands))
        self.register_buffer('sum_target_x2', torch.zeros(n_bands))
        self.counts: torch.Tensor
        self.sum_x: torch.Tensor
        self.sum_x2: torch.Tensor
        self.sum_target_x2: torch.Tensor
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen.set_generation_params
def set_generation_params(self, use_sampling: bool = True, top_k: int = 250,
                              top_p: float = 0.0, temperature: float = 1.0,
                              duration: float = 10.0, cfg_coef: float = 3.0,
                              two_step_cfg: bool = False, extend_stride: float = 2):
        """Set the generation parameters for AudioGen.

        Args:
            use_sampling (bool, optional): Use sampling if True, else do argmax decoding. Defaults to True.
            top_k (int, optional): top_k used for sampling. Defaults to 250.
            top_p (float, optional): top_p used for sampling, when set to 0 top_k is used. Defaults to 0.0.
            temperature (float, optional): Softmax temperature parameter. Defaults to 1.0.
            duration (float, optional): Duration of the generated waveform. Defaults to 10.0.
            cfg_coef (float, optional): Coefficient used for classifier free guidance. Defaults to 3.0.
            two_step_cfg (bool, optional): If True, performs 2 forward for Classifier Free Guidance,
                instead of batching together the two. This has some impact on how things
                are padded but seems to have little impact in practice.
            extend_stride: when doing extended generation (i.e. more than 10 seconds), by how much
                should we extend the audio each time. Larger values will mean less context is
                preserved, and shorter value will require extra computations.
        """
        assert extend_stride < self.max_duration, "Cannot stride by more than max generation duration."
        self.extend_stride = extend_stride
        self.duration = duration
        self.generation_params = {
            'use_sampling': use_sampling,
            'temp': temperature,
            'top_k': top_k,
            'top_p': top_p,
            'cfg_coef': cfg_coef,
            'two_step_cfg': two_step_cfg,
        }
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_utils.py
BlockTypes.METHOD, convert_audio
def convert_audio(wav: torch.Tensor, from_rate: float,
                  to_rate: float, to_channels: int) -> torch.Tensor:
    """Convert audio to new sample rate and number of audio channels."""
    wav = julius.resample_frac(wav, int(from_rate), int(to_rate))
    wav = convert_audio_channels(wav, to_channels)
    return wav
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric.reset
def reset(self, log_folder: tp.Optional[tp.Union[Path, str]] = None):
        """Reset torchmetrics.Metrics state."""
        log_folder = Path(log_folder or tempfile.mkdtemp())
        self.tmp_dir = log_folder / 'fad'
        self.tmp_dir.mkdir(exist_ok=True)
        self.samples_tests_dir = self.tmp_dir / 'tests'
        self.samples_tests_dir.mkdir(exist_ok=True)
        self.samples_background_dir = self.tmp_dir / 'background'
        self.samples_background_dir.mkdir(exist_ok=True)
        self.manifest_tests = self.tmp_dir / 'files_tests.cvs'
        self.manifest_background = self.tmp_dir / 'files_background.cvs'
        self.stats_tests_dir = self.tmp_dir / 'stats_tests'
        self.stats_background_dir = self.tmp_dir / 'stats_background'
        self.counter = 0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DataProcess.inverse_process
def inverse_process(self, x):
        """Upsampling only."""
        if self.use_resampling:
            x = julius.resample_frac(x, old_sr=self.target_sr, new_sr=self.target_sr)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestConvertAudioChannels.test_convert_audio_channels_upmix_error
def test_convert_audio_channels_upmix_error(self):
        b, c, t = 2, 2, 100
        audio = get_batch_white_noise(b, c, t)
        with pytest.raises(ValueError):
            convert_audio_channels(audio, channels=3)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.frame_rate
def frame_rate(self) -> float:
        hop_length = int(np.prod(self.model.config.upsampling_ratios))
        return self.sample_rate / hop_length
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\diffusion\_explorers.py
BlockTypes.METHOD, DiffusionExplorer.get_grid_metrics
def get_grid_metrics(self):
        """Return the metrics that should be displayed in the tracking table.
        """
        return [
            tt.group(
                "train",
                [
                    tt.leaf("epoch"),
                    tt.leaf("loss", ".3%"),
                ],
                align=">",
            ),
            tt.group(
                "valid",
                [
                    tt.leaf("loss", ".3%"),
                    # tt.leaf("loss_0", ".3%"),
                ],
                align=">",
            ),
            tt.group(
                "valid_ema",
                [
                    tt.leaf("loss", ".3%"),
                    # tt.leaf("loss_0", ".3%"),
                ],
                align=">",
            ),
            tt.group(
                "evaluate", [tt.leaf("rvm", ".4f"), tt.leaf("rvm_0", ".4f"),
                             tt.leaf("rvm_1", ".4f"), tt.leaf("rvm_2", ".4f"),
                             tt.leaf("rvm_3", ".4f"), ], align=">"
            ),
            tt.group(
                "evaluate_ema", [tt.leaf("rvm", ".4f"), tt.leaf("rvm_0", ".4f"),
                                 tt.leaf("rvm_1", ".4f"), tt.leaf("rvm_2", ".4f"),
                                 tt.leaf("rvm_3", ".4f")], align=">"
            ),
        ]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.save_checkpoints
def save_checkpoints(self):
        """Save checkpoint, optionally keeping a copy for a given epoch."""
        is_sharded = self.cfg.fsdp.use
        if not flashy.distrib.is_rank_zero() and not is_sharded:
            return
        self.logger.info("Model hash: %s", model_hash(self.model))
        state = self.state_dict()
        epoch = self.epoch - 1  # pushing metrics will increase the epoch in Flashy, so we do -1 here

        # save minimal state_dict as new checkpoint every X epoch
        if self.cfg.checkpoint.save_every:
            if epoch % self.cfg.checkpoint.save_every == 0:
                minimal_state = state
                if self.cfg.checkpoint.keep_every_states is not None and len(self.cfg.checkpoint.keep_every_states) > 0:
                    minimal_state = {
                        name: source for name, source in state.items()
                        if name in self.cfg.checkpoint.keep_every_states
                    }
                epoch_checkpoint_path = self.epoch_checkpoint_path(epoch)
                checkpoint.save_checkpoint(minimal_state, epoch_checkpoint_path, is_sharded)

        # save checkpoint as latest checkpoint
        if self.cfg.checkpoint.save_last:
            last_checkpoint_path = self.checkpoint_path()
            checkpoint.save_checkpoint(state, last_checkpoint_path, is_sharded)

        # flush any stale checkpoint to reduce disk footprint
        checkpoint.flush_stale_checkpoints(self.checkpoint_path())
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, is_clipped
def is_clipped(audio: torch.Tensor, clipping_threshold: float = 0.99) -> torch.Tensor:
    return (abs(audio) > clipping_threshold).any(1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ClassifierFreeGuidanceDropout.__repr__
def __repr__(self):
        return f"ClassifierFreeGuidanceDropout(p={self.p})"
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\rope.py
BlockTypes.METHOD, XPos.get_decay
def get_decay(self, start: int, end: int):
        """Create complex decay tensor, cache values for fast computation."""
        if self.decay is None or end > self.decay.shape[0]:
            assert isinstance(self.decay_rates, torch.Tensor)  # Satisfy type checker.
            idx = torch.arange(end, device=self.decay_rates.device, dtype=self.dtype)
            power = idx / self.base_scale
            scale = self.decay_rates ** power.unsqueeze(-1)
            self.decay = torch.polar(scale, torch.zeros_like(scale))
        return self.decay[start:end]  # [T, C/2]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, sample_top_k
def sample_top_k(probs: torch.Tensor, k: int) -> torch.Tensor:
    """Sample next token from top K values along the last dimension of the input probs tensor.

    Args:
        probs (torch.Tensor): Input probabilities with token candidates on the last dimension.
        k (int): The k in “top-k”.
    Returns:
        torch.Tensor: Sampled tokens.
    """
    top_k_value, _ = torch.topk(probs, k, dim=-1)
    min_value_top_k = top_k_value[..., [-1]]
    probs *= (probs >= min_value_top_k).float()
    probs.div_(probs.sum(dim=-1, keepdim=True))
    next_token = multinomial(probs, num_samples=1)
    return next_token
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestDiscriminatorAdversarialLoss.test_hinge_discriminator_adv_loss
def test_hinge_discriminator_adv_loss(self):
        loss_type = 'hinge'
        t0 = torch.FloatTensor([0.0, 0.0, 0.0])
        t1 = torch.FloatTensor([1.0, 2.0, 3.0])

        assert self._disc_loss(loss_type, t0, t0).item() == 2.0
        assert self._disc_loss(loss_type, t1, t1).item() == 3.0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\activations.py
BlockTypes.METHOD, GeGLU.__init__
def __init__(self, dim: int = -1):
        super(GeGLU, self).__init__(nn.GELU(), dim)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, BaseInfo._dict2fields
def _dict2fields(cls, dictionary: dict):
        return {
            field.name: dictionary[field.name]
            for field in fields(cls) if field.name in dictionary
        }
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.frame_rate
def frame_rate(self) -> float:
        return self.model.sample_rate / self.model.hop_length
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestAdversarialLoss.test_adversarial_single_multidiscriminator
def test_adversarial_single_multidiscriminator(self):
        adv = MultiScaleDiscriminator()
        optimizer = torch.optim.Adam(
            adv.parameters(),
            lr=1e-4,
        )
        loss, loss_real, loss_fake = get_adv_criterion('mse'), get_real_criterion('mse'), get_fake_criterion('mse')
        adv_loss = AdversarialLoss(adv, optimizer, loss, loss_real, loss_fake)

        B, C, T = 4, 1, random.randint(1000, 5000)
        real = torch.randn(B, C, T)
        fake = torch.randn(B, C, T)

        disc_loss = adv_loss.train_adv(fake, real)
        assert isinstance(disc_loss, torch.Tensor) and isinstance(disc_loss.item(), float)

        loss, loss_feat = adv_loss(fake, real)
        assert isinstance(loss, torch.Tensor) and isinstance(loss.item(), float)
        # we did not specify feature loss
        assert loss_feat.item() == 0.
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, MRSTFTLoss.__init__
def __init__(self, n_ffts: tp.Sequence[int] = [1024, 2048, 512], hop_lengths: tp.Sequence[int] = [120, 240, 50],
                 win_lengths: tp.Sequence[int] = [600, 1200, 240], window: str = "hann_window",
                 factor_sc: float = 0.1, factor_mag: float = 0.1,
                 normalized: bool = False, epsilon: float = torch.finfo(torch.float32).eps):
        super().__init__()
        assert len(n_ffts) == len(hop_lengths) == len(win_lengths)
        self.stft_losses = torch.nn.ModuleList()
        for fs, ss, wl in zip(n_ffts, hop_lengths, win_lengths):
            self.stft_losses += [STFTLosses(fs, ss, wl, window, normalized, epsilon)]
        self.factor_sc = factor_sc
        self.factor_mag = factor_mag
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, mix_samples
def mix_samples(wavs: torch.Tensor, infos: tp.List[SoundInfo], aug_p: float, mix_p: float,
                snr_low: int, snr_high: int, min_overlap: float):
    """Mix samples within a batch, summing the waveforms and concatenating the text infos.

    Args:
        wavs (torch.Tensor): Audio tensors of shape [B, C, T].
        infos (list[SoundInfo]): List of SoundInfo items corresponding to the audio.
        aug_p (float): Augmentation probability.
        mix_p (float): Proportion of items in the batch to mix (and merge) together.
        snr_low (int): Lowerbound for sampling SNR.
        snr_high (int): Upperbound for sampling SNR.
        min_overlap (float): Minimum overlap between mixed samples.
    Returns:
        tuple[torch.Tensor, list[SoundInfo]]: A tuple containing the mixed wavs
            and mixed SoundInfo for the given batch.
    """
    # no mixing to perform within the batch
    if mix_p == 0:
        return wavs, infos

    if random.uniform(0, 1) < aug_p:
        # perform all augmentations on waveforms as [B, T]
        # randomly picking pairs of audio to mix
        assert wavs.size(1) == 1, f"Mix samples requires monophonic audio but C={wavs.size(1)}"
        wavs = wavs.mean(dim=1, keepdim=False)
        B, T = wavs.shape
        k = int(mix_p * B)
        mixed_sources_idx = torch.randperm(B)[:k]
        mixed_targets_idx = torch.randperm(B)[:k]
        aug_wavs = snr_mix(
            wavs[mixed_sources_idx],
            wavs[mixed_targets_idx],
            snr_low,
            snr_high,
            min_overlap,
        )
        # mixing textual descriptions in metadata
        descriptions = [info.description for info in infos]
        aug_infos = []
        for i, j in zip(mixed_sources_idx, mixed_targets_idx):
            text = mix_text(descriptions[i], descriptions[j])
            m = replace(infos[i])
            m.description = text
            aug_infos.append(m)

        # back to [B, C, T]
        aug_wavs = aug_wavs.unsqueeze(1)
        assert aug_wavs.shape[0] > 0, "Samples mixing returned empty batch."
        assert aug_wavs.dim() == 3, f"Returned wav should be [B, C, T] but dim = {aug_wavs.dim()}"
        assert aug_wavs.shape[0] == len(aug_infos), "Mismatch between number of wavs and infos in the batch"

        return aug_wavs, aug_infos  # [B, C, T]
    else:
        # randomly pick samples in the batch to match
        # the batch size when performing audio mixing
        B, C, T = wavs.shape
        k = int(mix_p * B)
        wav_idx = torch.randperm(B)[:k]
        wavs = wavs[wav_idx]
        infos = [infos[i] for i in wav_idx]
        assert wavs.shape[0] == len(infos), "Mismatch between number of wavs and infos in the batch"

        return wavs, infos  # [B, C, T]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\losses\test_losses.py
BlockTypes.METHOD, test_mrstft_loss
def test_mrstft_loss():
    N, C, T = 2, 2, random.randrange(1000, 100_000)
    t1 = torch.randn(N, C, T)
    t2 = torch.randn(N, C, T)

    mrstft = MRSTFTLoss()
    loss = mrstft(t1, t2)

    assert isinstance(loss, torch.Tensor)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\specloss.py
BlockTypes.METHOD, MelSpectrogramL1Loss.__init__
def __init__(self, sample_rate: int, n_fft: int = 1024, hop_length: int = 256, win_length: int = 1024,
                 n_mels: int = 80, f_min: float = 0.0, f_max: tp.Optional[float] = None,
                 log: bool = True, normalized: bool = False, floor_level: float = 1e-5):
        super().__init__()
        self.l1 = torch.nn.L1Loss()
        self.melspec = MelSpectrogramWrapper(n_fft=n_fft, hop_length=hop_length, win_length=win_length,
                                             n_mels=n_mels, sample_rate=sample_rate, f_min=f_min, f_max=f_max,
                                             log=log, normalized=normalized, floor_level=floor_level)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\info_audio_dataset.py
BlockTypes.METHOD, get_keyword_list
def get_keyword_list(values: tp.Union[str, tp.List[str]]) -> tp.Optional[tp.List[str]]:
    """Preprocess a list of keywords."""
    if isinstance(values, str):
        values = [v.strip() for v in re.split(r'[,\s]', values)]
    elif isinstance(values, float) and math.isnan(values):
        values = []
    if not isinstance(values, list):
        logger.debug(f"Unexpected keyword list {values}")
        values = [str(values)]

    kws = [get_keyword(v) for v in values]
    kw_list = [k for k in kws if k is not None]
    if len(kw_list) == 0:
        return None
    else:
        return kw_list
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner.tokenize
def tokenize(self, x: JointEmbedCondition) -> JointEmbedCondition:
        # Trying to limit as much as possible sync points when the cache is warm.
        no_undefined_paths = all(p is not None for p in x.path)
        if self.wav_cache is not None and no_undefined_paths:
            assert all([p is not None for p in x.path]), "Cache requires all JointEmbedCondition paths to be provided"
            paths = [Path(p) for p in x.path if p is not None]
            self.wav_cache.populate_embed_cache(paths, x)
        if self.text_cache is not None and no_undefined_paths:
            assert all([p is not None for p in x.path]), "Cache requires all JointEmbedCondition paths to be provided"
            paths = [Path(p) for p in x.path if p is not None]
            self.text_cache.populate_embed_cache(paths, x)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, mix_pair
def mix_pair(src: torch.Tensor, dst: torch.Tensor, min_overlap: float) -> torch.Tensor:
    start = random.randint(0, int(src.shape[1] * (1 - min_overlap)))
    remainder = src.shape[1] - start
    if dst.shape[1] > remainder:
        src[:, start:] = src[:, start:] + dst[:, :remainder]
    else:
        src[:, start:start+dst.shape[1]] = src[:, start:start+dst.shape[1]] + dst
    return src
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\zip.py
BlockTypes.METHOD, PathInZip.__init__
def __init__(self, path: str) -> None:
        split_path = path.split(self.INFO_PATH_SEP)
        assert len(split_path) == 2
        self.zip_path, self.file_path = split_path
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_rope.py
BlockTypes.METHOD, test_rope_with_xpos
def test_rope_with_xpos():
    set_efficient_attention_backend('xformers')
    B, T, H, C = 8, 75, 16, 128

    rope = RotaryEmbedding(dim=C, xpos=True)
    xq = torch.rand((B, T, H, C))
    xk = torch.rand((B, T, H, C))
    xq_out, xk_out = rope.rotate_qk(xq, xk, start=7)

    assert list(xq_out.shape) == [B, T, H, C]
    assert list(xk_out.shape) == [B, T, H, C]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, EncoderLayer.forward
def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, C, T = x.shape
        stride, = self.conv.stride
        pad = (stride - (T % stride)) % stride
        x = F.pad(x, (0, pad))

        x = self.conv(x)
        x = self.norm(x)
        x = self.activation(x)
        x = self.res_blocks(x)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\streaming.py
BlockTypes.METHOD, StreamingModule.set_streaming_state
def set_streaming_state(self, state: State):
        """Set the streaming state, including that of sub-modules."""
        state = dict(state)

        def _set(name: str, module: StreamingModule):
            if name:
                name += "."
            module._streaming_state.clear()
            for key, value in list(state.items()):
                # complexity is not ideal here, but probably fine.
                if key.startswith(name):
                    local_key = key[len(name):]
                    if '.' not in local_key:
                        module._streaming_state[local_key] = value
                        del state[key]

        self._apply_named_streaming(_set)
        assert len(state) == 0, list(state.keys())
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingTransformer.make_optim_group
def make_optim_group(self):
        group = {"params": list(self.parameters())}
        if self.lr is not None:
            group["lr"] = self.lr
        if self.weight_decay is not None:
            group["weight_decay"] = self.weight_decay
        return group
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, TestStreamableConv1d.test_streamable_conv1d
def test_streamable_conv1d(self):
        N, C, T = 2, 2, random.randrange(1, 100_000)
        t0 = torch.randn(N, C, T)
        C_out = 1

        # conv params are [(kernel_size, stride, dilation)]
        conv_params = [(4, 1, 1), (4, 2, 1), (3, 1, 3), (10, 5, 1), (3, 2, 3)]
        for causal, (kernel_size, stride, dilation) in product([False, True], conv_params):
            expected_out_length = self.get_streamable_conv1d_output_length(T, kernel_size, stride, dilation)
            sconv = StreamableConv1d(C, C_out, kernel_size=kernel_size, stride=stride, dilation=dilation, causal=causal)
            out = sconv(t0)
            assert isinstance(out, torch.Tensor)
            print(list(out.shape), [N, C_out, expected_out_length])
            assert list(out.shape) == [N, C_out, expected_out_length]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.sample_rate
def sample_rate(self) -> int:
        return self.model.config.sampling_rate
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.postprocess_emb
def postprocess_emb(self, embed_ind, shape):
        return embed_ind.view(*shape[:-1])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager._init_hash
def _init_hash(self):
        return hashlib.sha1()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, toggle_audio_src
def toggle_audio_src(choice):
    if choice == "mic":
        return gr.update(source="microphone", value=None, label="Microphone")
    else:
        return gr.update(source="upload", value=None, label="File")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, BaseInfo.from_dict
def from_dict(cls, dictionary: dict):
        _dictionary = cls._dict2fields(dictionary)
        return cls(**_dictionary)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.sample_rate
def sample_rate(self) -> int:
        return self.model.sample_rate
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchLoader.__len__
def __len__(self):
        path = CachedBatchWriter._get_zip_path(self.cache_folder, self._current_epoch or 0, 0).parent
        return len([p for p in path.iterdir() if p.suffix == ".zip"])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, KLDivergenceMetric.update
def update(self, preds: torch.Tensor, targets: torch.Tensor,
               sizes: torch.Tensor, sample_rates: torch.Tensor) -> None:
        """Calculates running KL-Divergence loss between batches of audio
        preds (generated) and target (ground-truth)
        Args:
            preds (torch.Tensor): Audio samples to evaluate, of shape [B, C, T].
            targets (torch.Tensor): Target samples to compare against, of shape [B, C, T].
            sizes (torch.Tensor): Actual audio sample length, of shape [B].
            sample_rates (torch.Tensor): Actual audio sample rate, of shape [B].
        """
        assert preds.shape == targets.shape
        assert preds.size(0) > 0, "Cannot update the loss with empty tensors"
        preds_probs = self._get_label_distribution(preds, sizes, sample_rates)
        targets_probs = self._get_label_distribution(targets, sizes, sample_rates)
        if preds_probs is not None and targets_probs is not None:
            assert preds_probs.shape == targets_probs.shape
            kld_scores = kl_divergence(preds_probs, targets_probs)
            assert not torch.isnan(kld_scores).any(), "kld_scores contains NaN value(s)!"
            self.kld_pq_sum += torch.sum(kld_scores)
            kld_qp_scores = kl_divergence(targets_probs, preds_probs)
            self.kld_qp_sum += torch.sum(kld_qp_scores)
            self.weight += torch.tensor(kld_scores.size(0))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, CodebooksPatternProvider.__init__
def __init__(self, n_q: int, cached: bool = True):
        assert n_q > 0
        self.n_q = n_q
        self.get_pattern = lru_cache(100)(self.get_pattern)  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, VALLEPattern.__init__
def __init__(self, n_q: int, delays: tp.Optional[tp.List[int]] = None):
        super().__init__(n_q)
        if delays is None:
            delays = [0] * (n_q - 1)
        self.delays = delays
        assert len(self.delays) == self.n_q - 1
        assert sorted(self.delays) == self.delays
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestDiscriminatorAdversarialLoss.test_mse_discriminator_adv_loss
def test_mse_discriminator_adv_loss(self):
        loss_type = 'mse'

        t0 = torch.FloatTensor([0.0, 0.0, 0.0])
        t1 = torch.FloatTensor([1.0, 1.0, 1.0])

        assert self._disc_loss(loss_type, t0, t0).item() == 1.0
        assert self._disc_loss(loss_type, t1, t0).item() == 2.0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_utils.py
BlockTypes.METHOD, normalize_loudness
def normalize_loudness(wav: torch.Tensor, sample_rate: int, loudness_headroom_db: float = 14,
                       loudness_compressor: bool = False, energy_floor: float = 2e-3):
    """Normalize an input signal to a user loudness in dB LKFS.
    Audio loudness is defined according to the ITU-R BS.1770-4 recommendation.

    Args:
        wav (torch.Tensor): Input multichannel audio data.
        sample_rate (int): Sample rate.
        loudness_headroom_db (float): Target loudness of the output in dB LUFS.
        loudness_compressor (bool): Uses tanh for soft clipping.
        energy_floor (float): anything below that RMS level will not be rescaled.
    Returns:
        torch.Tensor: Loudness normalized output data.
    """
    energy = wav.pow(2).mean().sqrt().item()
    if energy < energy_floor:
        return wav
    transform = torchaudio.transforms.Loudness(sample_rate)
    input_loudness_db = transform(wav).item()
    # calculate the gain needed to scale to the desired loudness level
    delta_loudness = -loudness_headroom_db - input_loudness_db
    gain = 10.0 ** (delta_loudness / 20.0)
    output = gain * wav
    if loudness_compressor:
        output = torch.tanh(output)
    assert output.isfinite().all(), (input_loudness_db, wav.pow(2).mean().sqrt())
    return output
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider.__init__
def __init__(self, conditioners: tp.Dict[str, BaseConditioner], device: tp.Union[torch.device, str] = "cpu"):
        super().__init__()
        self.device = device
        self.conditioners = nn.ModuleDict(conditioners)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\train.py
BlockTypes.METHOD, main
def main(cfg):
    init_seed_and_system(cfg)

    # Setup logging both to XP specific folder, and to stderr.
    log_name = '%s.log.{rank}' % cfg.execute_only if cfg.execute_only else 'solver.log.{rank}'
    flashy.setup_logging(level=str(cfg.logging.level).upper(), log_name=log_name)
    # Initialize distributed training, no need to specify anything when using Dora.
    flashy.distrib.init()
    solver = get_solver(cfg)
    if cfg.show:
        solver.show()
        return

    if cfg.execute_only:
        assert cfg.execute_inplace or cfg.continue_from is not None, \
            "Please explicitly specify the checkpoint to continue from with continue_from=<sig_or_path> " + \
            "when running with execute_only or set execute_inplace to True."
        solver.restore(replay_metrics=False)  # load checkpoint
        solver.run_one_stage(cfg.execute_only)
        return

    return solver.run()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_debug_lm_model
def get_debug_lm_model(device='cpu'):
    """Instantiate a debug LM to be used for unit tests."""
    pattern = DelayedPatternProvider(n_q=4)
    dim = 16
    providers = {
        'description': LUTConditioner(n_bins=128, dim=dim, output_dim=dim, tokenizer="whitespace"),
    }
    condition_provider = ConditioningProvider(providers)
    fuser = ConditionFuser(
        {'cross': ['description'], 'prepend': [],
         'sum': [], 'input_interpolate': []})
    lm = LMModel(
        pattern, condition_provider, fuser,
        n_q=4, card=400, dim=dim, num_heads=4, custom=True, num_layers=2,
        cross_attention=True, causal=True)
    return lm.to(device).eval()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.cardinality
def cardinality(self) -> int:
        return self.model.config.codebook_size
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, JointEmbeddingConditioner._get_embed
def _get_embed(self, x: JointEmbedCondition) -> tp.Tuple[torch.Tensor, torch.Tensor]:
        """Get joint embedding in latent space from the inputs.

        Returns:
            tuple[torch.Tensor, torch.Tensor]: Tensor for the latent embedding
                and corresponding empty indexes.
        """
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msstftd.py
BlockTypes.METHOD, DiscriminatorSTFT.forward
def forward(self, x: torch.Tensor):
        fmap = []
        z = self.spec_transform(x)  # [B, 2, Freq, Frames, 2]
        z = torch.cat([z.real, z.imag], dim=1)
        z = rearrange(z, 'b c w t -> b c t w')
        for i, layer in enumerate(self.convs):
            z = layer(z)
            z = self.activation(z)
            fmap.append(z)
        z = self.conv_post(z)
        return z, fmap
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._get_wav_embedding_for_cache
def _get_wav_embedding_for_cache(self, path: tp.Union[str, Path],
                                     x: JointEmbedCondition, idx: int) -> torch.Tensor:
        """Compute audio wave embedding for the cache.
        The embedding is computed on a given audio read from file.

        Args:
            path (str or Path): Path to the full audio file.
        Returns:
            torch.Tensor: Single-item tensor of shape [F, D], F being the number of chunks, D the dimension.
        """
        wav, sr = audio_read(path)  # [C, T]
        wav = wav.unsqueeze(0).to(self.device)  # [1, C, T]
        wav_len = torch.LongTensor([wav.shape[-1]]).to(self.device)
        embed = self._compute_wav_embedding(wav, wav_len, [sr], reduce_mean=False)  # [B, F, D]
        return embed.squeeze(0)  # [F, D]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.dequantize
def dequantize(self, embed_ind):
        quantize = F.embedding(embed_ind, self.embed)
        return quantize
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager._get_tensor_id
def _get_tensor_id(self, tensor: torch.Tensor) -> str:
        hash_id = self._init_hash()
        hash_id.update(tensor.numpy().data)
        return hash_id.hexdigest()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.cardinality
def cardinality(self) -> int:
        return self.model.codebook_size
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._get_chroma_len
def _get_chroma_len(self) -> int:
        """Get length of chroma during training."""
        dummy_wav = torch.zeros((1, int(self.sample_rate * self.duration)), device=self.device)
        dummy_chr = self.chroma(dummy_wav)
        return dummy_chr.shape[1]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestAvRead.test_avread_seek_edge
def test_avread_seek_edge(self):
        sample_rates = [8000, 16_000]
        # some of these values will have
        # int(((frames - 1) / sample_rate) * sample_rate) != (frames - 1)
        n_frames = [1000, 1001, 1002]
        channels = [1, 2]
        for sample_rate, ch, frames in product(sample_rates, channels, n_frames):
            duration = frames / sample_rate
            wav = get_white_noise(ch, frames)
            path = self.get_temp_path(f'reference_d_{sample_rate}_{ch}.wav')
            save_wav(path, wav, sample_rate)
            seek_time = (frames - 1) / sample_rate
            seek_frames = int(seek_time * sample_rate)
            read_wav, read_sr = _av_read(path, seek_time, duration)
            assert read_sr == sample_rate
            assert read_wav.shape[0] == wav.shape[0]
            assert read_wav.shape[-1] == (frames - seek_frames)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\activations.py
BlockTypes.METHOD, ReGLU.__init__
def __init__(self, dim: int = -1):
        super(ReGLU, self).__init__(nn.ReLU(), dim)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_seanet.py
BlockTypes.METHOD, TestSEANetModel.test_causal
def test_causal(self):
        encoder = SEANetEncoder(causal=True)
        decoder = SEANetDecoder(causal=True)
        x = torch.randn(1, 1, 24000)

        z = encoder(x)
        assert list(z.shape) == [1, 128, 75], z.shape
        y = decoder(z)
        assert y.shape == x.shape, (x.shape, y.shape)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, nullify_wav
def nullify_wav(cond: WavCondition) -> WavCondition:
    """Transform a WavCondition to a nullified WavCondition.
    It replaces the wav by a null tensor, forces its length to 0, and replaces metadata by dummy attributes.

    Args:
        cond (WavCondition): Wav condition with wav, tensor of shape [B, T].
    Returns:
        WavCondition: Nullified wav condition.
    """
    null_wav, _ = nullify_condition((cond.wav, torch.zeros_like(cond.wav)), dim=cond.wav.dim() - 1)
    return WavCondition(
        wav=null_wav,
        length=torch.tensor([0] * cond.wav.shape[0], device=cond.wav.device),
        sample_rate=cond.sample_rate,
        path=[None] * cond.wav.shape[0],
        seek_time=[None] * cond.wav.shape[0],
    )
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioMeta.test_load_audio_meta
def test_load_audio_meta(self):
        try:
            import dora
        except ImportError:
            dora = None  # type: ignore

        audio_meta = [
            AudioMeta("mypath1", 1., 16_000, None, None, PathInZip('/foo/bar.zip:/relative/file1.json')),
            AudioMeta("mypath2", 2., 16_000, None, None, PathInZip('/foo/bar.zip:/relative/file2.json'))
            ]
        empty_meta = []
        for idx, meta in enumerate([audio_meta, empty_meta]):
            path = self.get_temp_path(f'data_{idx}_load.jsonl')
            with open(path, 'w') as f:
                for m in meta:
                    json_str = json.dumps(m.to_dict()) + '\n'
                    f.write(json_str)
            read_meta = load_audio_meta(path)
            assert len(read_meta) == len(meta)
            for m, read_m in zip(meta, read_meta):
                if dora:
                    m.path = dora.git_save.to_absolute_path(m.path)
                assert m == read_m, f'original={m}, read={read_m}'
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.set_num_codebooks
def set_num_codebooks(self, n: int):
        """Set the active number of codebooks used by the quantizer."""
        self.quantizer.set_num_codebooks(n)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_rope.py
BlockTypes.METHOD, test_positional_scale
def test_positional_scale():
    set_efficient_attention_backend('xformers')
    B, T, H, C = 8, 75, 16, 128

    rope = RotaryEmbedding(dim=C, xpos=True, scale=0.0)
    xq = torch.rand((B, T, H, C))
    xk = torch.rand((B, T, H, C))
    xq_out, xk_out = rope.rotate_qk(xq, xk, start=7)

    assert torch.allclose(xq, xq_out)
    assert torch.allclose(xk, xk_out)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, BaseInfo.to_dict
def to_dict(self):
        return {
            field.name: self.__getattribute__(field.name)
            for field in fields(self)
            }
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\specloss.py
BlockTypes.METHOD, MelSpectrogramL1Loss.forward
def forward(self, x, y):
        self.melspec.to(x.device)
        s_x = self.melspec(x)
        s_y = self.melspec(y)
        return self.l1(s_x, s_y)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\visqol.py
BlockTypes.METHOD, ViSQOL.__call__
def __call__(
        self,
        ref_sig: torch.Tensor,
        deg_sig: torch.Tensor,
        sr: int,
        pad_with_silence: bool = False,
    ):
        """Calculate the ViSQOL metric for a pair of audio signals at a given sample rate.
        Args:
            ref_sig (torch.Tensor): Reference signals as [B, C, T].
            deg_sig (torch.Tensor): Degraded signals as [B, C, T].
            sr (int): Sample rate of the two audio signals.
            pad_with_silence (bool): Whether to pad the file with silences as recommended
                in visqol guidelines (see: https://github.com/google/visqol#general-guidelines-for-input).
        Returns:
            float: The ViSQOL score or mean score for the batch.
        """
        logger.debug(f"Calculating visqol with mode={self.visqol_mode} on {len(ref_sig)} samples")
        tmp_dir, input_csv, results_csv, debug_json = self._prepare_files(
            ref_sig, deg_sig, sr, self.target_sr, pad_with_silence
        )
        try:
            if input_csv and results_csv:
                self._run_visqol(
                    input_csv,
                    results_csv,
                    debug_json if self.debug else None,
                )
                mosqol = self._collect_moslqo_score(results_csv)
                return mosqol
            else:
                raise RuntimeError("Something unexpected happened when running VISQOL!")
        except Exception as e:
            logger.error("Exception occurred when running ViSQOL: %s", e)
        finally:
            self._flush_files(tmp_dir)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, LUTConditioner.tokenize
def tokenize(self, x: tp.List[tp.Optional[str]]) -> tp.Tuple[torch.Tensor, torch.Tensor]:
        device = self.embed.weight.device
        tokens, mask = self.tokenizer(x)
        tokens, mask = tokens.to(device), mask.to(device)
        return tokens, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\vq.py
BlockTypes.METHOD, ResidualVectorQuantizer.forward
def forward(self, x: torch.Tensor, frame_rate: int):
        n_q = self.n_q
        if self.training and self.q_dropout:
            n_q = int(torch.randint(1, self.n_q + 1, (1,)).item())
        bw_per_q = math.log2(self.bins) * frame_rate / 1000
        quantized, codes, commit_loss = self.vq(x, n_q=n_q)
        codes = codes.transpose(0, 1)
        # codes is [B, K, T], with T frames, K nb of codebooks.
        bw = torch.tensor(n_q * bw_per_q).to(x)
        return QuantizedResult(quantized, codes, bw, penalty=torch.mean(commit_loss))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, ui_batched
def ui_batched(launch_kwargs):
    with gr.Blocks() as demo:
        gr.Markdown(
            """
            # MusicGen

            This is the demo for [MusicGen](https://github.com/facebookresearch/audiocraft),
            a simple and controllable model for music generation
            presented at: ["Simple and Controllable Music Generation"](https://huggingface.co/papers/2306.05284).
            <br/>
            <a href="https://huggingface.co/spaces/facebook/MusicGen?duplicate=true"
                style="display: inline-block;margin-top: .5em;margin-right: .25em;" target="_blank">
            <img style="margin-bottom: 0em;display: inline;margin-top: -.25em;"
                src="https://bit.ly/3gLdBN6" alt="Duplicate Space"></a>
            for longer sequences, more control and no queue.</p>
            """
        )
        with gr.Row():
            with gr.Column():
                with gr.Row():
                    text = gr.Text(label="Describe your music", lines=2, interactive=True)
                    with gr.Column():
                        radio = gr.Radio(["file", "mic"], value="file",
                                         label="Condition on a melody (optional) File or Mic")
                        melody = gr.Audio(source="upload", type="numpy", label="File",
                                          interactive=True, elem_id="melody-input")
                with gr.Row():
                    submit = gr.Button("Generate")
            with gr.Column():
                output = gr.Video(label="Generated Music")
                audio_output = gr.Audio(label="Generated Music (wav)", type='filepath')
        submit.click(predict_batched, inputs=[text, melody],
                     outputs=[output, audio_output], batch=True, max_batch_size=MAX_BATCH_SIZE)
        radio.change(toggle_audio_src, radio, [melody], queue=False, show_progress=False)
        gr.Examples(
            fn=predict_batched,
            examples=[
                [
                    "An 80s driving pop song with heavy drums and synth pads in the background",
                    "./assets/bach.mp3",
                ],
                [
                    "A cheerful country song with acoustic guitars",
                    "./assets/bolero_ravel.mp3",
                ],
                [
                    "90s rock song with electric guitar and heavy drums",
                    None,
                ],
                [
                    "a light and cheerly EDM track, with syncopated drums, aery pads, and strong emotions bpm: 130",
                    "./assets/bach.mp3",
                ],
                [
                    "lofi slow bpm electro chill with organic samples",
                    None,
                ],
            ],
            inputs=[text, melody],
            outputs=[output]
        )
        gr.Markdown("""
        ### More details

        The model will generate 12 seconds of audio based on the description you provided.
        You can optionally provide a reference audio from which a broad melody will be extracted.
        The model will then try to follow both the description and melody provided.
        All samples are generated with the `melody` model.

        You can also use your own GPU or a Google Colab by following the instructions on our repo.

        See [github.com/facebookresearch/audiocraft](https://github.com/facebookresearch/audiocraft)
        for more details.
        """)

        demo.queue(max_size=8 * 4).launch(**launch_kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.num_codebooks
def num_codebooks(self) -> int:
        return self._num_codebooks
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.get_steps_with_timestep
def get_steps_with_timestep(self, t: int, q: tp.Optional[int] = None) -> tp.List[int]:
        return [step for step, coords in self.get_sequence_coords_with_timestep(t, q)]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\zip.py
BlockTypes.METHOD, PathInZip.from_paths
def from_paths(cls, zip_path: str, file_path: str):
        return cls(zip_path + cls.INFO_PATH_SEP + file_path)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.num_codebooks
def num_codebooks(self) -> int:
        return self.n_quantizers
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchWriter.__init__
def __init__(self, cache_folder: Path):
        self.cache_folder = cache_folder
        self._current_epoch: tp.Optional[int] = None
        self._current_index = 0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchLoader.start_epoch
def start_epoch(self, epoch: int):
        """Call at the beginning of each epoch.
        """
        self._current_epoch = epoch
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.__init__
def __init__(self,
                 encoder: nn.Module,
                 decoder: nn.Module,
                 quantizer: qt.BaseQuantizer,
                 frame_rate: int,
                 sample_rate: int,
                 channels: int,
                 causal: bool = False,
                 renormalize: bool = False):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.quantizer = quantizer
        self.frame_rate = frame_rate
        self.sample_rate = sample_rate
        self.channels = channels
        self.renormalize = renormalize
        self.causal = causal
        if self.causal:
            # we force disabling here to avoid handling linear overlap of segments
            # as supported in original EnCodec codebase.
            assert not self.renormalize, 'Causal model does not support renormalize'
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\export.py
BlockTypes.METHOD, export_encodec
def export_encodec(checkpoint_path: tp.Union[Path, str], out_file: tp.Union[Path, str]):
    """Export only the best state from the given EnCodec checkpoint. This
    should be used if you trained your own EnCodec model.
    """
    pkg = torch.load(checkpoint_path, 'cpu')
    new_pkg = {
        'best_state': pkg['best_state']['model'],
        'xp.cfg': OmegaConf.to_yaml(pkg['xp.cfg']),
        'version': __version__,
        'exported': True,
    }
    Path(out_file).parent.mkdir(exist_ok=True, parents=True)
    torch.save(new_pkg, out_file)
    return out_file
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, toggle_diffusion
def toggle_diffusion(choice):
    if choice == "MultiBand_Diffusion":
        return [gr.update(visible=True)] * 2
    else:
        return [gr.update(visible=False)] * 2
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, JointEmbeddingConditioner.forward
def forward(self, x: JointEmbedCondition) -> ConditionType:
        with self.autocast:
            embed, empty_idx = self._get_embed(x)
            if self.quantizer is not None:
                embed = embed.view(-1, self.dim, 1)
                q_res = self.quantizer(embed, frame_rate=1)
                out_embed = q_res.x.view(-1, self.dim)
            else:
                out_embed = embed
            out_embed = self.output_proj(out_embed).view(-1, 1, self.output_dim)
            mask = torch.ones(*out_embed.shape[:2], device=out_embed.device)
            mask[empty_idx, :] = 0  # zero-out index where the input is non-existant
            out_embed = (out_embed * mask.unsqueeze(-1))
            return out_embed, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_multibanddiffusion.py
BlockTypes.METHOD, TestMBD.test_model
def test_model(self):
        random.seed(1234)
        sample_rate = 24_000
        channels = 1
        codec_dim = 128
        mbd = self._create_mbd(sample_rate=sample_rate, channels=channels, codec_dim=codec_dim)
        for _ in range(10):
            length = random.randrange(1, 10_000)
            x = torch.randn(2, channels, length)
            res = mbd.regenerate(x, sample_rate)
            assert res.shape == x.shape
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\clap_consistency.py
BlockTypes.METHOD, CLAPTextConsistencyMetric._tokenizer
def _tokenizer(self, texts: tp.Union[str, tp.List[str]]) -> dict:
        # we use the default params from CLAP module here as well
        return self.tokenize(texts, padding="max_length", truncation=True, max_length=77, return_tensors="pt")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.encode
def encode(self, x):
        shape = x.shape
        # pre-process
        x = self.preprocess(x)
        # quantize
        embed_ind = self.quantize(x)
        # post-process
        embed_ind = self.postprocess_emb(embed_ind, shape)
        return embed_ind
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\resample_dataset.py
BlockTypes.METHOD, read_txt_files
def read_txt_files(path: tp.Union[str, Path]):
    with open(args.files_path) as f:
        lines = [line.rstrip() for line in f]
        print(f"Read {len(lines)} in .txt")
        lines = [line for line in lines if Path(line).suffix not in ['.json', '.txt', '.csv']]
        print(f"Filtered and keep {len(lines)} from .txt")
        return lines
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\mpd.py
BlockTypes.METHOD, MultiPeriodDiscriminator.num_discriminators
def num_discriminators(self):
        return len(self.discriminators)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen.set_custom_progress_callback
def set_custom_progress_callback(self, progress_callback: tp.Optional[tp.Callable[[int, int], None]] = None):
        """Override the default progress callback."""
        self._progress_callback = progress_callback
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_wrapped_compression_model
def get_wrapped_compression_model(
        compression_model: CompressionModel,
        cfg: omegaconf.DictConfig) -> CompressionModel:
    # more to come.
    return compression_model
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\activations.py
BlockTypes.METHOD, get_activation_fn
def get_activation_fn(
    activation: Union[str, Callable[[Tensor], Tensor]]
) -> Union[str, Callable[[Tensor], Tensor]]:
    """Helper function to map an activation string to the activation class.
    If the supplied activation is not a string that is recognized, the activation is passed back.

    Args:
        activation (str, or Callable[[Tensor], Tensor]): Activation to check
    """
    if isinstance(activation, str):
        if activation == "reglu":
            return ReGLU()
        elif activation == "geglu":
            return GeGLU()
        elif activation == "swiglu":
            return SwiGLU()
    return activation
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider.joint_embed_conditions
def joint_embed_conditions(self):
        return [m.attribute for m in self.conditioners.values() if isinstance(m, JointEmbeddingConditioner)]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.run
def run(self):
        """Training loop."""
        assert len(self.state_dict()) > 0
        self.restore(replay_metrics=True)  # load checkpoint and replay history
        self.log_hyperparams(dict_from_config(self.cfg))
        for epoch in range(self.epoch, self.cfg.optim.epochs + 1):
            if self.should_stop_training():
                return
            self.run_epoch()
            # Commit will send the metrics to Dora and save checkpoints by default.
            self.commit()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.total_codebooks
def total_codebooks(self) -> int:
        return max(self.possible_num_codebooks)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager._get_sample_id
def _get_sample_id(self, index: int, prompt_wav: tp.Optional[torch.Tensor],
                       conditions: tp.Optional[tp.Dict[str, str]]) -> str:
        """Computes an id for a sample given its input data.
        This id is deterministic if prompt and/or conditions are provided by using a sha1 hash on the input.
        Otherwise, a random id of the form "noinput_{uuid4().hex}" is returned.

        Args:
            index (int): Batch index, Helpful to differentiate samples from the same batch.
            prompt_wav (torch.Tensor): Prompt used during generation.
            conditions (dict[str, str]): Conditioning used during generation.
        """
        # For totally unconditioned generations we will just use a random UUID.
        # The function get_samples_for_xps will do a simple ordered match with a custom key.
        if prompt_wav is None and not conditions:
            return f"noinput_{uuid.uuid4().hex}"

        # Human readable portion
        hr_label = ""
        # Create a deterministic id using hashing
        hash_id = self._init_hash()
        hash_id.update(f"{index}".encode())
        if prompt_wav is not None:
            hash_id.update(prompt_wav.numpy().data)
            hr_label += "_prompted"
        else:
            hr_label += "_unprompted"
        if conditions:
            encoded_json = json.dumps(conditions, sort_keys=True).encode()
            hash_id.update(encoded_json)
            cond_str = "-".join([f"{key}={slugify(value)}"
                                 for key, value in sorted(conditions.items())])
            cond_str = cond_str[:100]  # some raw text might be too long to be a valid filename
            cond_str = cond_str if len(cond_str) > 0 else "unconditioned"
            hr_label += f"_{cond_str}"
        else:
            hr_label += "_unconditioned"

        return hash_id.hexdigest() + hr_label
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\zip.py
BlockTypes.METHOD, PathInZip.__str__
def __str__(self) -> str:
        return self.zip_path + self.INFO_PATH_SEP + self.file_path
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.total_codebooks
def total_codebooks(self) -> int:
        return self.model.n_codebooks
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.__init__
def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__(cfg)
        self.cfg = cfg
        self.device = cfg.device
        self.sample_rate: int = self.cfg.sample_rate
        self.codec_model = CompressionSolver.model_from_checkpoint(
            cfg.compression_model_checkpoint, device=self.device)

        self.codec_model.set_num_codebooks(cfg.n_q)
        assert self.codec_model.sample_rate == self.cfg.sample_rate, (
            f"Codec model sample rate is {self.codec_model.sample_rate} but "
            f"Solver sample rate is {self.cfg.sample_rate}."
            )
        assert self.codec_model.sample_rate == self.sample_rate, \
            f"Sample rate of solver {self.sample_rate} and codec {self.codec_model.sample_rate} " \
            "don't match."

        self.sample_processor = get_processor(cfg.processor, sample_rate=self.sample_rate)
        self.register_stateful('sample_processor')
        self.sample_processor.to(self.device)

        self.schedule = NoiseSchedule(
            **cfg.schedule, device=self.device, sample_processor=self.sample_processor)

        self.eval_metric: tp.Optional[torch.nn.Module] = None

        self.rvm = RelativeVolumeMel()
        self.data_processor = DataProcess(initial_sr=self.sample_rate, target_sr=cfg.resampling.target_sr,
                                          use_resampling=cfg.resampling.use, cutoffs=cfg.filter.cutoffs,
                                          use_filter=cfg.filter.use, n_bands=cfg.filter.n_bands,
                                          idx_band=cfg.filter.idx_band, device=self.device)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.evaluate
def evaluate(self) -> dict:
        """Evaluate stage."""
        self.model.eval()
        with torch.no_grad():
            metrics: dict = {}
            if self.cfg.evaluate.metrics.base:
                metrics.update(self.common_train_valid('evaluate'))
            gen_metrics = self.evaluate_audio_generation()
            return {**metrics, **gen_metrics}
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, sample_top_p
def sample_top_p(probs: torch.Tensor, p: float) -> torch.Tensor:
    """Sample next token from top P probabilities along the last dimension of the input probs tensor.

    Args:
        probs (torch.Tensor): Input probabilities with token candidates on the last dimension.
        p (int): The p in “top-p”.
    Returns:
        torch.Tensor: Sampled tokens.
    """
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort *= (~mask).float()
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)
    return next_token
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestConvertAudio.test_convert_audio_channels_downmix
def test_convert_audio_channels_downmix(self):
        b, c, dur = 2, 3, 4.
        sr = 128
        audio = get_batch_white_noise(b, c, int(sr * dur))
        out = convert_audio(audio, from_rate=sr, to_rate=sr, to_channels=2)
        assert list(out.shape) == [audio.shape[0], 2, audio.shape[-1]]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.generate_audio
def generate_audio(self) -> dict:
        """Audio generation stage."""
        generate_stage_name = f'{self.current_stage}'
        sample_manager = SampleManager(self.xp)
        self.logger.info(f"Generating samples in {sample_manager.base_folder}")
        loader = self.dataloaders['generate']
        updates = len(loader)
        lp = self.log_progress(generate_stage_name, loader, total=updates, updates=self.log_updates)

        dataset = get_dataset_from_loader(loader)
        dataset_duration = dataset.segment_duration
        assert dataset_duration is not None
        assert isinstance(dataset, AudioDataset)
        target_duration = self.cfg.generate.lm.gen_duration
        prompt_duration = self.cfg.generate.lm.prompt_duration
        if target_duration is None:
            target_duration = dataset_duration
        if prompt_duration is None:
            prompt_duration = dataset_duration / 4
        assert prompt_duration < dataset_duration, (
            f"Specified prompt duration ({prompt_duration}s) is longer",
            f" than reference audio duration ({dataset_duration}s)"
        )

        def get_hydrated_conditions(meta: tp.List[SegmentWithAttributes]):
            hydrated_conditions = []
            for sample in [x.to_condition_attributes() for x in meta]:
                cond_dict = {}
                for cond_type in sample.__annotations__.keys():
                    for cond_key, cond_val in getattr(sample, cond_type).items():
                        if cond_key not in self.model.condition_provider.conditioners.keys():
                            continue
                        if is_jsonable(cond_val):
                            cond_dict[cond_key] = cond_val
                        elif isinstance(cond_val, WavCondition):
                            cond_dict[cond_key] = cond_val.path
                        elif isinstance(cond_val, JointEmbedCondition):
                            cond_dict[cond_key] = cond_val.text  # only support text at inference for now
                        else:
                            # if we reached this point, it is not clear how to log the condition
                            # so we just log the type.
                            cond_dict[cond_key] = str(type(cond_val))
                            continue
                hydrated_conditions.append(cond_dict)
            return hydrated_conditions

        metrics: dict = {}
        average = flashy.averager()
        for batch in lp:
            audio, meta = batch
            # metadata for sample manager
            hydrated_conditions = get_hydrated_conditions(meta)
            sample_generation_params = {
                **{f'classifier_free_guidance_{k}': v for k, v in self.cfg.classifier_free_guidance.items()},
                **self.generation_params
            }
            if self.cfg.generate.lm.unprompted_samples:
                if self.cfg.generate.lm.gen_gt_samples:
                    # get the ground truth instead of generation
                    self.logger.warn(
                        "Use ground truth instead of audio generation as generate.lm.gen_gt_samples=true")
                    gen_unprompted_audio = audio
                    rtf = 1.
                else:
                    gen_unprompted_outputs = self.run_generate_step(
                        batch, gen_duration=target_duration, prompt_duration=prompt_duration,
                        **self.generation_params)
                    gen_unprompted_audio = gen_unprompted_outputs['gen_audio'].cpu()
                    rtf = gen_unprompted_outputs['rtf']
                sample_manager.add_samples(
                    gen_unprompted_audio, self.epoch, hydrated_conditions,
                    ground_truth_wavs=audio, generation_args=sample_generation_params)

            if self.cfg.generate.lm.prompted_samples:
                gen_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration, prompt_duration=prompt_duration,
                    **self.generation_params)
                gen_audio = gen_outputs['gen_audio'].cpu()
                prompt_audio = gen_outputs['prompt_audio'].cpu()
                sample_manager.add_samples(
                    gen_audio, self.epoch, hydrated_conditions,
                    prompt_wavs=prompt_audio, ground_truth_wavs=audio,
                    generation_args=sample_generation_params)

            metrics['rtf'] = rtf
            metrics = average(metrics)

        flashy.distrib.barrier()
        return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\losses\test_losses.py
BlockTypes.METHOD, test_sisnr_loss
def test_sisnr_loss():
    N, C, T = 2, 2, random.randrange(1000, 100_000)
    t1 = torch.randn(N, C, T)
    t2 = torch.randn(N, C, T)

    sisnr = SISNR()
    loss = sisnr(t1, t2)

    assert isinstance(loss, torch.Tensor)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_seanet.py
BlockTypes.METHOD, TestSEANetModel.test_conv_skip_connection
def test_conv_skip_connection(self):
        encoder = SEANetEncoder(true_skip=False)
        decoder = SEANetDecoder(true_skip=False)

        x = torch.randn(1, 1, 24000)
        z = encoder(x)
        assert list(z.shape) == [1, 128, 75], z.shape
        y = decoder(z)
        assert y.shape == x.shape, (x.shape, y.shape)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\clap_consistency.py
BlockTypes.METHOD, CLAPTextConsistencyMetric.update
def update(self, audio: torch.Tensor, text: tp.List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) -> None:
        """Compute cosine similarity between audio and text pairs and accumulate scores over the dataset."""
        assert audio.size(0) == len(text), "Number of audio and text samples should match"
        assert torch.all(sample_rates == sample_rates[0].item()), "All items in batch should have the same sample rate"
        sample_rate = int(sample_rates[0].item())
        # convert audio batch to 48kHz monophonic audio with no channel dimension: [B, C, T] -> [B, T]
        audio = convert_audio(audio, from_rate=sample_rate, to_rate=self.model_sample_rate, to_channels=1).mean(dim=1)
        audio_embeddings = self.model.get_audio_embedding_from_data(audio, use_tensor=True)
        text_embeddings = self.model.get_text_embedding(text, tokenizer=self._tokenizer, use_tensor=True)
        # cosine similarity between the text and the audio embedding
        cosine_sim = torch.nn.functional.cosine_similarity(audio_embeddings, text_embeddings, dim=1, eps=1e-8)
        self.cosine_sum += cosine_sim.sum(dim=0)
        self.weight += torch.tensor(cosine_sim.size(0))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, nullify_joint_embed
def nullify_joint_embed(embed: JointEmbedCondition) -> JointEmbedCondition:
    """Nullify the joint embedding condition by replacing it by a null tensor, forcing its length to 0,
    and replacing metadata by dummy attributes.

    Args:
        cond (JointEmbedCondition): Joint embedding condition with wav and text, wav tensor of shape [B, C, T].
    """
    null_wav, _ = nullify_condition((embed.wav, torch.zeros_like(embed.wav)), dim=embed.wav.dim() - 1)
    return JointEmbedCondition(
        wav=null_wav, text=[None] * len(embed.text),
        length=torch.LongTensor([0]).to(embed.wav.device),
        sample_rate=embed.sample_rate,
        path=[None] * embed.wav.shape[0],
        seek_time=[0] * embed.wav.shape[0],
    )
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, CodebooksPatternProvider.get_pattern
def get_pattern(self, timesteps: int) -> Pattern:
        """Builds pattern with specific interleaving between codebooks.

        Args:
            timesteps (int): Total number of timesteps.
        """
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, snr_mixer
def snr_mixer(clean: torch.Tensor, noise: torch.Tensor, snr: int, min_overlap: float,
              target_level: int = -25, clipping_threshold: float = 0.99) -> torch.Tensor:
    """Function to mix clean speech and noise at various SNR levels.

    Args:
        clean (torch.Tensor): Clean audio source to mix, of shape [B, T].
        noise (torch.Tensor): Noise audio source to mix, of shape [B, T].
        snr (int): SNR level when mixing.
        min_overlap (float): Minimum overlap between the two mixed sources.
        target_level (int): Gain level in dB.
        clipping_threshold (float): Threshold for clipping the audio.
    Returns:
        torch.Tensor: The mixed audio, of shape [B, T].
    """
    if clean.shape[1] > noise.shape[1]:
        noise = torch.nn.functional.pad(noise, (0, clean.shape[1] - noise.shape[1]))
    else:
        noise = noise[:, :clean.shape[1]]

    # normalizing to -25 dB FS
    clean = clean / (clean.max(1)[0].abs().unsqueeze(1) + EPS)
    clean = normalize(clean, target_level)
    rmsclean = rms_f(clean)

    noise = noise / (noise.max(1)[0].abs().unsqueeze(1) + EPS)
    noise = normalize(noise, target_level)
    rmsnoise = rms_f(noise)

    # set the noise level for a given SNR
    noisescalar = (rmsclean / (10 ** (snr / 20)) / (rmsnoise + EPS)).unsqueeze(1)
    noisenewlevel = noise * noisescalar

    # mix noise and clean speech
    noisyspeech = mix_pair(clean, noisenewlevel, min_overlap)

    # randomly select RMS value between -15 dBFS and -35 dBFS and normalize noisyspeech with that value
    # there is a chance of clipping that might happen with very less probability, which is not a major issue.
    noisy_rms_level = np.random.randint(TARGET_LEVEL_LOWER, TARGET_LEVEL_UPPER)
    rmsnoisy = rms_f(noisyspeech)
    scalarnoisy = (10 ** (noisy_rms_level / 20) / (rmsnoisy + EPS)).unsqueeze(1)
    noisyspeech = noisyspeech * scalarnoisy
    clean = clean * scalarnoisy
    noisenewlevel = noisenewlevel * scalarnoisy

    # final check to see if there are any amplitudes exceeding +/- 1. If so, normalize all the signals accordingly
    clipped = is_clipped(noisyspeech)
    if clipped.any():
        noisyspeech_maxamplevel = noisyspeech[clipped].max(1)[0].abs().unsqueeze(1) / (clipping_threshold - EPS)
        noisyspeech[clipped] = noisyspeech[clipped] / noisyspeech_maxamplevel

    return noisyspeech
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._get_stemmed_wav
def _get_stemmed_wav(self, wav: torch.Tensor, sample_rate: int) -> torch.Tensor:
        """Get parts of the wav that holds the melody, extracting the main stems from the wav."""
        from demucs.apply import apply_model
        from demucs.audio import convert_audio
        with self.autocast:
            wav = convert_audio(
                wav, sample_rate, self.demucs.samplerate, self.demucs.audio_channels)  # type: ignore
            stems = apply_model(self.demucs, wav, device=self.device)
            stems = stems[:, self.stem_indices]  # extract relevant stems for melody conditioning
            mix_wav = stems.sum(1)  # merge extracted stems to single waveform
            mix_wav = convert_audio(mix_wav, self.demucs.samplerate, self.sample_rate, 1)  # type: ignore
            return mix_wav
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider.has_joint_embed_conditions
def has_joint_embed_conditions(self):
        return len(self.joint_embed_conditions) > 0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchLoader._zip_path
def _zip_path(self, index: int):
        assert self._current_epoch is not None
        return CachedBatchWriter._get_zip_path(self.cache_folder, self._current_epoch, index)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver._prepare_tokens_and_attributes
def _prepare_tokens_and_attributes(
        self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
        check_synchronization_points: bool = False
    ) -> tp.Tuple[dict, torch.Tensor, torch.Tensor]:
        """Prepare input batchs for language model training.

        Args:
            batch (tuple[torch.Tensor, list[SegmentWithAttributes]]): Input batch with audio tensor of shape [B, C, T]
                and corresponding metadata as SegmentWithAttributes (with B items).
            check_synchronization_points (bool): Whether to check for synchronization points slowing down training.
        Returns:
            Condition tensors (dict[str, any]): Preprocessed condition attributes.
            Tokens (torch.Tensor): Audio tokens from compression model, of shape [B, K, T_s],
                with B the batch size, K the number of codebooks, T_s the token timesteps.
            Padding mask (torch.Tensor): Mask with valid positions in the tokens tensor, of shape [B, K, T_s].
        """
        if self._cached_batch_loader is None or self.current_stage != "train":
            audio, infos = batch
            audio = audio.to(self.device)
            audio_tokens = None
            assert audio.size(0) == len(infos), (
                f"Mismatch between number of items in audio batch ({audio.size(0)})",
                f" and in metadata ({len(infos)})"
            )
        else:
            audio = None
            # In that case the batch will be a tuple coming from the _cached_batch_writer bit below.
            infos, = batch  # type: ignore
            assert all([isinstance(info, AudioInfo) for info in infos])
            assert all([info.audio_tokens is not None for info in infos])  # type: ignore
            audio_tokens = torch.stack([info.audio_tokens for info in infos]).to(self.device)  # type: ignore
            audio_tokens = audio_tokens.long()
            for info in infos:
                if isinstance(info, MusicInfo):
                    # Careful here, if you want to use this condition_wav (e.b. chroma conditioning),
                    # then you must be using the chroma cache! otherwise the code will try
                    # to use this segment and fail (by that I mean you will see NaN everywhere).
                    info.self_wav = WavCondition(
                        torch.full([1, info.channels, info.total_frames], float('NaN')),
                        length=torch.tensor([info.n_frames]),
                        sample_rate=[info.sample_rate],
                        path=[info.meta.path],
                        seek_time=[info.seek_time])
                    dataset = get_dataset_from_loader(self.dataloaders['original_train'])
                    assert isinstance(dataset, MusicDataset), type(dataset)
                    if dataset.paraphraser is not None and info.description is not None:
                        # Hackingly reapplying paraphraser when using cache.
                        info.description = dataset.paraphraser.sample_paraphrase(
                            info.meta.path, info.description)
        # prepare attributes
        attributes = [info.to_condition_attributes() for info in infos]
        attributes = self.model.cfg_dropout(attributes)
        attributes = self.model.att_dropout(attributes)
        tokenized = self.model.condition_provider.tokenize(attributes)

        # Now we should be synchronization free.
        if self.device == "cuda" and check_synchronization_points:
            torch.cuda.set_sync_debug_mode("warn")

        if audio_tokens is None:
            with torch.no_grad():
                audio_tokens, scale = self.compression_model.encode(audio)
                assert scale is None, "Scaled compression model not supported with LM."

        with self.autocast:
            condition_tensors = self.model.condition_provider(tokenized)

        # create a padding mask to hold valid vs invalid positions
        padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)
        # replace encodec tokens from padded audio with special_token_id
        if self.cfg.tokens.padding_with_special_token:
            audio_tokens = audio_tokens.clone()
            padding_mask = padding_mask.clone()
            token_sample_rate = self.compression_model.frame_rate
            B, K, T_s = audio_tokens.shape
            for i in range(B):
                n_samples = infos[i].n_frames
                audio_sample_rate = infos[i].sample_rate
                # take the last token generated from actual audio frames (non-padded audio)
                valid_tokens = math.floor(float(n_samples) / audio_sample_rate * token_sample_rate)
                audio_tokens[i, :, valid_tokens:] = self.model.special_token_id
                padding_mask[i, :, valid_tokens:] = 0

        if self.device == "cuda" and check_synchronization_points:
            torch.cuda.set_sync_debug_mode("default")

        if self._cached_batch_writer is not None and self.current_stage == 'train':
            assert self._cached_batch_loader is None
            assert audio_tokens is not None
            for info, one_audio_tokens in zip(infos, audio_tokens):
                assert isinstance(info, AudioInfo)
                if isinstance(info, MusicInfo):
                    assert not info.joint_embed, "joint_embed and cache not supported yet."
                    info.self_wav = None
                assert one_audio_tokens.max() < 2**15, one_audio_tokens.max().item()
                info.audio_tokens = one_audio_tokens.short().cpu()
            self._cached_batch_writer.save(infos)

        return condition_tensors, audio_tokens, padding_mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchWriter.start_epoch
def start_epoch(self, epoch: int):
        """Call at the beginning of each epoch.
        """
        self._current_epoch = epoch
        self._current_index = 0
        self._zip_path.parent.mkdir(exist_ok=True, parents=True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.set_num_codebooks
def set_num_codebooks(self, n: int):
        """Set the active number of codebooks used by the quantizer.
        """
        if n not in self.possible_num_codebooks:
            raise ValueError(f"Allowed values for num codebooks: {self.possible_num_codebooks}")
        self._num_codebooks = n
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.set_num_codebooks
def set_num_codebooks(self, n: int):
        """Set the active number of codebooks used by the quantizer.
        """
        assert n >= 1
        assert n <= self.total_codebooks
        self.n_quantizers = n
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.should_stop_training
def should_stop_training(self) -> bool:
        """Check whether we should stop training or not."""
        return self.epoch > self.cfg.optim.epochs
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, Paraphraser.__init__
def __init__(self, paraphrase_source: tp.Union[str, Path], paraphrase_p: float = 0.):
        self.paraphrase_p = paraphrase_p
        open_fn = gzip.open if str(paraphrase_source).lower().endswith('.gz') else open
        with open_fn(paraphrase_source, 'rb') as f:  # type: ignore
            self.paraphrase_source = json.loads(f.read())
        logger.info(f"loaded paraphrasing source from: {paraphrase_source}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, LUTConditioner.forward
def forward(self, inputs: tp.Tuple[torch.Tensor, torch.Tensor]) -> ConditionType:
        tokens, mask = inputs
        embeds = self.embed(tokens)
        embeds = self.output_proj(embeds)
        embeds = (embeds * mask.unsqueeze(-1))
        return embeds, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\export.py
BlockTypes.METHOD, export_pretrained_compression_model
def export_pretrained_compression_model(pretrained_encodec: str, out_file: tp.Union[Path, str]):
    """Export a compression model (potentially EnCodec) from a pretrained model.
    This is required for packaging the audio tokenizer along a MusicGen or AudioGen model.
    Do not include the //pretrained/ prefix. For instance if you trained a model
    with `facebook/encodec_32khz`, just put that as a name. Same for `dac_44khz`.

    In that case, this will not actually include a copy of the model, simply the reference
    to the model used.
    """
    if Path(pretrained_encodec).exists():
        pkg = torch.load(pretrained_encodec)
        assert 'best_state' in pkg
        assert 'xp.cfg' in pkg
        assert 'version' in pkg
        assert 'exported' in pkg
    else:
        pkg = {
            'pretrained': pretrained_encodec,
            'exported': True,
            'version': __version__,
        }
    Path(out_file).parent.mkdir(exist_ok=True, parents=True)
    torch.save(pkg, out_file)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.get_first_step_with_timesteps
def get_first_step_with_timesteps(self, t: int, q: tp.Optional[int] = None) -> tp.Optional[int]:
        steps_with_timesteps = self.get_steps_with_timestep(t, q)
        return steps_with_timesteps[0] if len(steps_with_timesteps) > 0 else None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, ui_full
def ui_full(launch_kwargs):
    with gr.Blocks() as interface:
        gr.Markdown(
            """
            # MusicGen
            This is your private demo for [MusicGen](https://github.com/facebookresearch/audiocraft),
            a simple and controllable model for music generation
            presented at: ["Simple and Controllable Music Generation"](https://huggingface.co/papers/2306.05284)
            """
        )
        with gr.Row():
            with gr.Column():
                with gr.Row():
                    text = gr.Text(label="Input Text", interactive=True)
                    with gr.Column():
                        radio = gr.Radio(["file", "mic"], value="file",
                                         label="Condition on a melody (optional) File or Mic")
                        melody = gr.Audio(source="upload", type="numpy", label="File",
                                          interactive=True, elem_id="melody-input")
                with gr.Row():
                    submit = gr.Button("Submit")
                    # Adapted from https://github.com/rkfg/audiocraft/blob/long/app.py, MIT license.
                    _ = gr.Button("Interrupt").click(fn=interrupt, queue=False)
                with gr.Row():
                    model = gr.Radio(["facebook/musicgen-melody", "facebook/musicgen-medium", "facebook/musicgen-small",
                                      "facebook/musicgen-large"],
                                     label="Model", value="facebook/musicgen-melody", interactive=True)
                with gr.Row():
                    decoder = gr.Radio(["Default", "MultiBand_Diffusion"],
                                       label="Decoder", value="Default", interactive=True)
                with gr.Row():
                    duration = gr.Slider(minimum=1, maximum=120, value=10, label="Duration", interactive=True)
                with gr.Row():
                    topk = gr.Number(label="Top-k", value=250, interactive=True)
                    topp = gr.Number(label="Top-p", value=0, interactive=True)
                    temperature = gr.Number(label="Temperature", value=1.0, interactive=True)
                    cfg_coef = gr.Number(label="Classifier Free Guidance", value=3.0, interactive=True)
            with gr.Column():
                output = gr.Video(label="Generated Music")
                audio_output = gr.Audio(label="Generated Music (wav)", type='filepath')
                diffusion_output = gr.Video(label="MultiBand Diffusion Decoder")
                audio_diffusion = gr.Audio(label="MultiBand Diffusion Decoder (wav)", type='filepath')
        submit.click(toggle_diffusion, decoder, [diffusion_output, audio_diffusion], queue=False,
                     show_progress=False).then(predict_full, inputs=[model, decoder, text, melody, duration, topk, topp,
                                                                     temperature, cfg_coef],
                                               outputs=[output, audio_output, diffusion_output, audio_diffusion])
        radio.change(toggle_audio_src, radio, [melody], queue=False, show_progress=False)

        gr.Examples(
            fn=predict_full,
            examples=[
                [
                    "An 80s driving pop song with heavy drums and synth pads in the background",
                    "./assets/bach.mp3",
                    "facebook/musicgen-melody",
                    "Default"
                ],
                [
                    "A cheerful country song with acoustic guitars",
                    "./assets/bolero_ravel.mp3",
                    "facebook/musicgen-melody",
                    "Default"
                ],
                [
                    "90s rock song with electric guitar and heavy drums",
                    None,
                    "facebook/musicgen-medium",
                    "Default"
                ],
                [
                    "a light and cheerly EDM track, with syncopated drums, aery pads, and strong emotions",
                    "./assets/bach.mp3",
                    "facebook/musicgen-melody",
                    "Default"
                ],
                [
                    "lofi slow bpm electro chill with organic samples",
                    None,
                    "facebook/musicgen-medium",
                    "Default"
                ],
                [
                    "Punk rock with loud drum and power guitar",
                    None,
                    "facebook/musicgen-medium",
                    "MultiBand_Diffusion"
                ],
            ],
            inputs=[text, melody, model, decoder],
            outputs=[output]
        )
        gr.Markdown(
            """
            ### More details

            The model will generate a short music extract based on the description you provided.
            The model can generate up to 30 seconds of audio in one pass. It is now possible
            to extend the generation by feeding back the end of the previous chunk of audio.
            This can take a long time, and the model might lose consistency. The model might also
            decide at arbitrary positions that the song ends.

            **WARNING:** Choosing long durations will take a long time to generate (2min might take ~10min).
            An overlap of 12 seconds is kept with the previously generated chunk, and 18 "new" seconds
            are generated each time.

            We present 4 model variations:
            1. facebook/musicgen-melody -- a music generation model capable of generating music condition
                on text and melody inputs. **Note**, you can also use text only.
            2. facebook/musicgen-small -- a 300M transformer decoder conditioned on text only.
            3. facebook/musicgen-medium -- a 1.5B transformer decoder conditioned on text only.
            4. facebook/musicgen-large -- a 3.3B transformer decoder conditioned on text only.

            We also present two way of decoding the audio tokens
            1. Use the default GAN based compression model
            2. Use MultiBand Diffusion from (paper linknano )

            When using `facebook/musicgen-melody`, you can optionally provide a reference audio from
            which a broad melody will be extracted. The model will then try to follow both
            the description and melody provided.

            You can also use your own GPU or a Google Colab by following the instructions on our repo.
            See [github.com/facebookresearch/audiocraft](https://github.com/facebookresearch/audiocraft)
            for more details.
            """
        )

        interface.queue().launch(**launch_kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, MRSTFTLoss.forward
def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """Calculate forward propagation.

        Args:
            x (torch.Tensor): Predicted signal (B, T).
            y (torch.Tensor): Groundtruth signal (B, T).
        Returns:
            torch.Tensor: Multi resolution STFT loss.
        """
        sc_loss = torch.Tensor([0.0])
        mag_loss = torch.Tensor([0.0])
        for f in self.stft_losses:
            sc_l, mag_l = f(x, y)
            sc_loss += sc_l
            mag_loss += mag_l
        sc_loss /= len(self.stft_losses)
        mag_loss /= len(self.stft_losses)

        return self.factor_sc * sc_loss + self.factor_mag * mag_loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestRead.test_read_seek_time_wav
def test_read_seek_time_wav(self):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 1.
        read_duration = 1.
        for sample_rate, ch in product(sample_rates, channels):
            n_frames = int(sample_rate * duration)
            wav = get_white_noise(ch, n_frames).clamp(-0.99, 0.99)
            path = self.get_temp_path('sample_wav.wav')
            save_wav(path, wav, sample_rate)
            seek_time = torch.rand(1).item()
            read_wav, read_sr = audio_read(path, seek_time, read_duration)
            seek_frames = int(sample_rate * seek_time)
            expected_frames = n_frames - seek_frames
            assert read_sr == sample_rate
            assert read_wav.shape[0] == wav.shape[0]
            assert read_wav.shape[1] == expected_frames
            assert torch.allclose(read_wav, wav[..., seek_frames:], rtol=1e-03, atol=1e-04)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider.text_conditions
def text_conditions(self):
        return [k for k, v in self.conditioners.items() if isinstance(v, TextConditioner)]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingMultiheadAttention._complete_kv
def _complete_kv(self, k, v):
        time_dim = _get_attention_time_dimension()
        if self.cross_attention:
            # With cross attention we assume all keys and values
            # are already available, and streaming is with respect
            # to the queries only.
            return k, v
        # Complete the key/value pair using the streaming state.
        if self._streaming_state:
            pk = self._streaming_state['past_keys']
            nk = torch.cat([pk, k], dim=time_dim)
            if v is k:
                nv = nk
            else:
                pv = self._streaming_state['past_values']
                nv = torch.cat([pv, v], dim=time_dim)
        else:
            nk = k
            nv = v

        assert nk.shape[time_dim] == nv.shape[time_dim]
        offset = 0
        if self.past_context is not None:
            offset = max(0, nk.shape[time_dim] - self.past_context)
        if self._is_streaming:
            self._streaming_state['past_keys'] = nk[:, offset:]
            if v is not k:
                self._streaming_state['past_values'] = nv[:, offset:]
            if 'offset' in self._streaming_state:
                self._streaming_state['offset'] += offset
            else:
                self._streaming_state['offset'] = torch.tensor(0)
        return nk, nv
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\best_state.py
BlockTypes.METHOD, BestStateDictManager.__init__
def __init__(self, device: tp.Union[torch.device, str] = 'cpu',
                 dtype: tp.Optional[torch.dtype] = None):
        self.device = device
        self.states: dict = {}
        self.param_ids: dict = defaultdict(dict)
        self.dtype = dtype
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, VALLEPattern.get_pattern
def get_pattern(self, timesteps: int) -> Pattern:
        out: PatternLayout = [[]]
        for t in range(timesteps):
            out.append([LayoutCoord(t, 0)])
        max_delay = max(self.delays)
        for t in range(timesteps + max_delay):
            v = []
            for q, delay in enumerate(self.delays):
                t_for_q = t - delay
                if t_for_q >= 0:
                    v.append(LayoutCoord(t_for_q, q + 1))
            out.append(v)
        return Pattern(out, n_q=self.n_q, timesteps=timesteps)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, collate
def collate(tensors: tp.List[torch.Tensor], dim: int = 0) -> tp.Tuple[torch.Tensor, torch.Tensor]:
    """Get a list of tensors and collate them to a single tensor. according to the following logic:
    - `dim` specifies the time dimension which will be stacked and padded.
    - The output will contain 1 new dimension (dimension index 0) which will be the size of
    of the original list.

    Args:
        tensors (tp.List[torch.Tensor]): List of tensors to collate.
        dim (int): Dimension which will be stacked and padded.
    Returns:
        tp.Tuple[torch.Tensor, torch.Tensor]:
            torch.Tensor: Stacked and padded tensor. The output will contain 1 new dimension
                (dimension index 0) which will be the size of the original list.
            torch.Tensor: Tensor containing length of original tensor sizes (without padding).
    """
    tensors = [x.transpose(0, dim) for x in tensors]
    lens = torch.LongTensor([len(x) for x in tensors])
    padded_tensors = pad_sequence(tensors)
    padded_tensors = padded_tensors.transpose(0, 1)
    padded_tensors = padded_tensors.transpose(1, dim + 1)
    return padded_tensors, lens
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.__init__
def __init__(self) -> None:
        """Loads configuration."""
        self.team: str = os.getenv("AUDIOCRAFT_TEAM", self.DEFAULT_TEAM)
        cluster_type = _guess_cluster_type()
        cluster = os.getenv(
            "AUDIOCRAFT_CLUSTER", cluster_type.value
        )
        logger.info("Detecting cluster type %s", cluster_type)

        self.cluster: str = cluster

        config_path = os.getenv(
            "AUDIOCRAFT_CONFIG",
            Path(__file__)
            .parent.parent.joinpath("config/teams", self.team)
            .with_suffix(".yaml"),
        )
        self.config = omegaconf.OmegaConf.load(config_path)
        self._dataset_mappers = []
        cluster_config = self._get_cluster_config()
        if "dataset_mappers" in cluster_config:
            for pattern, repl in cluster_config["dataset_mappers"].items():
                regex = re.compile(pattern)
                self._dataset_mappers.append((regex, repl))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msstftd.py
BlockTypes.METHOD, MultiScaleSTFTDiscriminator.num_discriminators
def num_discriminators(self):
        return len(self.discriminators)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.autocast
def autocast(self):
        """Convenient autocast (or not) using the solver configuration."""
        return TorchAutocast(enabled=self.cfg.autocast, device_type=self.device, dtype=self.autocast_dtype)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.replace_
def replace_(self, samples, mask):
        modified_codebook = torch.where(
            mask[..., None], sample_vectors(samples, self.codebook_size), self.embed
        )
        self.embed.data.copy_(modified_codebook)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, PerStageMetrics.__init__
def __init__(self, num_steps: int, num_stages: int = 4):
        self.num_steps = num_steps
        self.num_stages = num_stages
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestConvertAudio.test_convert_audio_channels_upmix
def test_convert_audio_channels_upmix(self):
        b, c, dur = 2, 1, 4.
        sr = 128
        audio = get_batch_white_noise(b, c, int(sr * dur))
        out = convert_audio(audio, from_rate=sr, to_rate=sr, to_channels=3)
        assert list(out.shape) == [audio.shape[0], 3, audio.shape[-1]]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\mpd.py
BlockTypes.METHOD, MultiPeriodDiscriminator.__init__
def __init__(self, in_channels: int = 1, out_channels: int = 1,
                 periods: tp.Sequence[int] = [2, 3, 5, 7, 11], **kwargs):
        super().__init__()
        self.discriminators = nn.ModuleList([
            PeriodDiscriminator(p, in_channels, out_channels, **kwargs) for p in periods
        ])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen.generate
def generate(self, descriptions: tp.List[str], progress: bool = False) -> torch.Tensor:
        """Generate samples conditioned on text.

        Args:
            descriptions (list of str): A list of strings used as text conditioning.
            progress (bool, optional): Flag to display progress of the generation process. Defaults to False.
        """
        attributes, prompt_tokens = self._prepare_tokens_and_attributes(descriptions, None)
        assert prompt_tokens is None
        return self._generate_tokens(attributes, prompt_tokens, progress)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, DelayedPatternProvider.__init__
def __init__(self, n_q: int, delays: tp.Optional[tp.List[int]] = None,
                 flatten_first: int = 0, empty_initial: int = 0):
        super().__init__(n_q)
        if delays is None:
            delays = list(range(n_q))
        self.delays = delays
        self.flatten_first = flatten_first
        self.empty_initial = empty_initial
        assert len(self.delays) == self.n_q
        assert sorted(self.delays) == self.delays
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchLoader._load_one
def _load_one(self, index: int):
        zip_path = self._zip_path(index)
        if not zip_path.exists():
            if index < self.min_length:
                raise RuntimeError(f"Cache should have at least {self.min_length} batches, but {index} doesn't exist")

            return None
        mode = "rb" if sys.version_info >= (3, 9) else "r"
        try:
            with zipfile.ZipFile(zip_path, 'r') as zf:
                rank = flashy.distrib.rank()
                world_size = flashy.distrib.world_size()
                root = zipfile.Path(zf)
                items = list(root.iterdir())
                total_batch_size = self.batch_size * world_size
                if len(items) < total_batch_size:
                    raise RuntimeError(
                        f"The cache can handle a max batch size of {len(items)}, "
                        f"but {total_batch_size} is needed.")
                start = rank * self.batch_size
                items = items[start: start + self.batch_size]
                assert len(items) == self.batch_size
                entries = []
                entries = [torch.load(item.open(mode), 'cpu') for item in items]  # type: ignore
                transposed = zip(*entries)
                out = []
                for part in transposed:
                    assert len(part) > 0
                    if isinstance(part[0], torch.Tensor):
                        out.append(torch.stack(part))
                    else:
                        out.append(part)
                return out
        except Exception:
            logger.error("Error when reading zip path %s", zip_path)
            raise
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern._build_pattern_sequence_scatter_indexes
def _build_pattern_sequence_scatter_indexes(self, timesteps: int, n_q: int, keep_only_valid_steps: bool,
                                                device: tp.Union[torch.device, str] = 'cpu'):
        """Build scatter indexes corresponding to the pattern, up to the provided sequence_steps.

        Args:
            timesteps (int): Maximum number of timesteps steps to consider.
            keep_only_valid_steps (bool): Restrict the pattern layout to match only valid steps.
            device (torch.device or str): Device for created tensors.
        Returns:
            indexes (torch.Tensor): Indexes corresponding to the sequence, of shape [K, S].
            mask (torch.Tensor): Mask corresponding to indexes that matches valid indexes, of shape [K, S].
        """
        assert n_q == self.n_q, f"invalid number of codebooks for the sequence and the pattern: {n_q} != {self.n_q}"
        assert timesteps <= self.timesteps, "invalid number of timesteps used to build the sequence from the pattern"
        # use the proper layout based on whether we limit ourselves to valid steps only or not,
        # note that using the valid_layout will result in a truncated sequence up to the valid steps
        ref_layout = self.valid_layout if keep_only_valid_steps else self.layout
        # single item indexing being super slow with pytorch vs. numpy, so we use numpy here
        indexes = torch.zeros(n_q, len(ref_layout), dtype=torch.long).numpy()
        mask = torch.zeros(n_q, len(ref_layout), dtype=torch.bool).numpy()
        # fill indexes with last sequence step value that will correspond to our special token
        # the last value is n_q * timesteps as we have flattened z and append special token as the last token
        # which will correspond to the index: n_q * timesteps
        indexes[:] = n_q * timesteps
        # iterate over the pattern and fill scattered indexes and mask
        for s, sequence_coords in enumerate(ref_layout):
            for coords in sequence_coords:
                if coords.t < timesteps:
                    indexes[coords.q, s] = coords.t + coords.q * timesteps
                    mask[coords.q, s] = 1
        indexes = torch.from_numpy(indexes).to(device)
        mask = torch.from_numpy(mask).to(device)
        return indexes, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, BaseQuantizer.forward
def forward(self, x: torch.Tensor, frame_rate: int) -> QuantizedResult:
        """
        Given input tensor x, returns first the quantized (or approximately quantized)
        representation along with quantized codes, bandwidth, and any penalty term for the loss.
        Finally, this returns a dict of metrics to update logging etc.
        Frame rate must be passed so that the bandwidth is properly computed.
        """
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\zip.py
BlockTypes.METHOD, _open_zip
def _open_zip(path: str, mode: MODE = 'r'):
    return zipfile.ZipFile(path, mode)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, PasstKLDivergenceMetric._process_audio
def _process_audio(self, wav: torch.Tensor, sample_rate: int, wav_len: int) -> tp.List[torch.Tensor]:
        """Process audio to feed to the pretrained model."""
        wav = wav.unsqueeze(0)
        wav = wav[..., :wav_len]
        wav = convert_audio(wav, from_rate=sample_rate, to_rate=self.model_sample_rate, to_channels=1)
        wav = wav.squeeze(0)
        # we don't pad but return a list of audio segments as this otherwise affects the KLD computation
        segments = torch.split(wav, self.max_input_frames, dim=-1)
        valid_segments = []
        for s in segments:
            # ignoring too small segments that are breaking the model inference
            if s.size(-1) > self.min_input_frames:
                valid_segments.append(s)
        return [s[None] for s in valid_segments]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\sisnr.py
BlockTypes.METHOD, _unfold
def _unfold(a: torch.Tensor, kernel_size: int, stride: int) -> torch.Tensor:
    """Given input of size [*OT, T], output Tensor of size [*OT, F, K]
    with K the kernel size, by extracting frames with the given stride.
    This will pad the input so that `F = ceil(T / K)`.
    see https://github.com/pytorch/pytorch/issues/60466
    """
    *shape, length = a.shape
    n_frames = math.ceil(length / stride)
    tgt_length = (n_frames - 1) * stride + kernel_size
    a = F.pad(a, (0, tgt_length - length))
    strides = list(a.stride())
    assert strides[-1] == 1, "data should be contiguous"
    strides = strides[:-1] + [stride, 1]
    return a.as_strided([*shape, n_frames, kernel_size], strides)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\specloss.py
BlockTypes.METHOD, MultiScaleMelSpectrogramLoss.__init__
def __init__(self, sample_rate: int, range_start: int = 6, range_end: int = 11,
                 n_mels: int = 64, f_min: float = 0.0, f_max: tp.Optional[float] = None,
                 normalized: bool = False, alphas: bool = True, floor_level: float = 1e-5):
        super().__init__()
        l1s = list()
        l2s = list()
        self.alphas = list()
        self.total = 0
        self.normalized = normalized
        for i in range(range_start, range_end):
            l1s.append(
                MelSpectrogramWrapper(n_fft=2 ** i, hop_length=(2 ** i) / 4, win_length=2 ** i,
                                      n_mels=n_mels, sample_rate=sample_rate, f_min=f_min, f_max=f_max,
                                      log=False, normalized=normalized, floor_level=floor_level))
            l2s.append(
                MelSpectrogramWrapper(n_fft=2 ** i, hop_length=(2 ** i) / 4, win_length=2 ** i,
                                      n_mels=n_mels, sample_rate=sample_rate, f_min=f_min, f_max=f_max,
                                      log=True, normalized=normalized, floor_level=floor_level))
            if alphas:
                self.alphas.append(np.sqrt(2 ** i - 1))
            else:
                self.alphas.append(1)
            self.total += self.alphas[-1] + 1

        self.l1s = nn.ModuleList(l1s)
        self.l2s = nn.ModuleList(l2s)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.generate_continuation
def generate_continuation(self, prompt: torch.Tensor, prompt_sample_rate: int,
                              descriptions: tp.Optional[tp.List[tp.Optional[str]]] = None,
                              progress: bool = False, return_tokens: bool = False) \
            -> tp.Union[torch.Tensor, tp.Tuple[torch.Tensor, torch.Tensor]]:
        """Generate samples conditioned on audio prompts.

        Args:
            prompt (torch.Tensor): A batch of waveforms used for continuation.
                Prompt should be [B, C, T], or [C, T] if only one sample is generated.
            prompt_sample_rate (int): Sampling rate of the given audio waveforms.
            descriptions (list of str, optional): A list of strings used as text conditioning. Defaults to None.
            progress (bool, optional): Flag to display progress of the generation process. Defaults to False.
        """
        if prompt.dim() == 2:
            prompt = prompt[None]
        if prompt.dim() != 3:
            raise ValueError("prompt should have 3 dimensions: [B, C, T] (C = 1).")
        prompt = convert_audio(prompt, prompt_sample_rate, self.sample_rate, self.audio_channels)
        if descriptions is None:
            descriptions = [None] * len(prompt)
        attributes, prompt_tokens = self._prepare_tokens_and_attributes(descriptions, prompt)
        assert prompt_tokens is not None
        tokens = self._generate_tokens(attributes, prompt_tokens, progress)
        if return_tokens:
            return self.generate_audio(tokens), tokens
        return self.generate_audio(tokens)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider.wav_conditions
def wav_conditions(self):
        return [k for k, v in self.conditioners.items() if isinstance(v, WaveformConditioner)]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, find_audio_files
def find_audio_files(path: tp.Union[Path, str],
                     exts: tp.List[str] = DEFAULT_EXTS,
                     resolve: bool = True,
                     minimal: bool = True,
                     progress: bool = False,
                     workers: int = 0) -> tp.List[AudioMeta]:
    """Build a list of AudioMeta from a given path,
    collecting relevant audio files and fetching meta info.

    Args:
        path (str or Path): Path to folder containing audio files.
        exts (list of str): List of file extensions to consider for audio files.
        minimal (bool): Whether to only load the minimal set of metadata (takes longer if not).
        progress (bool): Whether to log progress on audio files collection.
        workers (int): number of parallel workers, if 0, use only the current thread.
    Returns:
        list of AudioMeta: List of audio file path and its metadata.
    """
    audio_files = []
    futures: tp.List[Future] = []
    pool: tp.Optional[ThreadPoolExecutor] = None
    with ExitStack() as stack:
        if workers > 0:
            pool = ThreadPoolExecutor(workers)
            stack.enter_context(pool)

        if progress:
            print("Finding audio files...")
        for root, folders, files in os.walk(path, followlinks=True):
            for file in files:
                full_path = Path(root) / file
                if full_path.suffix.lower() in exts:
                    audio_files.append(full_path)
                    if pool is not None:
                        futures.append(pool.submit(_get_audio_meta, str(audio_files[-1]), minimal))
                    if progress:
                        print(format(len(audio_files), " 8d"), end='\r', file=sys.stderr)

        if progress:
            print("Getting audio metadata...")
        meta: tp.List[AudioMeta] = []
        for idx, file_path in enumerate(audio_files):
            try:
                if pool is None:
                    m = _get_audio_meta(str(file_path), minimal)
                else:
                    m = futures[idx].result()
                if resolve:
                    m = _resolve_audio_meta(m)
            except Exception as err:
                print("Error with", str(file_path), err, file=sys.stderr)
                continue
            meta.append(m)
            if progress:
                print(format((1 + idx) / len(audio_files), " 3.1%"), end='\r', file=sys.stderr)
    meta.sort()
    return meta
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric.update
def update(self, preds: torch.Tensor, targets: torch.Tensor,
               sizes: torch.Tensor, sample_rates: torch.Tensor,
               stems: tp.Optional[tp.List[str]] = None):
        """Update torchmetrics.Metrics by saving the audio and updating the manifest file."""
        assert preds.shape == targets.shape, f"preds={preds.shape} != targets={targets.shape}"
        num_samples = preds.shape[0]
        assert num_samples == sizes.size(0) and num_samples == sample_rates.size(0)
        assert stems is None or num_samples == len(set(stems))
        for i in range(num_samples):
            self.total_files += 1  # type: ignore
            self.counter += 1
            wav_len = int(sizes[i].item())
            sample_rate = int(sample_rates[i].item())
            pred_wav = preds[i]
            target_wav = targets[i]
            pred_wav = pred_wav[..., :wav_len]
            target_wav = target_wav[..., :wav_len]
            stem_name = stems[i] if stems is not None else f'sample_{self.counter}_{flashy.distrib.rank()}'
            # dump audio files
            try:
                pred_wav = convert_audio(
                    pred_wav.unsqueeze(0), from_rate=sample_rate,
                    to_rate=self.model_sample_rate, to_channels=1).squeeze(0)
                audio_write(
                    self.samples_tests_dir / stem_name, pred_wav, sample_rate=self.model_sample_rate,
                    format=self.format, strategy="peak")
            except Exception as e:
                logger.error(f"Exception occured when saving tests files for FAD computation: {repr(e)} - {e}")
            try:
                # for the ground truth audio, we enforce the 'peak' strategy to avoid modifying
                # the original audio when writing it
                target_wav = convert_audio(
                    target_wav.unsqueeze(0), from_rate=sample_rate,
                    to_rate=self.model_sample_rate, to_channels=1).squeeze(0)
                audio_write(
                    self.samples_background_dir / stem_name, target_wav, sample_rate=self.model_sample_rate,
                    format=self.format, strategy="peak")
            except Exception as e:
                logger.error(f"Exception occured when saving background files for FAD computation: {repr(e)} - {e}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioMeta.from_dict
def from_dict(cls, dictionary: dict):
        base = cls._dict2fields(dictionary)
        if 'info_path' in base and base['info_path'] is not None:
            base['info_path'] = PathInZip(base['info_path'])
        return cls(**base)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, MultiBandDiffusion.tokens_to_wav
def tokens_to_wav(self, tokens: torch.Tensor, n_bands: int = 32):
        """Generate Waveform audio with diffusion from the discrete codes.
        Args:
            tokens (torch.Tensor): discrete codes
            n_bands (int): bands for the eq matching.
        """
        wav_encodec = self.codec_model.decode(tokens)
        condition = self.get_emb(tokens)
        wav_diffusion = self.generate(emb=condition, size=wav_encodec.size())
        return self.re_eq(wav=wav_diffusion, ref=wav_encodec, n_bands=n_bands)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestFeatureMatchingLoss.test_features_matching_loss_base
def test_features_matching_loss_base(self):
        ft_matching_loss = FeatureMatchingLoss()
        length = random.randrange(1, 100_000)
        t1 = torch.randn(1, 2, length)

        loss = ft_matching_loss([t1], [t1])
        assert isinstance(loss, torch.Tensor)
        assert loss.item() == 0.0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, Tokenizer.__call__
def __call__(self, texts: tp.List[tp.Optional[str]]) -> tp.Tuple[torch.Tensor, torch.Tensor]:
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver._get_state_source
def _get_state_source(self, name) -> flashy.state.StateDictSource:
        # Internal utility to get a state source from the solver
        return self.stateful.sources[name]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchWriter._get_zip_path
def _get_zip_path(cache_folder: Path, epoch: int, index: int):
        return cache_folder / f"{epoch:05d}" / f"{index:06d}.zip"
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, NoiseSchedule.get_beta
def get_beta(self, step: tp.Union[int, torch.Tensor]):
        if self.n_bands is None:
            return self.betas[step]
        else:
            return self.betas[:, step]  # [n_bands, len(step)]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, ResidualVectorQuantization.__init__
def __init__(self, *, num_quantizers, **kwargs):
        super().__init__()
        self.layers = nn.ModuleList(
            [VectorQuantization(**kwargs) for _ in range(num_quantizers)]
        )
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, BLSTM.__init__
def __init__(self, dim, layers=2):
        super().__init__()
        self.lstm = nn.LSTM(bidirectional=True, num_layers=layers, hidden_size=dim, input_size=dim)
        self.linear = nn.Linear(2 * dim, dim)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, apply_parametrization_norm
def apply_parametrization_norm(module: nn.Module, norm: str = 'none'):
    assert norm in CONV_NORMALIZATIONS
    if norm == 'weight_norm':
        return weight_norm(module)
    elif norm == 'spectral_norm':
        return spectral_norm(module)
    else:
        # We already check was in CONV_NORMALIZATION, so any other choice
        # doesn't need reparametrization.
        return module
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.build_dataloaders
def build_dataloaders(self) -> None:
        """Instantiate audio dataloaders for each stage."""
        self.dataloaders = builders.get_audio_datasets(self.cfg, dataset_type=self.DATASET_TYPE)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\zip.py
BlockTypes.METHOD, set_zip_cache_size
def set_zip_cache_size(max_size: int):
    """Sets the maximal LRU caching for zip file opening.

    Args:
        max_size (int): the maximal LRU cache.
    """
    global _cached_open_zip
    _cached_open_zip = lru_cache(max_size)(_open_zip)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._extract_wav_embedding_chunk
def _extract_wav_embedding_chunk(self, full_embed: torch.Tensor, x: JointEmbedCondition, idx: int) -> torch.Tensor:
        """Extract the chunk of embedding matching the seek_time and length from the full CLAP audio embedding.

        Args:
            full_embed (torch.Tensor): CLAP embedding computed on the full wave, of shape [F, D].
            x (JointEmbedCondition): Joint embedding condition for the full batch.
            idx (int): Index considered for the given embedding to extract.
        Returns:
            torch.Tensor: Wav embedding averaged on sliding window, of shape [1, D].
        """
        sample_rate = x.sample_rate[idx]
        seek_time = x.seek_time[idx]
        seek_time = 0. if seek_time is None else seek_time
        clap_stride = int(self.clap_stride / self.clap_sample_rate) * sample_rate
        end_seek_time = seek_time + self.clap_max_frames / self.clap_sample_rate
        start_offset = int(seek_time * sample_rate // clap_stride)
        end_offset = int(end_seek_time * sample_rate // clap_stride)
        wav_embed = full_embed[start_offset:end_offset, ...]
        wav_embed = wav_embed.mean(dim=0, keepdim=True)
        return wav_embed.to(self.device)  # [F, D]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider.has_wav_condition
def has_wav_condition(self):
        return len(self.wav_conditions) > 0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\rope.py
BlockTypes.METHOD, RotaryEmbedding.__init__
def __init__(self, dim: int, max_period: float = 10000.0, xpos: bool = False,
                 scale: float = 1.0, device=None, dtype: torch.dtype = torch.float32):
        super().__init__()
        assert dim % 2 == 0
        self.scale = scale
        assert dtype in [torch.float64, torch.float32]
        self.dtype = dtype

        adim = torch.arange(0, dim, 2, device=device, dtype=dtype)[: (dim // 2)]
        frequencies = 1.0 / (max_period ** (adim / dim))
        self.register_buffer("frequencies", frequencies)
        self.rotation: tp.Optional[torch.Tensor] = None

        self.xpos = XPos(dim, device=device, dtype=dtype) if xpos else None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._get_embed
def _get_embed(self, x: JointEmbedCondition) -> tp.Tuple[torch.Tensor, torch.Tensor]:
        """Extract shared latent representation from either the wav or the text using CLAP."""
        # decide whether to use text embedding at train time or not
        use_text_embed = random.random() < self.text_p
        if self.training and not use_text_embed:
            embed = self._get_wav_embedding(x)
            empty_idx = torch.LongTensor([])  # we assume we always have the audio wav
        else:
            embed = self._get_text_embedding(x)
            empty_idx = torch.LongTensor([i for i, xi in enumerate(x.text) if xi is None or xi == ""])
        return embed, empty_idx
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, PerStageMetrics.__call__
def __call__(self, losses: dict, step: tp.Union[int, torch.Tensor]):
        if type(step) is int:
            stage = int((step / self.num_steps) * self.num_stages)
            return {f"{name}_{stage}": loss for name, loss in losses.items()}
        elif type(step) is torch.Tensor:
            stage_tensor = ((step / self.num_steps) * self.num_stages).long()
            out: tp.Dict[str, float] = {}
            for stage_idx in range(self.num_stages):
                mask = (stage_tensor == stage_idx)
                N = mask.sum()
                stage_out = {}
                if N > 0:  # pass if no elements in the stage
                    for name, loss in losses.items():
                        stage_loss = (mask * loss).sum() / N
                        stage_out[f"{name}_{stage_idx}"] = stage_loss
                out = {**out, **stage_out}
            return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric._compute_fad_score
def _compute_fad_score(self, gpu_index: tp.Optional[int] = None):
        cmd = [
            self.python_path, "-m", "frechet_audio_distance.compute_fad",
            "--test_stats", f"{str(self.stats_tests_dir)}",
            "--background_stats", f"{str(self.stats_background_dir)}",
        ]
        logger.info(f"Launching frechet_audio_distance compute fad method: {' '.join(cmd)}")
        env = os.environ
        if gpu_index is not None:
            env["CUDA_VISIBLE_DEVICES"] = str(gpu_index)
        result = subprocess.run(cmd, env={**env, **self.tf_env}, capture_output=True)
        if result.returncode:
            logger.error(
                "Error with FAD computation from stats: \n %s \n %s",
                result.stdout.decode(), result.stderr.decode()
            )
            raise RuntimeError("Error while executing FAD computation from stats")
        try:
            # result is "FAD: (d+).(d+)" hence we remove the prefix with (d+) being one digit or more
            fad_score = float(result.stdout[4:])
            return fad_score
        except Exception as e:
            raise RuntimeError(f"Error parsing FAD score from command stdout: {e}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, BaseQuantizer.encode
def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode a given input tensor with the specified sample rate at the given bandwidth."""
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.best_metric_name
def best_metric_name(self) -> tp.Optional[str]:
        """Metric name used to identify the best state. This metric should be stored in the metrics
        used on the stage for best state identification (most likely, `valid`). If None, then
        no best state is saved.
        """
        return None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\checkpoint.py
BlockTypes.METHOD, _safe_save_checkpoint
def _safe_save_checkpoint(state: tp.Any, checkpoint_path: Path, is_sharded: bool = False) -> None:
    """Save checkpoints in a safe manner even with when sharded checkpoints across nodes."""
    def _barrier_if_sharded():
        if is_sharded:
            flashy.distrib.barrier()

    if flashy.distrib.is_rank_zero():
        token = Path(str(checkpoint_path) + '.tmp.done')
        if token.exists():
            token.unlink()
    _barrier_if_sharded()
    with flashy.utils.write_and_rename(checkpoint_path) as f:
        torch.save(state, f)
        _barrier_if_sharded()
        if flashy.distrib.is_rank_zero():
            token.touch()
        _barrier_if_sharded()
    _barrier_if_sharded()
    if flashy.distrib.rank() == 0:
        token.unlink()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\losses\test_losses.py
BlockTypes.METHOD, test_stft_loss
def test_stft_loss():
    N, C, T = 2, 2, random.randrange(1000, 100_000)
    t1 = torch.randn(N, C, T)
    t2 = torch.randn(N, C, T)

    mrstft = STFTLoss()
    loss = mrstft(t1, t2)

    assert isinstance(loss, torch.Tensor)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_rope.py
BlockTypes.METHOD, test_rope_streaming_past_context
def test_rope_streaming_past_context():
    set_efficient_attention_backend('xformers')
    torch.manual_seed(1234)

    for context in [None, 10]:
        tr = StreamingTransformer(
            16, 4, 1 if context else 2,
            causal=True, past_context=context, custom=True,
            dropout=0., positional_embedding='rope')
        tr.eval()

        steps = 20
        x = torch.randn(3, steps, 16)
        ref = tr(x)

        with tr.streaming():
            outs = []
            frame_sizes = [1] * steps

            for frame_size in frame_sizes:
                frame = x[:, :frame_size]
                x = x[:, frame_size:]
                outs.append(tr(frame))

        out = torch.cat(outs, dim=1)
        assert list(out.shape) == [3, steps, 16]
        delta = torch.norm(out - ref) / torch.norm(out)
        assert delta < 1e-6, delta
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\best_state.py
BlockTypes.METHOD, BestStateDictManager._get_parameter_ids
def _get_parameter_ids(self, state_dict):
        return {id(p): name for name, p in state_dict.items() if isinstance(p, torch.Tensor)}
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchWriter._zip_path
def _zip_path(self):
        assert self._current_epoch is not None
        return CachedBatchWriter._get_zip_path(self.cache_folder, self._current_epoch, self._current_index)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\resample_dataset.py
BlockTypes.METHOD, read_egs_files
def read_egs_files(path: tp.Union[str, Path]):
    path = Path(path)
    if path.is_dir():
        if (path / 'data.jsonl').exists():
            path = path / 'data.jsonl'
        elif (path / 'data.jsonl.gz').exists():
            path = path / 'data.jsonl.gz'
        else:
            raise ValueError("Don't know where to read metadata from in the dir. "
                             "Expecting either a data.jsonl or data.jsonl.gz file but none found.")
    meta = load_audio_meta(path)
    return [m.path for m in meta]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.load_from_pretrained
def load_from_pretrained(self, name: str) -> dict:
        model = models.CompressionModel.get_pretrained(name)
        if isinstance(model, models.DAC):
            raise RuntimeError("Cannot fine tune a DAC model.")
        elif isinstance(model, models.HFEncodecCompressionModel):
            self.logger.warning('Trying to automatically convert a HuggingFace model '
                                'to AudioCraft, this might fail!')
            state = model.model.state_dict()
            new_state = {}
            for k, v in state.items():
                if k.startswith('decoder.layers') and '.conv.' in k and '.block.' not in k:
                    # We need to determine if this a convtr or a regular conv.
                    layer = int(k.split('.')[2])
                    if isinstance(model.model.decoder.layers[layer].conv, torch.nn.ConvTranspose1d):

                        k = k.replace('.conv.', '.convtr.')
                k = k.replace('encoder.layers.', 'encoder.model.')
                k = k.replace('decoder.layers.', 'decoder.model.')
                k = k.replace('conv.', 'conv.conv.')
                k = k.replace('convtr.', 'convtr.convtr.')
                k = k.replace('quantizer.layers.', 'quantizer.vq.layers.')
                k = k.replace('.codebook.', '._codebook.')
                new_state[k] = v
            state = new_state
        elif isinstance(model, models.EncodecModel):
            state = model.state_dict()
        else:
            raise RuntimeError(f"Cannot fine tune model type {type(model)}.")
        return {
            'best_state': {'model': state}
        }
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.run_epoch
def run_epoch(self):
        # reset random seed at the beginning of the epoch
        self.rng = torch.Generator()
        self.rng.manual_seed(1234 + self.epoch)
        self.per_stage = PerStageMetrics(self.schedule.num_steps, self.cfg.metrics.num_stage)
        # run epoch
        super().run_epoch()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\mpd.py
BlockTypes.METHOD, MultiPeriodDiscriminator.forward
def forward(self, x: torch.Tensor) -> MultiDiscriminatorOutputType:
        logits = []
        fmaps = []
        for disc in self.discriminators:
            logit, fmap = disc(x)
            logits.append(logit)
            fmaps.append(fmap)
        return logits, fmaps
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.decode
def decode(self, embed_ind):
        quantize = self.dequantize(embed_ind)
        return quantize
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\base.py
BlockTypes.METHOD, MultiDiscriminator.__init__
def __init__(self):
        super().__init__()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider.tokenize
def tokenize(self, inputs: tp.List[ConditioningAttributes]) -> tp.Dict[str, tp.Any]:
        """Match attributes/wavs with existing conditioners in self, and compute tokenize them accordingly.
        This should be called before starting any real GPU work to avoid synchronization points.
        This will return a dict matching conditioner names to their arbitrary tokenized representations.

        Args:
            inputs (list[ConditioningAttributes]): List of ConditioningAttributes objects containing
                text and wav conditions.
        """
        assert all([isinstance(x, ConditioningAttributes) for x in inputs]), (
            "Got unexpected types input for conditioner! should be tp.List[ConditioningAttributes]",
            f" but types were {set([type(x) for x in inputs])}"
        )

        output = {}
        text = self._collate_text(inputs)
        wavs = self._collate_wavs(inputs)
        joint_embeds = self._collate_joint_embeds(inputs)

        assert set(text.keys() | wavs.keys() | joint_embeds.keys()).issubset(set(self.conditioners.keys())), (
            f"Got an unexpected attribute! Expected {self.conditioners.keys()}, ",
            f"got {text.keys(), wavs.keys(), joint_embeds.keys()}"
        )

        for attribute, batch in chain(text.items(), wavs.items(), joint_embeds.items()):
            output[attribute] = self.conditioners[attribute].tokenize(batch)
        return output
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_seanet.py
BlockTypes.METHOD, TestSEANetModel.test_seanet_encoder_decoder_final_act
def test_seanet_encoder_decoder_final_act(self):
        encoder = SEANetEncoder(true_skip=False)
        decoder = SEANetDecoder(true_skip=False, final_activation='Tanh')

        x = torch.randn(1, 1, 24000)
        z = encoder(x)
        assert list(z.shape) == [1, 128, 75], z.shape
        y = decoder(z)
        assert y.shape == x.shape, (x.shape, y.shape)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.show
def show(self) -> None:
        """Show the compression model and LM model."""
        self.logger.info("Compression model:")
        self.log_model_summary(self.compression_model)
        self.logger.info("LM model:")
        self.log_model_summary(self.model)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\vq.py
BlockTypes.METHOD, ResidualVectorQuantizer.encode
def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode a given input tensor with the specified frame rate at the given bandwidth.
        The RVQ encode method sets the appropriate number of quantizer to use
        and returns indices for each quantizer.
        """
        n_q = self.n_q
        codes = self.vq.encode(x, n_q=n_q)
        codes = codes.transpose(0, 1)
        # codes is [B, K, T], with T frames, K nb of codebooks.
        return codes
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.swap_best_state
def swap_best_state(self):
        self.logger.debug(f"Swapping to best state for: {', '.join(self.best_state.state_dict().keys())}")
        old_states = self._load_new_state_dict(self.best_state.state_dict())
        try:
            yield
        finally:
            self.logger.debug("Swapping back from best to original state")
            for name, old_state in old_states.items():
                state_source = self._get_state_source(name)
                state_source.load_state_dict(old_state)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_encodec_model.py
BlockTypes.METHOD, TestEncodecModel.test_model
def test_model(self):
        random.seed(1234)
        sample_rate = 24_000
        channels = 1
        model = self._create_encodec_model(sample_rate, channels)
        for _ in range(10):
            length = random.randrange(1, 10_000)
            x = torch.randn(2, channels, length)
            res = model(x)
            assert res.x.shape == x.shape
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric._get_samples_name
def _get_samples_name(self, is_background: bool):
        return 'background' if is_background else 'tests'
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, _verify_xformers_memory_efficient_compat
def _verify_xformers_memory_efficient_compat():
    try:
        from xformers.ops import memory_efficient_attention, LowerTriangularMask  # noqa
    except ImportError:
        raise ImportError(
            "xformers is not installed. Please install it and try again.\n"
            "To install on AWS and Azure, run \n"
            "FORCE_CUDA=1 TORCH_CUDA_ARCH_LIST='8.0'\\\n"
            "pip install -U git+https://git@github.com/fairinternal/xformers.git#egg=xformers\n"
            "To install on FAIR Cluster, run \n"
            "FORCE_CUDA=1 TORCH_CUDA_ARCH_LIST='6.0;7.0'\\\n"
            "pip install -U git+https://git@github.com/fairinternal/xformers.git#egg=xformers\n")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_solver
def get_solver(cfg: omegaconf.DictConfig) -> StandardSolver:
    """Instantiate solver from config."""
    from .audiogen import AudioGenSolver
    from .compression import CompressionSolver
    from .musicgen import MusicGenSolver
    from .diffusion import DiffusionSolver
    klass = {
        'compression': CompressionSolver,
        'musicgen': MusicGenSolver,
        'audiogen': AudioGenSolver,
        'lm': MusicGenSolver,  # backward compatibility
        'diffusion': DiffusionSolver,
        'sound_lm': AudioGenSolver,  # backward compatibility
    }[cfg.solver]
    return klass(cfg)  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager.add_samples
def add_samples(self, samples_wavs: torch.Tensor, epoch: int,
                    conditioning: tp.Optional[tp.List[tp.Dict[str, tp.Any]]] = None,
                    prompt_wavs: tp.Optional[torch.Tensor] = None,
                    ground_truth_wavs: tp.Optional[torch.Tensor] = None,
                    generation_args: tp.Optional[tp.Dict[str, tp.Any]] = None) -> tp.List[Sample]:
        """Adds a batch of samples.
        The samples are stored in the XP's sample output directory, under a corresponding
        epoch folder. Each sample is assigned an id which is computed using the input data and their batch index.
        In addition to the sample itself, a json file containing associated metadata is stored next to it.

        Args:
            sample_wavs (torch.Tensor): Batch of audio wavs to store. Tensor of shape [batch_size, channels, shape].
            epoch (int): Current training epoch.
            conditioning (list of dict[str, str], optional): List of conditions used during generation,
                one per sample in the batch.
            prompt_wavs (torch.Tensor, optional): Prompts used during generation. Tensor of shape
                [batch_size, channels, shape].
            ground_truth_wav (torch.Tensor, optional): Reference audio where prompts were extracted from.
                Tensor of shape [batch_size, channels, shape].
            generation_args (dict[str, Any], optional): Dictionary of other arguments used during generation.
        Returns:
            samples (list of Sample): The saved audio samples with prompts, ground truth and metadata.
        """
        samples = []
        for idx, wav in enumerate(samples_wavs):
            prompt_wav = prompt_wavs[idx] if prompt_wavs is not None else None
            gt_wav = ground_truth_wavs[idx] if ground_truth_wavs is not None else None
            conditions = conditioning[idx] if conditioning is not None else None
            samples.append(self.add_sample(wav, epoch, idx, conditions, prompt_wav, gt_wav, generation_args))
        return samples
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, MultiBandProcessor.mean
def mean(self):
        mean = self.sum_x / self.counts
        return mean
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, ResidualVectorQuantization.forward
def forward(self, x, n_q: tp.Optional[int] = None):
        quantized_out = 0.0
        residual = x

        all_losses = []
        all_indices = []

        n_q = n_q or len(self.layers)

        for i, layer in enumerate(self.layers[:n_q]):
            quantized, indices, loss = layer(residual)
            residual = residual - quantized
            quantized_out = quantized_out + quantized
            all_indices.append(indices)
            all_losses.append(loss)

        out_losses, out_indices = map(torch.stack, (all_losses, all_indices))
        return quantized_out, out_indices, out_losses
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, Paraphraser.sample_paraphrase
def sample_paraphrase(self, audio_path: str, description: str):
        if random.random() >= self.paraphrase_p:
            return description
        info_path = Path(audio_path).with_suffix('.json')
        if info_path not in self.paraphrase_source:
            warn_once(logger, f"{info_path} not in paraphrase source!")
            return description
        new_desc = random.choice(self.paraphrase_source[info_path])
        logger.debug(f"{description} -> {new_desc}")
        return new_desc
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\best_state.py
BlockTypes.METHOD, BestStateDictManager._validate_no_parameter_ids_overlap
def _validate_no_parameter_ids_overlap(self, name: str, param_ids: dict):
        for registered_name, registered_param_ids in self.param_ids.items():
            if registered_name != name:
                overlap = set.intersection(registered_param_ids.keys(), param_ids.keys())
                assert len(overlap) == 0, f"Found {len(overlap)} / {len(param_ids.keys())} overlapping parameters"
                f" in {name} and already registered {registered_name}: {' '.join(overlap)}"
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestFeatureMatchingLoss.test_features_matching_loss_raises_exception
def test_features_matching_loss_raises_exception(self):
        ft_matching_loss = FeatureMatchingLoss()
        length = random.randrange(1, 100_000)
        t1 = torch.randn(1, 2, length)
        t2 = torch.randn(1, 2, length + 1)

        with pytest.raises(AssertionError):
            ft_matching_loss([], [])

        with pytest.raises(AssertionError):
            ft_matching_loss([t1], [t1, t1])

        with pytest.raises(AssertionError):
            ft_matching_loss([t1], [t2])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestConvertAudio.test_convert_audio_upsample
def test_convert_audio_upsample(self):
        b, c, dur = 2, 1, 4.
        sr = 2
        new_sr = 3
        audio = get_batch_white_noise(b, c, int(sr * dur))
        out = convert_audio(audio, from_rate=sr, to_rate=new_sr, to_channels=c)
        out_j = julius.resample.resample_frac(audio, old_sr=sr, new_sr=new_sr)
        assert torch.allclose(out, out_j)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, NoopTokenizer.__init__
def __init__(self, n_bins: int, pad_idx: int = 0):
        self.n_bins = n_bins
        self.pad_idx = pad_idx
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, MultiBandDiffusion.get_condition
def get_condition(self, wav: torch.Tensor, sample_rate: int) -> torch.Tensor:
        """Get the conditioning (i.e. latent reprentatios of the compression model) from a waveform.
        Args:
            wav (torch.Tensor): The audio that we want to extract the conditioning from
            sample_rate (int): sample rate of the audio"""
        if sample_rate != self.sample_rate:
            wav = julius.resample_frac(wav, sample_rate, self.sample_rate)
        codes, scale = self.codec_model.encode(wav)
        assert scale is None, "Scaled compression models not supported."
        emb = self.get_emb(codes)
        return emb
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msd.py
BlockTypes.METHOD, MultiScaleDiscriminator.num_discriminators
def num_discriminators(self):
        return len(self.discriminators)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, NoiseSchedule.get_initial_noise
def get_initial_noise(self, x: torch.Tensor):
        if self.n_bands is None:
            return torch.randn_like(x)
        return torch.randn((x.size(0), self.n_bands, x.size(2)))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, predict_batched
def predict_batched(texts, melodies):
    max_text_length = 512
    texts = [text[:max_text_length] for text in texts]
    load_model('facebook/musicgen-melody')
    res = _do_predictions(texts, melodies, BATCHED_DURATION)
    return res
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\base.py
BlockTypes.METHOD, MultiDiscriminator.forward
def forward(self, x: torch.Tensor) -> MultiDiscriminatorOutputType:
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, BaseQuantizer.decode
def decode(self, codes: torch.Tensor) -> torch.Tensor:
        """Decode the given codes to the quantized representation."""
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver._load_new_state_dict
def _load_new_state_dict(self, state_dict: dict) -> dict:
        old_states = {}
        for name, new_state in state_dict.items():
            state_source = self._get_state_source(name)
            old_states[name] = copy_state(state_source.state_dict())
            state_source.load_state_dict(new_state)
        return old_states
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, TestStreamableConvTranspose1d.get_streamable_convtr1d_output_length
def get_streamable_convtr1d_output_length(self, length, kernel_size, stride):
        padding_total = (kernel_size - stride)
        return (length - 1) * stride - padding_total + (kernel_size - 1) + 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\zip.py
BlockTypes.METHOD, open_file_in_zip
def open_file_in_zip(path_in_zip: PathInZip, mode: str = 'r') -> typing.IO:
    """Opens a file stored inside a zip and returns a file-like object.

    Args:
        path_in_zip (PathInZip): A PathInZip object representing the file to return a file-like object of.
        mode (str): The mode in which to open the file with.
    Returns:
        A file-like object for PathInZip.
    """
    zf = _cached_open_zip(path_in_zip.zip_path)
    return zf.open(path_in_zip.file_path)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, BLSTM.forward
def forward(self, x):
        x = x.permute(2, 0, 1)
        x = self.lstm(x)[0]
        x = self.linear(x)
        x = x.permute(1, 2, 0)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, WhiteSpaceTokenizer.__init__
def __init__(self, n_bins: int, pad_idx: int = 0, language: str = "en_core_web_sm",
                 lemma: bool = True, stopwords: bool = True) -> None:
        self.n_bins = n_bins
        self.pad_idx = pad_idx
        self.lemma = lemma
        self.stopwords = stopwords
        try:
            self.nlp = spacy.load(language)
        except IOError:
            spacy.cli.download(language)  # type: ignore
            self.nlp = spacy.load(language)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.register_best_state
def register_best_state(self, *args: str):
        """Register state sources in `BestStateDictManager` to keep their best states along with their
        latest states. The best state will be used at evaluation stages instead of the latest states.

        Shortcut around `BestStateDictManager.register` method. You can pass any number of
        attribute, included nested attributes and those will be included into the checkpoints
        and automatically restored when `BaseSolver.restore` is called.
        """
        for name in args:
            state_source = self._get_state_source(name)
            assert name in self.stateful.sources, "Registered states in best should be registered in stateful first!"
            self.best_state.register(name, state_source)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchWriter.save
def save(self, *content):
        """Save one mini batch. This function is distributed-aware
        and will automatically merge all the items from the different
        workers.
        """
        all_contents = []
        for rank in range(flashy.distrib.world_size()):
            their_content = flashy.distrib.broadcast_object(content, src=rank)
            all_contents.append(their_content)

        if flashy.distrib.is_rank_zero():
            idx = 0
            with flashy.utils.write_and_rename(self._zip_path) as tmp:
                with zipfile.ZipFile(tmp, 'w') as zf:
                    for content in all_contents:
                        for vals in zip(*content):
                            with zf.open(f'{idx}', 'w') as f:  # type: ignore
                                torch.save(vals, f)
                            idx += 1
        flashy.distrib.barrier()
        self._current_index += 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\_explorers.py
BlockTypes.METHOD, GenerationEvalExplorer.stages
def stages(self) -> tp.List[str]:
        return ['evaluate']
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.forward
def forward(self, x):
        shape, dtype = x.shape, x.dtype
        x = self.preprocess(x)
        self.init_embed_(x)

        embed_ind = self.quantize(x)
        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)
        embed_ind = self.postprocess_emb(embed_ind, shape)
        quantize = self.dequantize(embed_ind)

        if self.training:
            # We do the expiry of code at that point as buffers are in sync
            # and all the workers will take the same decision.
            self.expire_codes_(x)
            ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)
            embed_sum = x.t() @ embed_onehot
            ema_inplace(self.embed_avg, embed_sum.t(), self.decay)
            cluster_size = (
                laplace_smoothing(self.cluster_size, self.codebook_size, self.epsilon)
                * self.cluster_size.sum()
            )
            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)
            self.embed.data.copy_(embed_normalized)

        return quantize, embed_ind
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, copy_state
def copy_state(state: tp.Any, device: tp.Union[torch.device, str] = 'cpu',
               dtype: tp.Optional[torch.dtype] = None) -> tp.Any:
    if isinstance(state, torch.Tensor):
        if dtype is None or not state.is_floating_point():
            dtype = state.dtype
        return state.detach().to(device=device, dtype=dtype, copy=True)
    elif isinstance(state, dict):
        return {k: copy_state(v, device, dtype) for k, v in state.items()}
    elif isinstance(state, list):
        return [copy_state(v, device, dtype) for v in state]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msstftd.py
BlockTypes.METHOD, MultiScaleSTFTDiscriminator.__init__
def __init__(self, filters: int, in_channels: int = 1, out_channels: int = 1, sep_channels: bool = False,
                 n_ffts: tp.List[int] = [1024, 2048, 512], hop_lengths: tp.List[int] = [256, 512, 128],
                 win_lengths: tp.List[int] = [1024, 2048, 512], **kwargs):
        super().__init__()
        assert len(n_ffts) == len(hop_lengths) == len(win_lengths)
        self.sep_channels = sep_channels
        self.discriminators = nn.ModuleList([
            DiscriminatorSTFT(filters, in_channels=in_channels, out_channels=out_channels,
                              n_fft=n_ffts[i], win_length=win_lengths[i], hop_length=hop_lengths[i], **kwargs)
            for i in range(len(n_ffts))
        ])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, KLDivergenceMetric.compute
def compute(self) -> dict:
        """Computes KL-Divergence across all evaluated pred/target pairs."""
        weight: float = float(self.weight.item())  # type: ignore
        assert weight > 0, "Unable to compute with total number of comparisons <= 0"
        logger.info(f"Computing KL divergence on a total of {weight} samples")
        kld_pq = self.kld_pq_sum.item() / weight  # type: ignore
        kld_qp = self.kld_qp_sum.item() / weight  # type: ignore
        kld_both = kld_pq + kld_qp
        return {'kld': kld_pq, 'kld_pq': kld_pq, 'kld_qp': kld_qp, 'kld_both': kld_both}
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.evaluate
def evaluate(self):
        """Evaluate stage.
        Runs audio reconstruction evaluation.
        """
        self.model.eval()
        evaluate_stage_name = f'{self.current_stage}'
        loader = self.dataloaders['evaluate']
        updates = len(loader)
        lp = self.log_progress(f'{evaluate_stage_name} estimate', loader, total=updates, updates=self.log_updates)

        metrics = {}
        n = 1
        for idx, batch in enumerate(lp):
            x = batch.to(self.device)
            with torch.no_grad():
                y_pred = self.regenerate(x)

            y_pred = y_pred.cpu()
            y = batch.cpu()  # should already be on CPU but just in case
            rvm = self.rvm(y_pred, y)
            lp.update(**rvm)
            if len(metrics) == 0:
                metrics = rvm
            else:
                for key in rvm.keys():
                    metrics[key] = (metrics[key] * n + rvm[key]) / (n + 1)
        metrics = flashy.distrib.average_metrics(metrics)
        return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioMeta.to_dict
def to_dict(self):
        d = super().to_dict()
        if d['info_path'] is not None:
            d['info_path'] = str(d['info_path'])
        return d
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\visqol.py
BlockTypes.METHOD, ViSQOL.__init__
def __init__(self, bin: tp.Union[Path, str], mode: str = "audio",
                 model: str = "libsvm_nu_svr_model.txt", debug: bool = False):
        assert bin is not None and Path(bin).exists(), f"Could not find ViSQOL binary in specified path: {bin}"
        self.visqol_bin = str(bin)
        self.visqol_mode = mode
        self.target_sr = self._get_target_sr(self.visqol_mode)
        self.model = model
        self.debug = debug
        assert Path(self.visqol_model).exists(), \
            f"Could not find the specified model in ViSQOL install: {self.visqol_model}"
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, get_norm_module
def get_norm_module(module: nn.Module, causal: bool = False, norm: str = 'none', **norm_kwargs):
    """Return the proper normalization module. If causal is True, this will ensure the returned
    module is causal, or return an error if the normalization doesn't support causal evaluation.
    """
    assert norm in CONV_NORMALIZATIONS
    if norm == 'time_group_norm':
        if causal:
            raise ValueError("GroupNorm doesn't support causal evaluation.")
        assert isinstance(module, nn.modules.conv._ConvNd)
        return nn.GroupNorm(1, module.out_channels, **norm_kwargs)
    else:
        return nn.Identity()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.total_codebooks
def total_codebooks(self):
        """Total number of quantizer codebooks available."""
        return self.quantizer.total_codebooks
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, MultiBandProcessor.std
def std(self):
        std = (self.sum_x2 / self.counts - self.mean**2).clamp(min=0).sqrt()
        return std
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\ema.py
BlockTypes.METHOD, _get_all_non_persistent_buffers_set
def _get_all_non_persistent_buffers_set(module: nn.Module, root: str = "") -> set:
    names: set = set()
    for (name, sub_module) in module.named_modules():
        if name == '':
            buffer_names = module._non_persistent_buffers_set
            buffer_names = {f"{root}.{buff_name}" if len(root) > 0 else buff_name
                            for buff_name in buffer_names}
            names.update(buffer_names)
        else:
            sub_name = f"{root}.{name}" if len(root) > 0 else name
            sub_buffer_names = _get_all_non_persistent_buffers_set(sub_module, sub_name)
            names.update(sub_buffer_names)
    return names
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, DummyPoolExecutor.DummyResult.__init__
def __init__(self, func, *args, **kwargs):
            self.func = func
            self.args = args
            self.kwargs = kwargs
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, get_fake_criterion
def get_fake_criterion(loss_type: str) -> tp.Callable:
    assert loss_type in ADVERSARIAL_LOSSES
    if loss_type == 'mse':
        return mse_fake_loss
    elif loss_type in ['hinge', 'hinge2']:
        return hinge_fake_loss
    raise ValueError('Unsupported loss')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msd.py
BlockTypes.METHOD, ScaleDiscriminator.__init__
def __init__(self, in_channels=1, out_channels=1, kernel_sizes: tp.Sequence[int] = [5, 3],
                 filters: int = 16, max_filters: int = 1024, downsample_scales: tp.Sequence[int] = [4, 4, 4, 4],
                 inner_kernel_sizes: tp.Optional[tp.Sequence[int]] = None, groups: tp.Optional[tp.Sequence[int]] = None,
                 strides: tp.Optional[tp.Sequence[int]] = None, paddings: tp.Optional[tp.Sequence[int]] = None,
                 norm: str = 'weight_norm', activation: str = 'LeakyReLU',
                 activation_params: dict = {'negative_slope': 0.2}, pad: str = 'ReflectionPad1d',
                 pad_params: dict = {}):
        super().__init__()
        assert len(kernel_sizes) == 2
        assert kernel_sizes[0] % 2 == 1
        assert kernel_sizes[1] % 2 == 1
        assert (inner_kernel_sizes is None or len(inner_kernel_sizes) == len(downsample_scales))
        assert (groups is None or len(groups) == len(downsample_scales))
        assert (strides is None or len(strides) == len(downsample_scales))
        assert (paddings is None or len(paddings) == len(downsample_scales))
        self.activation = getattr(torch.nn, activation)(**activation_params)
        self.convs = nn.ModuleList()
        self.convs.append(
            nn.Sequential(
                getattr(torch.nn, pad)((np.prod(kernel_sizes) - 1) // 2, **pad_params),
                NormConv1d(in_channels, filters, kernel_size=np.prod(kernel_sizes), stride=1, norm=norm)
            )
        )

        in_chs = filters
        for i, downsample_scale in enumerate(downsample_scales):
            out_chs = min(in_chs * downsample_scale, max_filters)
            default_kernel_size = downsample_scale * 10 + 1
            default_stride = downsample_scale
            default_padding = (default_kernel_size - 1) // 2
            default_groups = in_chs // 4
            self.convs.append(
                NormConv1d(in_chs, out_chs,
                           kernel_size=inner_kernel_sizes[i] if inner_kernel_sizes else default_kernel_size,
                           stride=strides[i] if strides else default_stride,
                           groups=groups[i] if groups else default_groups,
                           padding=paddings[i] if paddings else default_padding,
                           norm=norm))
            in_chs = out_chs

        out_chs = min(in_chs * 2, max_filters)
        self.convs.append(NormConv1d(in_chs, out_chs, kernel_size=kernel_sizes[0], stride=1,
                                     padding=(kernel_sizes[0] - 1) // 2, norm=norm))
        self.conv_post = NormConv1d(out_chs, out_channels, kernel_size=kernel_sizes[1], stride=1,
                                    padding=(kernel_sizes[1] - 1) // 2, norm=norm)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\_explorers.py
BlockTypes.METHOD, GenerationEvalExplorer.get_grid_metrics
def get_grid_metrics(self):
        """Return the metrics that should be displayed in the tracking table."""
        return [
            tt.group(
                'evaluate',
                [
                    tt.leaf('epoch', '.3f'),
                    tt.leaf('duration', '.1f'),
                    tt.leaf('ping'),
                    tt.leaf('ce', '.4f'),
                    tt.leaf('ppl', '.3f'),
                    tt.leaf('fad', '.3f'),
                    tt.leaf('kld', '.3f'),
                    tt.leaf('text_consistency', '.3f'),
                    tt.leaf('chroma_cosine', '.3f'),
                ],
                align='>',
            ),
        ]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.load_state_dict
def load_state_dict(self, state: dict) -> None:
        if 'condition_provider' in state:
            model_state = state['model']
            condition_provider_state = state.pop('condition_provider')
            prefix = 'condition_provider.'
            for key, value in condition_provider_state.items():
                key = prefix + key
                assert key not in model_state
                model_state[key] = value
        super().load_state_dict(state)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.__init__
def __init__(self, model: HFEncodecModel):
        super().__init__()
        self.model = model
        bws = self.model.config.target_bandwidths
        num_codebooks = [
            bw * 1000 / (self.frame_rate * math.log2(self.cardinality))
            for bw in bws
        ]
        deltas = [nc - int(nc) for nc in num_codebooks]
        # Checking we didn't do some bad maths and we indeed have integers!
        assert all(deltas) <= 1e-3, deltas
        self.possible_num_codebooks = [int(nc) for nc in num_codebooks]
        self.set_num_codebooks(max(self.possible_num_codebooks))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, NoopTokenizer.__call__
def __call__(self, texts: tp.List[tp.Optional[str]]) -> tp.Tuple[torch.Tensor, torch.Tensor]:
        output, lengths = [], []
        for text in texts:
            # if current sample doesn't have a certain attribute, replace with pad token
            if text is None:
                output.append(self.pad_idx)
                lengths.append(0)
            else:
                output.append(hash_trick(text, self.n_bins))
                lengths.append(1)

        tokens = torch.LongTensor(output).unsqueeze(1)
        mask = length_to_mask(torch.IntTensor(lengths)).int()
        return tokens, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, T5Conditioner.__init__
def __init__(self, name: str, output_dim: int, finetune: bool, device: str,
                 autocast_dtype: tp.Optional[str] = 'float32', word_dropout: float = 0.,
                 normalize_text: bool = False):
        assert name in self.MODELS, f"Unrecognized t5 model name (should in {self.MODELS})"
        super().__init__(self.MODELS_DIMS[name], output_dim)
        self.device = device
        self.name = name
        self.finetune = finetune
        self.word_dropout = word_dropout
        if autocast_dtype is None or self.device == 'cpu':
            self.autocast = TorchAutocast(enabled=False)
            if self.device != 'cpu':
                logger.warning("T5 has no autocast, this might lead to NaN")
        else:
            dtype = getattr(torch, autocast_dtype)
            assert isinstance(dtype, torch.dtype)
            logger.info(f"T5 will be evaluated with autocast as {autocast_dtype}")
            self.autocast = TorchAutocast(enabled=True, device_type=self.device, dtype=dtype)
        # Let's disable logging temporarily because T5 will vomit some errors otherwise.
        # thanks https://gist.github.com/simon-weber/7853144
        previous_level = logging.root.manager.disable
        logging.disable(logging.ERROR)
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            try:
                self.t5_tokenizer = T5Tokenizer.from_pretrained(name)
                t5 = T5EncoderModel.from_pretrained(name).train(mode=finetune)
            finally:
                logging.disable(previous_level)
        if finetune:
            self.t5 = t5
        else:
            # this makes sure that the t5 models is not part
            # of the saved checkpoint
            self.__dict__['t5'] = t5.to(device)

        self.normalize_text = normalize_text
        if normalize_text:
            self.text_normalizer = WhiteSpaceTokenizer(1, lemma=True, stopwords=True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_optim_parameter_groups
def get_optim_parameter_groups(model: nn.Module):
    """Create parameter groups for the model using the appropriate method
    if defined for each modules, to create the different groups.

    Args:
        model (nn.Module): torch model
    Returns:
        List of parameter groups
    """
    seen_params: tp.Set[nn.parameter.Parameter] = set()
    other_params = []
    groups = []
    for name, module in model.named_modules():
        if hasattr(module, 'make_optim_group'):
            group = module.make_optim_group()
            params = set(group['params'])
            assert params.isdisjoint(seen_params)
            seen_params |= set(params)
            groups.append(group)
    for param in model.parameters():
        if param not in seen_params:
            other_params.append(param)
    groups.insert(0, {'params': other_params})
    parameters = groups
    return parameters
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_codebooks_patterns.py
BlockTypes.METHOD, TestPattern.ref_revert_pattern_sequence
def ref_revert_pattern_sequence(self, z: torch.Tensor, pattern: Pattern, special_token: int):
        """Reference method to revert the sequence from the pattern without using fancy scatter."""
        z = z.cpu().numpy()
        bs, n_q, S = z.shape
        assert pattern.n_q == n_q
        inp = torch.full((bs, pattern.n_q, pattern.timesteps), special_token, dtype=torch.long).numpy()
        inp[:] = special_token
        for s, v in enumerate(pattern.layout):
            for (t, q) in v:
                if t < pattern.timesteps:
                    inp[:, q, t] = z[:, q, s]
        return torch.from_numpy(inp)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, TestStreamableConvTranspose1d.test_streamable_convtr1d
def test_streamable_convtr1d(self):
        N, C, T = 2, 2, random.randrange(1, 100_000)
        t0 = torch.randn(N, C, T)

        C_out = 1

        with pytest.raises(AssertionError):
            StreamableConvTranspose1d(C, C_out, kernel_size=4, causal=False, trim_right_ratio=0.5)
            StreamableConvTranspose1d(C, C_out, kernel_size=4, causal=True, trim_right_ratio=-1.)
            StreamableConvTranspose1d(C, C_out, kernel_size=4, causal=True, trim_right_ratio=2)

        # causal params are [(causal, trim_right)]
        causal_params = [(False, 1.0), (True, 1.0), (True, 0.5), (True, 0.0)]
        # conv params are [(kernel_size, stride)]
        conv_params = [(4, 1), (4, 2), (3, 1), (10, 5)]
        for ((causal, trim_right_ratio), (kernel_size, stride)) in product(causal_params, conv_params):
            expected_out_length = self.get_streamable_convtr1d_output_length(T, kernel_size, stride)
            sconvtr = StreamableConvTranspose1d(C, C_out, kernel_size=kernel_size, stride=stride,
                                                causal=causal, trim_right_ratio=trim_right_ratio)
            out = sconvtr(t0)
            assert isinstance(out, torch.Tensor)
            assert list(out.shape) == [N, C_out, expected_out_length]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_utils.py
BlockTypes.METHOD, _clip_wav
def _clip_wav(wav: torch.Tensor, log_clipping: bool = False, stem_name: tp.Optional[str] = None) -> None:
    """Utility function to clip the audio with logging if specified."""
    max_scale = wav.abs().max()
    if log_clipping and max_scale > 1:
        clamp_prob = (wav.abs() > 1).float().mean().item()
        print(f"CLIPPING {stem_name or ''} happening with proba (a bit of clipping is okay):",
              clamp_prob, "maximum scale: ", max_scale.item(), file=sys.stderr)
    wav.clamp_(-1, 1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric._log_process_result
def _log_process_result(self, returncode: int, log_file: tp.Union[Path, str], is_background: bool) -> None:
        beams_name = self._get_samples_name(is_background)
        if returncode:
            with open(log_file, "r") as f:
                error_log = f.read()
                logger.error(error_log)
            os._exit(1)
        else:
            logger.info(f"Successfully computed embedding beams on {beams_name} samples.")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, NoiseSchedule.generate_subsampled
def generate_subsampled(self, model: torch.nn.Module, initial: torch.Tensor, step_list: tp.Optional[list] = None,
                            condition: tp.Optional[torch.Tensor] = None, return_list: bool = False):
        """Reverse process that only goes through Markov chain states in step_list."""
        if step_list is None:
            step_list = list(range(1000))[::-50] + [0]
        alpha_bar = self.get_alpha_bar(step=self.num_steps - 1)
        alpha_bars_subsampled = (1 - self.betas).cumprod(dim=0)[list(reversed(step_list))].cpu()
        betas_subsampled = betas_from_alpha_bar(alpha_bars_subsampled)
        current = initial * self.noise_scale
        iterates = [current]
        for idx, step in enumerate(step_list[:-1]):
            with torch.no_grad():
                estimate = model(current, step, condition=condition).sample * self.noise_scale
            alpha = 1 - betas_subsampled[-1 - idx]
            previous = (current - (1 - alpha) / (1 - alpha_bar).sqrt() * estimate) / alpha.sqrt()
            previous_alpha_bar = self.get_alpha_bar(step_list[idx + 1])
            if step == step_list[-2]:
                sigma2 = 0
                previous_alpha_bar = torch.tensor(1.0)
            else:
                sigma2 = (1 - previous_alpha_bar) / (1 - alpha_bar) * (1 - alpha)
            if sigma2 > 0:
                previous += sigma2**0.5 * torch.randn_like(previous) * self.noise_scale
            if self.clip:
                previous = previous.clamp(-self.clip, self.clip)
            current = previous
            alpha_bar = previous_alpha_bar
            if step == 0:
                previous *= self.rescale
            if return_list:
                iterates.append(previous.cpu())
        if return_list:
            return iterates
        else:
            return self.sample_processor.return_sample(previous)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, BaseQuantizer.total_codebooks
def total_codebooks(self):
        """Total number of codebooks."""
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.model_from_checkpoint
def model_from_checkpoint(checkpoint_path: tp.Union[Path, str],
                              device: tp.Union[torch.device, str] = 'cpu') -> models.CompressionModel:
        """Instantiate a CompressionModel from a given checkpoint path or dora sig.
        This method is a convenient endpoint to load a CompressionModel to use in other solvers.

        Args:
            checkpoint_path (Path or str): Path to checkpoint or dora sig from where the checkpoint is resolved.
                This also supports pre-trained models by using a path of the form //pretrained/NAME.
                See `model_from_pretrained` for a list of supported pretrained models.
            use_ema (bool): Use EMA variant of the model instead of the actual model.
            device (torch.device or str): Device on which the model is loaded.
        """
        checkpoint_path = str(checkpoint_path)
        if checkpoint_path.startswith('//pretrained/'):
            name = checkpoint_path.split('/', 3)[-1]
            return models.CompressionModel.get_pretrained(name, device)
        logger = logging.getLogger(__name__)
        logger.info(f"Loading compression model from checkpoint: {checkpoint_path}")
        _checkpoint_path = checkpoint.resolve_checkpoint_path(checkpoint_path, use_fsdp=False)
        assert _checkpoint_path is not None, f"Could not resolve compression model checkpoint path: {checkpoint_path}"
        state = checkpoint.load_checkpoint(_checkpoint_path)
        assert state is not None and 'xp.cfg' in state, f"Could not load compression model from ckpt: {checkpoint_path}"
        cfg = state['xp.cfg']
        cfg.device = device
        compression_model = models.builders.get_compression_model(cfg).to(device)
        assert compression_model.sample_rate == cfg.sample_rate, "Compression model sample rate should match"

        assert 'best_state' in state and state['best_state'] != {}
        assert 'exported' not in state, "When loading an exported checkpoint, use the //pretrained/ prefix."
        compression_model.load_state_dict(state['best_state']['model'])
        compression_model.eval()
        logger.info("Compression model loaded!")
        return compression_model
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.generate
def generate(self) -> dict:
        """Generate stage."""
        self.model.eval()
        with torch.no_grad():
            return self.generate_audio()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\clap_consistency.py
BlockTypes.METHOD, CLAPTextConsistencyMetric.compute
def compute(self):
        """Computes the average cosine similarty across all audio/text pairs."""
        assert self.weight.item() > 0, "Unable to compute with total number of comparisons <= 0"  # type: ignore
        return (self.cosine_sum / self.weight).item()  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, JointEmbeddingConditioner.tokenize
def tokenize(self, x: JointEmbedCondition) -> JointEmbedCondition:
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, _verify_xformers_internal_compat
def _verify_xformers_internal_compat():
    try:
        from xformers.checkpoint_fairinternal import checkpoint, _get_default_policy  # noqa
    except ImportError:
        raise ImportError(
            "Francisco's fairinternal xformers is not installed. Please install it and try again.\n"
            "To install on AWS and Azure, run \n"
            "FORCE_CUDA=1 TORCH_CUDA_ARCH_LIST='8.0'\\\n"
            "pip install -U git+https://git@github.com/fairinternal/xformers.git#egg=xformers\n"
            "To install on FAIR Cluster, run \n"
            "FORCE_CUDA=1 TORCH_CUDA_ARCH_LIST='6.0;7.0'\\\n"
            "pip install -U git+https://git@github.com/fairinternal/xformers.git#egg=xformers\n")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditionFuser.__init__
def __init__(self, fuse2cond: tp.Dict[str, tp.List[str]], cross_attention_pos_emb: bool = False,
                 cross_attention_pos_emb_scale: float = 1.0):
        super().__init__()
        assert all(
            [k in self.FUSING_METHODS for k in fuse2cond.keys()]
        ), f"Got invalid fuse method, allowed methods: {self.FUSING_METHODS}"
        self.cross_attention_pos_emb = cross_attention_pos_emb
        self.cross_attention_pos_emb_scale = cross_attention_pos_emb_scale
        self.fuse2cond: tp.Dict[str, tp.List[str]] = fuse2cond
        self.cond2fuse: tp.Dict[str, str] = {}
        for fuse_method, conditions in fuse2cond.items():
            for condition in conditions:
                self.cond2fuse[condition] = fuse_method
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DataProcess.__init__
def __init__(self, initial_sr: int = 24000, target_sr: int = 16000, use_resampling: bool = False,
                 use_filter: bool = False, n_bands: int = 4,
                 idx_band: int = 0, device: torch.device = torch.device('cpu'), cutoffs=None, boost=False):
        """Apply filtering or resampling
        Args:
            initial_sr (int): sample rate of the dataset
            target_sr (int): sample rate after resampling
            use_resampling (bool): whether or not performs resampling
            use_filter (bool): when True filter the data to keep only one frequency band
            n_bands (int): Number of bands used
            cuts (none or list): The cutoff frequencies of the band filtering
                                if None then we use mel scale bands.
            idx_band (int): index of the frequency band. 0 are lows ... (n_bands - 1) highs
            boost (bool): make the data scale match our music dataset.
        """
        assert idx_band < n_bands
        self.idx_band = idx_band
        if use_filter:
            if cutoffs is not None:
                self.filter = julius.SplitBands(sample_rate=initial_sr, cutoffs=cutoffs).to(device)
            else:
                self.filter = julius.SplitBands(sample_rate=initial_sr, n_bands=n_bands).to(device)
        self.use_filter = use_filter
        self.use_resampling = use_resampling
        self.target_sr = target_sr
        self.initial_sr = initial_sr
        self.boost = boost
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.num_codebooks
def num_codebooks(self):
        """Active number of codebooks used by the quantizer."""
        return self.quantizer.num_codebooks
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, DelayedPatternProvider.get_pattern
def get_pattern(self, timesteps: int) -> Pattern:
        out: PatternLayout = [[]]
        max_delay = max(self.delays)
        if self.empty_initial:
            out += [[] for _ in range(self.empty_initial)]
        if self.flatten_first:
            for t in range(min(timesteps, self.flatten_first)):
                for q in range(self.n_q):
                    out.append([LayoutCoord(t, q)])
        for t in range(self.flatten_first, timesteps + max_delay):
            v = []
            for q, delay in enumerate(self.delays):
                t_for_q = t - delay
                if t_for_q >= self.flatten_first:
                    v.append(LayoutCoord(t_for_q, q))
            out.append(v)
        return Pattern(out, n_q=self.n_q, timesteps=timesteps)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\audiogen\audiogen_base_16khz.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=64, partition=partitions)
    launcher.bind_(solver='audiogen/audiogen_base_16khz')
    # replace this by the desired environmental sound dataset
    launcher.bind_(dset='internal/sounds_16khz')

    fsdp = {'autocast': False, 'fsdp.use': True}
    medium = {'model/lm/model_scale': 'medium'}

    launcher.bind_(fsdp)
    launcher(medium)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, NoiseSchedule.get_alpha_bar
def get_alpha_bar(self, step: tp.Optional[tp.Union[int, torch.Tensor]] = None) -> torch.Tensor:
        """Return 'alpha_bar', either for a given step, or as a tensor with its value for each step."""
        if step is None:
            return (1 - self.betas).cumprod(dim=-1)  # works for simgle and multi bands
        if type(step) is int:
            return (1 - self.betas[:step + 1]).prod()
        else:
            return (1 - self.betas).cumprod(dim=0)[step].view(-1, 1, 1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, MultiBandProcessor.target_std
def target_std(self):
        target_std = self.sum_target_x2 / self.counts
        return target_std
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.swap_ema_state
def swap_ema_state(self):
        if self.ema is None:
            yield
        else:
            ema_state_dict = self.ema.state_dict()['state']
            self.logger.debug(f"Swapping to EMA state for: {', '.join(ema_state_dict.keys())}")
            old_states = self._load_new_state_dict(ema_state_dict)
            try:
                yield
            finally:
                self.logger.debug("Swapping back from EMA state to original state")
                for name, old_state in old_states.items():
                    state_source = self._get_state_source(name)
                    state_source.load_state_dict(old_state)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\sisnr.py
BlockTypes.METHOD, _center
def _center(x: torch.Tensor) -> torch.Tensor:
    return x - x.mean(-1, True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.should_run_stage
def should_run_stage(self, stage_name) -> bool:
        """Check whether we want to run the specified stages."""
        stage_every = self.cfg[stage_name].get('every', None)
        is_last_epoch = self.epoch == self.cfg.optim.epochs
        is_epoch_every = (stage_every and self.epoch % stage_every == 0)
        return is_last_epoch or is_epoch_every
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestConvertAudio.test_convert_audio_resample
def test_convert_audio_resample(self):
        b, c, dur = 2, 1, 4.
        sr = 3
        new_sr = 2
        audio = get_batch_white_noise(b, c, int(sr * dur))
        out = convert_audio(audio, from_rate=sr, to_rate=new_sr, to_channels=c)
        out_j = julius.resample.resample_frac(audio, old_sr=sr, new_sr=new_sr)
        assert torch.allclose(out, out_j)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_codebooks_patterns.py
BlockTypes.METHOD, TestPattern.ref_revert_pattern_logits
def ref_revert_pattern_logits(self, z: torch.Tensor, pattern: Pattern, special_token: float):
        """Reference method to revert the logits from the pattern without using fancy scatter."""
        z = z.cpu().numpy()
        bs, card, n_q, S = z.shape
        assert pattern.n_q == n_q
        ref_layout = pattern.layout
        inp = torch.full((bs, card, pattern.n_q, pattern.timesteps), special_token, dtype=torch.float).numpy()
        inp[:] = special_token
        for s, v in enumerate(ref_layout[1:]):
            if s < S:
                for (t, q) in v:
                    if t < pattern.timesteps:
                        inp[:, :, q, t] = z[:, :, q, s]
        return torch.from_numpy(inp)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\vq.py
BlockTypes.METHOD, ResidualVectorQuantizer.decode
def decode(self, codes: torch.Tensor) -> torch.Tensor:
        """Decode the given codes to the quantized representation."""
        # codes is [B, K, T], with T frames, K nb of codebooks, vq.decode expects [K, B, T].
        codes = codes.transpose(0, 1)
        quantized = self.vq.decode(codes)
        return quantized
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, BaseQuantizer.num_codebooks
def num_codebooks(self):
        """Number of active codebooks."""
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, predict_full
def predict_full(model, decoder, text, melody, duration, topk, topp, temperature, cfg_coef, progress=gr.Progress()):
    global INTERRUPTING
    global USE_DIFFUSION
    INTERRUPTING = False
    if temperature < 0:
        raise gr.Error("Temperature must be >= 0.")
    if topk < 0:
        raise gr.Error("Topk must be non-negative.")
    if topp < 0:
        raise gr.Error("Topp must be non-negative.")

    topk = int(topk)
    if decoder == "MultiBand_Diffusion":
        USE_DIFFUSION = True
        load_diffusion()
    else:
        USE_DIFFUSION = False
    load_model(model)

    def _progress(generated, to_generate):
        progress((min(generated, to_generate), to_generate))
        if INTERRUPTING:
            raise gr.Error("Interrupted.")
    MODEL.set_custom_progress_callback(_progress)

    videos, wavs = _do_predictions(
        [text], [melody], duration, progress=True,
        top_k=topk, top_p=topp, temperature=temperature, cfg_coef=cfg_coef)
    if USE_DIFFUSION:
        return videos[0], wavs[0], videos[1], wavs[1]
    return videos[0], wavs[0], None, None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, DummyPoolExecutor.DummyResult.result
def result(self):
            return self.func(*self.args, **self.kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_seanet.py
BlockTypes.METHOD, TestSEANetModel._check_encoder_blocks_norm
def _check_encoder_blocks_norm(self, encoder: SEANetEncoder, n_disable_blocks: int, norm: str):
        n_blocks = 0
        for layer in encoder.model:
            if isinstance(layer, StreamableConv1d):
                n_blocks += 1
                assert layer.conv.norm_type == 'none' if n_blocks <= n_disable_blocks else norm
            elif isinstance(layer, SEANetResnetBlock):
                for resnet_layer in layer.block:
                    if isinstance(resnet_layer, StreamableConv1d):
                        # here we add + 1 to n_blocks as we increment n_blocks just after the block
                        assert resnet_layer.conv.norm_type == 'none' if (n_blocks + 1) <= n_disable_blocks else norm
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\seanet.py
BlockTypes.METHOD, SEANetDecoder.forward
def forward(self, z):
        y = self.model(z)
        return y
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\export.py
BlockTypes.METHOD, export_lm
def export_lm(checkpoint_path: tp.Union[Path, str], out_file: tp.Union[Path, str]):
    """Export only the best state from the given MusicGen or AudioGen checkpoint.
    """
    pkg = torch.load(checkpoint_path, 'cpu')
    if pkg['fsdp_best_state']:
        best_state = pkg['fsdp_best_state']['model']
    else:
        assert pkg['best_state']
        best_state = pkg['best_state']['model']
    new_pkg = {
        'best_state': best_state,
        'xp.cfg': OmegaConf.to_yaml(pkg['xp.cfg']),
        'version': __version__,
        'exported': True,
    }

    Path(out_file).parent.mkdir(exist_ok=True, parents=True)
    torch.save(new_pkg, out_file)
    return out_file
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\resample_dataset.py
BlockTypes.METHOD, process_dataset
def process_dataset(args, n_shards: int, node_index: int, task_index: tp.Optional[int] = None):
    if task_index is None:
        env = submitit.JobEnvironment()
        task_index = env.global_rank
    shard_index = node_index * args.tasks_per_node + task_index

    if args.files_path is None:
        lines = [m.path for m in find_audio_files(args.root_path, resolve=False, progress=True, workers=8)]
    else:
        files_path = Path(args.files_path)
        if files_path.suffix == '.txt':
            print(f"Reading file list from .txt file: {args.files_path}")
            lines = read_txt_files(args.files_path)
        else:
            print(f"Reading file list from egs: {args.files_path}")
            lines = read_egs_files(args.files_path)

    total_files = len(lines)
    print(
        f"Total of {total_files} processed with {n_shards} shards. " +
        f"Current idx = {shard_index} -> {total_files // n_shards} files to process"
    )
    for idx, line in tqdm.tqdm(enumerate(lines)):

        # skip if not part of this shard
        if idx % n_shards != shard_index:
            continue

        path = str(AudioCraftEnvironment.apply_dataset_mappers(line))
        root_path = str(args.root_path)
        if not root_path.endswith('/'):
            root_path += '/'
        assert path.startswith(str(root_path)), \
            f"Mismatch between path and provided root: {path} VS {root_path}"

        try:
            metadata_path = Path(path).with_suffix('.json')
            out_path = args.out_path / path[len(root_path):]
            out_metadata_path = out_path.with_suffix('.json')
            out_done_token = out_path.with_suffix('.done')

            # don't reprocess existing files
            if out_done_token.exists():
                continue

            print(idx, out_path, path)
            mix, sr = audio_read(path)
            mix_channels = args.channels if args.channels is not None and args.channels > 0 else mix.size(0)
            # enforce simple stereo
            out_channels = mix_channels
            if out_channels > 2:
                print(f"Mix has more than two channels: {out_channels}, enforcing 2 channels")
                out_channels = 2
            out_sr = args.sample_rate if args.sample_rate is not None else sr
            out_wav = convert_audio(mix, sr, out_sr, out_channels)
            audio_write(out_path.with_suffix(''), out_wav, sample_rate=out_sr,
                        format=args.format, normalize=False, strategy='clip')
            if metadata_path.exists():
                shutil.copy(metadata_path, out_metadata_path)
            else:
                print(f"No metadata found at {str(metadata_path)}")
            out_done_token.touch()
        except Exception as e:
            print(f"Error processing file line: {line}, {e}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestConvertAudioChannels.test_convert_audio_channels_nochange
def test_convert_audio_channels_nochange(self):
        b, c, t = 2, 3, 100
        audio = get_batch_white_noise(b, c, t)
        mixed = convert_audio_channels(audio, channels=c)
        assert list(mixed.shape) == list(audio.shape)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\sisnr.py
BlockTypes.METHOD, _norm2
def _norm2(x: torch.Tensor) -> torch.Tensor:
    return x.pow(2).sum(-1, True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\streaming.py
BlockTypes.METHOD, StreamingModule.flush
def flush(self, x: tp.Optional[torch.Tensor] = None):
        """Flush any remaining outputs that were waiting for completion.
        Typically, for convolutions, this will add the final padding
        and process the last buffer.

        This should take an optional argument `x`, which will be provided
        if a module before this one in the streaming pipeline has already
        spitted out a flushed out buffer.
        """
        if x is None:
            return None
        else:
            return self(x)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_cross_attention
def test_cross_attention():
    torch.manual_seed(1234)
    for norm_first in [True, False]:
        m = StreamingTransformer(
            16, 4, 2, cross_attention=False, norm_first=norm_first, dropout=0., custom=True)
        m_cross = StreamingTransformer(
            16, 4, 2, cross_attention=True, norm_first=norm_first, dropout=0., custom=True)
        m_cross.load_state_dict(m.state_dict(), strict=False)
        x = torch.randn(2, 5, 16)
        cross_x = torch.randn(2, 3, 16)
        y_ref = m(x)
        y_cross_zero = m_cross(x, cross_attention_src=0 * cross_x)
        # With norm_first, the two should be exactly the same,
        # but with norm_first=False, we get 2 normalization in a row
        # and the epsilon value leads to a tiny change.
        atol = 0. if norm_first else 1e-6
        print((y_ref - y_cross_zero).norm() / y_ref.norm())
        assert torch.allclose(y_ref, y_cross_zero, atol=atol)

        # We now expect a difference even with a generous atol of 1e-2.
        y_cross = m_cross(x, cross_attention_src=cross_x)
        assert not torch.allclose(y_cross, y_cross_zero, atol=1e-2)

        with pytest.raises(AssertionError):
            _ = m_cross(x)
            _ = m(x, cross_attention_src=cross_x)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\mpd.py
BlockTypes.METHOD, get_padding
def get_padding(kernel_size: int, dilation: int = 1) -> int:
    return int((kernel_size * dilation - dilation) / 2)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._extract_chroma
def _extract_chroma(self, wav: torch.Tensor) -> torch.Tensor:
        """Extract chroma features from the waveform."""
        with self.autocast:
            return self.chroma(wav)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestAudioWrite.test_audio_write_wav
def test_audio_write_wav(self):
        torch.manual_seed(1234)
        sample_rates = [8000, 16_000]
        n_frames = [1000, 1001, 1002]
        channels = [1, 2]
        strategies = ["peak", "clip", "rms"]
        formats = ["wav", "mp3"]
        for sample_rate, ch, frames in product(sample_rates, channels, n_frames):
            for format_, strategy in product(formats, strategies):
                wav = get_white_noise(ch, frames)
                path = self.get_temp_path(f'pred_{sample_rate}_{ch}')
                audio_write(path, wav, sample_rate, format_, strategy=strategy)
                read_wav, read_sr = torchaudio.load(f'{path}.{format_}')
                if format_ == "wav":
                    assert read_wav.shape == wav.shape

                if format_ == "wav" and strategy in ["peak", "rms"]:
                    rescaled_read_wav = read_wav / read_wav.abs().max() * wav.abs().max()
                    # for a Gaussian, the typical max scale will be less than ~5x the std.
                    # The error when writing to disk will ~ 1/2**15, and when rescaling, 5x that.
                    # For RMS target, rescaling leaves more headroom by default, leading
                    # to a 20x rescaling typically
                    atol = (5 if strategy == "peak" else 20) / 2**15
                    delta = (rescaled_read_wav - wav).abs().max()
                    assert torch.allclose(wav, rescaled_read_wav, rtol=0, atol=atol), (delta, atol)
            formats = ["wav"]  # faster unit tests
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msstftd.py
BlockTypes.METHOD, MultiScaleSTFTDiscriminator._separate_channels
def _separate_channels(self, x: torch.Tensor) -> torch.Tensor:
        B, C, T = x.shape
        return x.view(-1, 1, T)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\rope.py
BlockTypes.METHOD, RotaryEmbedding.get_rotation
def get_rotation(self, start: int, end: int):
        """Create complex rotation tensor, cache values for fast computation."""
        if self.rotation is None or end > self.rotation.shape[0]:
            assert isinstance(self.frequencies, torch.Tensor)  # Satisfy type checker.
            idx = torch.arange(end, device=self.frequencies.device, dtype=self.dtype)
            angles = torch.outer(idx, self.frequencies)
            self.rotation = torch.polar(torch.ones_like(angles), angles)
        return self.rotation[start:end]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.register_ema
def register_ema(self, *args: str):
        """Register state sources for exponential moving average.

        The registered sources are used to instantiate a ModuleDictEMA instance.
        The ModuleDictEMA keeps a `nn.ModuleDict` module that is updated when self.ema.step() is called
        and swapped with the original state sources with self.swap_ema_state() method.

        Usage:
            self.register_ema('model')
        """
        assert self.ema is None, "Cannot register state source to already instantiated EMA."
        for name in args:
            self._ema_sources[name] = getattr(self, name)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, get_real_criterion
def get_real_criterion(loss_type: str) -> tp.Callable:
    assert loss_type in ADVERSARIAL_LOSSES
    if loss_type == 'mse':
        return mse_real_loss
    elif loss_type in ['hinge', 'hinge2']:
        return hinge_real_loss
    raise ValueError('Unsupported loss')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, MultiBandProcessor.project_sample
def project_sample(self, x: torch.Tensor):
        assert x.dim() == 3
        bands = self.split_bands(x)
        if self.counts.item() < self.num_samples:
            ref_bands = self.split_bands(torch.randn_like(x))
            self.counts += len(x)
            self.sum_x += bands.mean(dim=(2, 3)).sum(dim=1)
            self.sum_x2 += bands.pow(2).mean(dim=(2, 3)).sum(dim=1)
            self.sum_target_x2 += ref_bands.pow(2).mean(dim=(2, 3)).sum(dim=1)
        rescale = (self.target_std / self.std.clamp(min=1e-12)) ** self.power_std  # same output size
        bands = (bands - self.mean.view(-1, 1, 1, 1)) * rescale.view(-1, 1, 1, 1)
        return bands.sum(dim=0)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\specloss.py
BlockTypes.METHOD, MelSpectrogramWrapper.__init__
def __init__(self, n_fft: int = 1024, hop_length: int = 256, win_length: tp.Optional[int] = None,
                 n_mels: int = 80, sample_rate: float = 22050, f_min: float = 0.0, f_max: tp.Optional[float] = None,
                 log: bool = True, normalized: bool = False, floor_level: float = 1e-5):
        super().__init__()
        self.n_fft = n_fft
        hop_length = int(hop_length)
        self.hop_length = hop_length
        self.mel_transform = MelSpectrogram(n_mels=n_mels, sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length,
                                            win_length=win_length, f_min=f_min, f_max=f_max, normalized=normalized,
                                            window_fn=torch.hann_window, center=False)
        self.floor_level = floor_level
        self.log = log
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\best_state.py
BlockTypes.METHOD, BestStateDictManager.update
def update(self, name: str, source: flashy.state.StateDictSource):
        if name not in self.states:
            raise ValueError(f"{name} missing from registered states.")
        self.states[name] = copy_state(source.state_dict(), device=self.device, dtype=self.dtype)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_encodec_model.py
BlockTypes.METHOD, TestEncodecModel.test_model_renorm
def test_model_renorm(self):
        random.seed(1234)
        sample_rate = 24_000
        channels = 1
        model_nonorm = self._create_encodec_model(sample_rate, channels, renormalize=False)
        model_renorm = self._create_encodec_model(sample_rate, channels, renormalize=True)

        for _ in range(10):
            length = random.randrange(1, 10_000)
            x = torch.randn(2, channels, length)
            codes, scales = model_nonorm.encode(x)
            codes, scales = model_renorm.encode(x)
            assert scales is not None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, DiffusionUnet.__init__
def __init__(self, chin: int = 3, hidden: int = 24, depth: int = 3, growth: float = 2.,
                 max_channels: int = 10_000, num_steps: int = 1000, emb_all_layers=False, cross_attention: bool = False,
                 bilstm: bool = False, transformer: bool = False,
                 codec_dim: tp.Optional[int] = None, **kwargs):
        super().__init__()
        self.encoders = nn.ModuleList()
        self.decoders = nn.ModuleList()
        self.embeddings: tp.Optional[nn.ModuleList] = None
        self.embedding = nn.Embedding(num_steps, hidden)
        if emb_all_layers:
            self.embeddings = nn.ModuleList()
        self.condition_embedding: tp.Optional[nn.Module] = None
        for d in range(depth):
            encoder = EncoderLayer(chin, hidden, **kwargs)
            decoder = DecoderLayer(hidden, chin, **kwargs)
            self.encoders.append(encoder)
            self.decoders.insert(0, decoder)
            if emb_all_layers and d > 0:
                assert self.embeddings is not None
                self.embeddings.append(nn.Embedding(num_steps, hidden))
            chin = hidden
            hidden = min(int(chin * growth), max_channels)
        self.bilstm: tp.Optional[nn.Module]
        if bilstm:
            self.bilstm = BLSTM(chin)
        else:
            self.bilstm = None
        self.use_transformer = transformer
        self.cross_attention = False
        if transformer:
            self.cross_attention = cross_attention
            self.transformer = StreamingTransformer(chin, 8, 6, bias_ff=False, bias_attn=False,
                                                    cross_attention=cross_attention)

        self.use_codec = False
        if codec_dim is not None:
            self.conv_codec = nn.Conv1d(codec_dim, chin, 1)
            self.use_codec = True
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\base.py
BlockTypes.METHOD, BaseQuantizer.set_num_codebooks
def set_num_codebooks(self, n: int):
        """Set the number of active codebooks."""
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner.__init__
def __init__(self, dim: int, output_dim: int, device: str, attribute: str,
                 quantize: bool, n_q: int, bins: int, checkpoint: tp.Union[str, Path], model_arch: str,
                 enable_fusion: bool, sample_rate: int, max_audio_length: int, audio_stride: int,
                 normalize: bool, text_p: bool, batch_size: tp.Optional[int] = None,
                 autocast_dtype: tp.Optional[str] = 'float32', cache_path: tp.Optional[str] = None, **kwargs):
        try:
            import laion_clap  # type: ignore
        except ImportError:
            raise ImportError("Please install CLAP to use the CLAPEmbeddingConditioner: 'pip install laion_clap'")
        checkpoint = AudioCraftEnvironment.resolve_reference_path(checkpoint)
        clap_tokenize = RobertaTokenizer.from_pretrained('roberta-base')
        clap_model = laion_clap.CLAP_Module(enable_fusion=enable_fusion, amodel=model_arch)
        load_clap_state_dict(clap_model, checkpoint)
        clap_model.eval()
        clap_model.to(device)
        super().__init__(dim=dim, output_dim=output_dim, device=device, attribute=attribute,
                         autocast_dtype=autocast_dtype, quantize=quantize, n_q=n_q, bins=bins,
                         **kwargs)
        self.checkpoint = checkpoint
        self.enable_fusion = enable_fusion
        self.model_arch = model_arch
        self.clap: laion_clap.CLAP_Module
        self.clap_tokenize: RobertaTokenizer
        self.clap_sample_rate = sample_rate
        self.clap_max_frames = int(self.clap_sample_rate * max_audio_length)
        self.clap_stride = int(self.clap_sample_rate * audio_stride)
        self.batch_size = batch_size or 1
        self.normalize = normalize
        self.text_p = text_p
        self.__dict__['clap_tokenize'] = clap_tokenize
        self.__dict__['clap'] = clap_model
        self.wav_cache, self.text_cache = None, None
        if cache_path is not None:
            self.wav_cache = EmbeddingCache(Path(cache_path) / 'wav', self.device,
                                            compute_embed_fn=self._get_wav_embedding_for_cache,
                                            extract_embed_fn=self._extract_wav_embedding_chunk)
            self.text_cache = EmbeddingCache(Path(cache_path) / 'text', self.device,
                                             compute_embed_fn=self._get_text_embedding_for_cache)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen._prepare_tokens_and_attributes
def _prepare_tokens_and_attributes(
            self,
            descriptions: tp.Sequence[tp.Optional[str]],
            prompt: tp.Optional[torch.Tensor],
    ) -> tp.Tuple[tp.List[ConditioningAttributes], tp.Optional[torch.Tensor]]:
        """Prepare model inputs.

        Args:
            descriptions (list of str): A list of strings used as text conditioning.
            prompt (torch.Tensor): A batch of waveforms used for continuation.
        """
        attributes = [
            ConditioningAttributes(text={'description': description})
            for description in descriptions]

        if prompt is not None:
            if descriptions is not None:
                assert len(descriptions) == len(prompt), "Prompt and nb. descriptions doesn't match"
            prompt = prompt.to(self.device)
            prompt_tokens, scale = self.compression_model.encode(prompt)
            assert scale is None
        else:
            prompt_tokens = None
        return attributes, prompt_tokens
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, _is_custom
def _is_custom(custom: bool, memory_efficient: bool):
    return custom or memory_efficient
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.is_training
def is_training(self):
        return self.current_stage == 'train'
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestFeatureMatchingLoss.test_features_matching_loss_output
def test_features_matching_loss_output(self):
        loss_nonorm = FeatureMatchingLoss(normalize=False)
        loss_layer_normed = FeatureMatchingLoss(normalize=True)

        length = random.randrange(1, 100_000)
        t1 = torch.randn(1, 2, length)
        t2 = torch.randn(1, 2, length)

        assert loss_nonorm([t1, t2], [t1, t2]).item() == 0.0
        assert loss_layer_normed([t1, t2], [t1, t2]).item() == 0.0

        t3 = torch.FloatTensor([1.0, 2.0, 3.0])
        t4 = torch.FloatTensor([2.0, 10.0, 3.0])

        assert loss_nonorm([t3], [t4]).item() == 3.0
        assert loss_nonorm([t3, t3], [t4, t4]).item() == 6.0

        assert loss_layer_normed([t3], [t4]).item() == 3.0
        assert loss_layer_normed([t3, t3], [t4, t4]).item() == 3.0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, MultiBandDiffusion.get_emb
def get_emb(self, codes: torch.Tensor):
        """Get latent representation from the discrete codes
        Argrs:
            codes (torch.Tensor): discrete tokens"""
        emb = self.codec_model.decode_latent(codes)
        return emb
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.get_formatter
def get_formatter(self, stage_name: str) -> flashy.Formatter:
        return flashy.Formatter({
            'lr': '.2E',
            'ce': '.3f',
            'ppl': '.3f',
            'grad_norm': '.3E',
        }, exclude_keys=['ce_q*', 'ppl_q*'])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, DummyPoolExecutor.__init__
def __init__(self, workers, mp_context=None):
        pass
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, WhiteSpaceTokenizer.__call__
def __call__(self, texts: tp.List[tp.Optional[str]],
                 return_text: bool = False) -> tp.Tuple[torch.Tensor, torch.Tensor]:
        """Take a list of strings and convert them to a tensor of indices.

        Args:
            texts (list[str]): List of strings.
            return_text (bool, optional): Whether to return text as additional tuple item. Defaults to False.
        Returns:
            tuple[torch.Tensor, torch.Tensor]:
                - Indices of words in the LUT.
                - And a mask indicating where the padding tokens are
        """
        output, lengths = [], []
        texts = deepcopy(texts)
        for i, text in enumerate(texts):
            # if current sample doesn't have a certain attribute, replace with pad token
            if text is None:
                output.append(torch.Tensor([self.pad_idx]))
                lengths.append(0)
                continue

            # convert numbers to words
            text = re.sub(r"(\d+)", lambda x: num2words(int(x.group(0))), text)  # type: ignore
            # normalize text
            text = self.nlp(text)  # type: ignore
            # remove stopwords
            if self.stopwords:
                text = [w for w in text if not w.is_stop]  # type: ignore
            # remove punctuation
            text = [w for w in text if w.text not in self.PUNCTUATION]  # type: ignore
            # lemmatize if needed
            text = [getattr(t, "lemma_" if self.lemma else "text") for t in text]  # type: ignore

            texts[i] = " ".join(text)
            lengths.append(len(text))
            # convert to tensor
            tokens = torch.Tensor([hash_trick(w, self.n_bins) for w in text])
            output.append(tokens)

        mask = length_to_mask(torch.IntTensor(lengths)).int()
        padded_output = pad_sequence(output, padding_value=self.pad_idx).int().t()
        if return_text:
            return padded_output, mask, texts  # type: ignore
        return padded_output, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager.get_samples
def get_samples(self, epoch: int = -1, max_epoch: int = -1, exclude_prompted: bool = False,
                    exclude_unprompted: bool = False, exclude_conditioned: bool = False,
                    exclude_unconditioned: bool = False) -> tp.Set[Sample]:
        """Returns a set of samples for this XP. Optionally, you can filter which samples to obtain.
        Please note that existing samples are loaded during the manager's initialization, and added samples through this
        manager are also tracked. Any other external changes are not tracked automatically, so creating a new manager
        is the only way detect them.

        Args:
            epoch (int): If provided, only return samples corresponding to this epoch.
            max_epoch (int): If provided, only return samples corresponding to the latest epoch that is <= max_epoch.
            exclude_prompted (bool): If True, does not include samples that used a prompt.
            exclude_unprompted (bool): If True, does not include samples that did not use a prompt.
            exclude_conditioned (bool): If True, excludes samples that used conditioning.
            exclude_unconditioned (bool): If True, excludes samples that did not use conditioning.
        Returns:
            Samples (set of Sample): The retrieved samples matching the provided filters.
        """
        if max_epoch >= 0:
            samples_epoch = max(sample.epoch for sample in self.samples if sample.epoch <= max_epoch)
        else:
            samples_epoch = self.latest_epoch if epoch < 0 else epoch
        samples = {
            sample
            for sample in self.samples
            if (
                (sample.epoch == samples_epoch) and
                (not exclude_prompted or sample.prompt is None) and
                (not exclude_unprompted or sample.prompt is not None) and
                (not exclude_conditioned or not sample.conditioning) and
                (not exclude_unconditioned or sample.conditioning)
            )
        }
        return samples
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, swap_state
def swap_state(model, state, **kwargs):
    old_state = copy_state(model.state_dict())
    model.load_state_dict(state, **kwargs)
    try:
        yield
    finally:
        model.load_state_dict(old_state)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset.from_meta
def from_meta(cls, root: tp.Union[str, Path], **kwargs):
        """Instantiate AudioDataset from a path to a directory containing a manifest as a jsonl file.

        Args:
            root (str or Path): Path to root folder containing audio files.
            kwargs: Additional keyword arguments for the AudioDataset.
        """
        root = Path(root)
        if root.is_dir():
            if (root / 'data.jsonl').exists():
                root = root / 'data.jsonl'
            elif (root / 'data.jsonl.gz').exists():
                root = root / 'data.jsonl.gz'
            else:
                raise ValueError("Don't know where to read metadata from in the dir. "
                                 "Expecting either a data.jsonl or data.jsonl.gz file but none found.")
        meta = load_audio_meta(root)
        return cls(meta, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.__init__
def __init__(self, name: str, compression_model: CompressionModel, lm: LMModel,
                 max_duration: tp.Optional[float] = None):
        self.name = name
        self.compression_model = compression_model
        self.lm = lm
        if max_duration is None:
            if hasattr(lm, 'cfg'):
                max_duration = lm.cfg.dataset.segment_duration  # type: ignore
            else:
                raise ValueError("You must provide max_duration when building directly MusicGen")
        assert max_duration is not None
        self.max_duration: float = max_duration
        self.device = next(iter(lm.parameters())).device
        self.generation_params: dict = {}
        self.set_generation_params(duration=15)  # 15 seconds by default
        self._progress_callback: tp.Optional[tp.Callable[[int, int], None]] = None
        if self.device.type == 'cpu':
            self.autocast = TorchAutocast(enabled=False)
        else:
            self.autocast = TorchAutocast(
                enabled=True, device_type=self.device.type, dtype=torch.float16)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msstftd.py
BlockTypes.METHOD, get_2d_padding
def get_2d_padding(kernel_size: tp.Tuple[int, int], dilation: tp.Tuple[int, int] = (1, 1)):
    return (((kernel_size[0] - 1) * dilation[0]) // 2, ((kernel_size[1] - 1) * dilation[1]) // 2)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.cardinality
def cardinality(self):
        """Cardinality of each codebook."""
        return self.quantizer.bins
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\visqol.py
BlockTypes.METHOD, ViSQOL._get_target_sr
def _get_target_sr(self, mode: str) -> int:
        # returns target sampling rate for the corresponding ViSQOL mode.
        if mode not in ViSQOL.SAMPLE_RATES_MODES:
            raise ValueError(
                f"Unsupported mode! Allowed are: {', '.join(ViSQOL.SAMPLE_RATES_MODES.keys())}"
            )
        return ViSQOL.SAMPLE_RATES_MODES[mode]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.log_model_summary
def log_model_summary(self, model: nn.Module):
        """Log model summary, architecture and size of the model."""
        self.logger.info(model)
        mb = sum(p.numel() for p in model.parameters()) * 4 / 2 ** 20
        self.logger.info("Size: %.1f MB", mb)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.train
def train(self):
        """Train stage."""
        return self.common_train_valid('train')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.run_epoch
def run_epoch(self):
        if self.cfg.cache.write:
            if ((self.epoch - 1) % self.cfg.cache.write_num_shards) != self.cfg.cache.write_shard:
                return
        super().run_epoch()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\sisnr.py
BlockTypes.METHOD, SISNR.__init__
def __init__(
        self,
        sample_rate: int = 16000,
        segment: tp.Optional[float] = 20,
        overlap: float = 0.5,
        epsilon: float = torch.finfo(torch.float32).eps,
    ):
        super().__init__()
        self.sample_rate = sample_rate
        self.segment = segment
        self.overlap = overlap
        self.epsilon = epsilon
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, PasstKLDivergenceMetric._get_model_preds
def _get_model_preds(self, wav: torch.Tensor) -> torch.Tensor:
        """Run the pretrained model and get the predictions."""
        assert wav.dim() == 3, f"Unexpected number of dims for preprocessed wav: {wav.shape}"
        wav = wav.mean(dim=1)
        # PaSST is printing a lot of garbage that we are not interested in
        with open(os.devnull, "w") as f, contextlib.redirect_stdout(f):
            with torch.no_grad(), _patch_passt_stft():
                logits = self.model(wav.to(self.device))
                probs = torch.softmax(logits, dim=-1)
                return probs
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.load_from_pretrained
def load_from_pretrained(self, name: str) -> dict:
        raise NotImplementedError("Solver does not provide a way to load pretrained models.")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.get_condition
def get_condition(self, wav: torch.Tensor) -> torch.Tensor:
        codes, scale = self.codec_model.encode(wav)
        assert scale is None, "Scaled compression models not supported."
        emb = self.codec_model.decode_latent(codes)
        return emb
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\mpd.py
BlockTypes.METHOD, PeriodDiscriminator.__init__
def __init__(self, period: int, in_channels: int = 1, out_channels: int = 1,
                 n_layers: int = 5, kernel_sizes: tp.List[int] = [5, 3], stride: int = 3,
                 filters: int = 8, filters_scale: int = 4, max_filters: int = 1024,
                 norm: str = 'weight_norm', activation: str = 'LeakyReLU',
                 activation_params: dict = {'negative_slope': 0.2}):
        super().__init__()
        self.period = period
        self.n_layers = n_layers
        self.activation = getattr(torch.nn, activation)(**activation_params)
        self.convs = nn.ModuleList()
        in_chs = in_channels
        for i in range(self.n_layers):
            out_chs = min(filters * (filters_scale ** (i + 1)), max_filters)
            eff_stride = 1 if i == self.n_layers - 1 else stride
            self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=(kernel_sizes[0], 1), stride=(eff_stride, 1),
                                         padding=((kernel_sizes[0] - 1) // 2, 0), norm=norm))
            in_chs = out_chs
        self.conv_post = NormConv2d(in_chs, out_channels, kernel_size=(kernel_sizes[1], 1), stride=1,
                                    padding=((kernel_sizes[1] - 1) // 2, 0), norm=norm)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msstftd.py
BlockTypes.METHOD, MultiScaleSTFTDiscriminator.forward
def forward(self, x: torch.Tensor) -> MultiDiscriminatorOutputType:
        logits = []
        fmaps = []
        for disc in self.discriminators:
            logit, fmap = disc(x)
            logits.append(logit)
            fmaps.append(fmap)
        return logits, fmaps
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider.forward
def forward(self, tokenized: tp.Dict[str, tp.Any]) -> tp.Dict[str, ConditionType]:
        """Compute pairs of `(embedding, mask)` using the configured conditioners and the tokenized representations.
        The output is for example:
        {
            "genre": (torch.Tensor([B, 1, D_genre]), torch.Tensor([B, 1])),
            "description": (torch.Tensor([B, T_desc, D_desc]), torch.Tensor([B, T_desc])),
            ...
        }

        Args:
            tokenized (dict): Dict of tokenized representations as returned by `tokenize()`.
        """
        output = {}
        for attribute, inputs in tokenized.items():
            condition, mask = self.conditioners[attribute](inputs)
            output[attribute] = (condition, mask)
        return output
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, DummyPoolExecutor.submit
def submit(self, func, *args, **kwargs):
        return DummyPoolExecutor.DummyResult(func, *args, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment._get_cluster_config
def _get_cluster_config(self) -> omegaconf.DictConfig:
        assert isinstance(self.config, omegaconf.DictConfig)
        return self.config[self.cluster]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.best_metric_name
def best_metric_name(self) -> tp.Optional[str]:
        return self._best_metric_name
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_utils.py
BlockTypes.METHOD, normalize_audio
def normalize_audio(wav: torch.Tensor, normalize: bool = True,
                    strategy: str = 'peak', peak_clip_headroom_db: float = 1,
                    rms_headroom_db: float = 18, loudness_headroom_db: float = 14,
                    loudness_compressor: bool = False, log_clipping: bool = False,
                    sample_rate: tp.Optional[int] = None,
                    stem_name: tp.Optional[str] = None) -> torch.Tensor:
    """Normalize the audio according to the prescribed strategy (see after).

    Args:
        wav (torch.Tensor): Audio data.
        normalize (bool): if `True` (default), normalizes according to the prescribed
            strategy (see after). If `False`, the strategy is only used in case clipping
            would happen.
        strategy (str): Can be either 'clip', 'peak', or 'rms'. Default is 'peak',
            i.e. audio is normalized by its largest value. RMS normalizes by root-mean-square
            with extra headroom to avoid clipping. 'clip' just clips.
        peak_clip_headroom_db (float): Headroom in dB when doing 'peak' or 'clip' strategy.
        rms_headroom_db (float): Headroom in dB when doing 'rms' strategy. This must be much larger
            than the `peak_clip` one to avoid further clipping.
        loudness_headroom_db (float): Target loudness for loudness normalization.
        loudness_compressor (bool): If True, uses tanh based soft clipping.
        log_clipping (bool): If True, basic logging on stderr when clipping still
            occurs despite strategy (only for 'rms').
        sample_rate (int): Sample rate for the audio data (required for loudness).
        stem_name (str, optional): Stem name for clipping logging.
    Returns:
        torch.Tensor: Normalized audio.
    """
    scale_peak = 10 ** (-peak_clip_headroom_db / 20)
    scale_rms = 10 ** (-rms_headroom_db / 20)
    if strategy == 'peak':
        rescaling = (scale_peak / wav.abs().max())
        if normalize or rescaling < 1:
            wav = wav * rescaling
    elif strategy == 'clip':
        wav = wav.clamp(-scale_peak, scale_peak)
    elif strategy == 'rms':
        mono = wav.mean(dim=0)
        rescaling = scale_rms / mono.pow(2).mean().sqrt()
        if normalize or rescaling < 1:
            wav = wav * rescaling
        _clip_wav(wav, log_clipping=log_clipping, stem_name=stem_name)
    elif strategy == 'loudness':
        assert sample_rate is not None, "Loudness normalization requires sample rate."
        wav = normalize_loudness(wav, sample_rate, loudness_headroom_db, loudness_compressor)
        _clip_wav(wav, log_clipping=log_clipping, stem_name=stem_name)
    else:
        assert wav.abs().max() < 1
        assert strategy == '' or strategy == 'none', f"Unexpected strategy: '{strategy}'"
    return wav
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.run_step
def run_step(self, idx: int, batch: tp.Any, metrics: dict):
        """Perform one training or valid step on a given batch."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, get_extra_padding_for_conv1d
def get_extra_padding_for_conv1d(x: torch.Tensor, kernel_size: int, stride: int,
                                 padding_total: int = 0) -> int:
    """See `pad_for_conv1d`."""
    length = x.shape[-1]
    n_frames = (length - kernel_size + padding_total) / stride + 1
    ideal_length = (math.ceil(n_frames) - 1) * stride + (kernel_size - padding_total)
    return ideal_length - length
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.common_train_valid
def common_train_valid(self, dataset_split: str, **kwargs: tp.Any):
        """Common logic for train and valid stages."""
        self.model.train(self.is_training)

        loader = self.dataloaders[dataset_split]
        # get a different order for distributed training, otherwise this will get ignored
        if flashy.distrib.world_size() > 1 \
           and isinstance(loader.sampler, torch.utils.data.distributed.DistributedSampler):
            loader.sampler.set_epoch(self.epoch)
        updates_per_epoch = self.train_updates_per_epoch if self.is_training else len(loader)
        if self.cfg.benchmark_no_load:
            self.logger.warning("Fake loading for benchmarking: re-using first batch")
            batch = next(iter(loader))
            loader = [batch] * updates_per_epoch  # type: ignore
        lp = self.log_progress(self.current_stage, loader, total=updates_per_epoch, updates=self.log_updates)
        average = flashy.averager()  # epoch wise average
        instant_average = flashy.averager()  # average between two logging
        metrics: dict = {}

        with self.profiler, self.deadlock_detect:  # profiler will only run for the first 20 updates.
            for idx, batch in enumerate(lp):
                self.deadlock_detect.update('batch')
                if idx >= updates_per_epoch:
                    break
                metrics = {}
                metrics = self.run_step(idx, batch, metrics)
                self.deadlock_detect.update('step')
                # run EMA step
                if self.ema is not None and self.is_training and (idx + 1) % self.cfg.optim.ema.updates == 0:
                    self.logger.debug("EMA model step")
                    self.ema.step()
                self.deadlock_detect.update('ema')
                self.profiler.step()
                instant_metrics = instant_average(metrics)
                if lp.update(**instant_metrics):
                    instant_average = flashy.averager()  # reset averager between two logging
                metrics = average(metrics)  # epoch wise average
                self.deadlock_detect.update('end_batch')

        metrics = flashy.distrib.average_metrics(metrics, updates_per_epoch)
        return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\vq.py
BlockTypes.METHOD, ResidualVectorQuantizer.total_codebooks
def total_codebooks(self):
        return self.max_n_q
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.load_checkpoints
def load_checkpoints(self, load_best: bool = False, ignore_state_keys: tp.List[str] = []) -> tp.Optional[dict]:
        """Load last checkpoint or the one specified in continue_from.

        Args:
            load_best (bool): Whether to load from best state dict or not.
                Best state dict is always used when not loading the current xp.
            ignore_state_keys (list of str): List of sources to ignore when loading the state, e.g. `optimizer`.
        Returns:
            state (dict, optional): The loaded state dictionary.
        """
        # load checkpoints from xp folder or cfg.continue_from
        is_sharded = self.cfg.fsdp.use
        load_from_path: tp.Optional[Path] = None
        checkpoint_source: tp.Optional[checkpoint.CheckpointSource] = None

        if load_best:
            self.logger.info("Trying to load state_dict from best state.")

        state: tp.Optional[dict] = None
        rank0_checkpoint_path = self.checkpoint_path(use_fsdp=False)
        current_checkpoint_path = self.checkpoint_path()
        _pretrained_prefix = '//pretrained/'
        continue_pretrained = (self.cfg.continue_from or '').startswith(_pretrained_prefix)
        if rank0_checkpoint_path.exists():
            self.logger.info(f"Loading existing checkpoint: {current_checkpoint_path}")
            load_from_path = current_checkpoint_path
            checkpoint.check_sharded_checkpoint(current_checkpoint_path, rank0_checkpoint_path)
            checkpoint_source = checkpoint.CheckpointSource.CURRENT_XP
        elif self.cfg.continue_from and not continue_pretrained:
            self.logger.info(f"Continuing from provided checkpoint: {self.cfg.continue_from}")
            # we're always continuing from consolidated checkpoints: self.cfg.use_fsdp and not continue_best
            load_from_path = checkpoint.resolve_checkpoint_path(self.cfg.continue_from, use_fsdp=False)
            if load_from_path is None:
                self.logger.error('Could not resolve the continue_from checkpoint %s', self.cfg.continue_from)
                raise RuntimeError(f'Could not resolve continue_from checkpoint {self.cfg.continue_from}')
            checkpoint_source = checkpoint.CheckpointSource.OTHER

        if load_from_path is not None:
            state = checkpoint.load_checkpoint(load_from_path, is_sharded)
        elif continue_pretrained:
            self.logger.info("Loading a pretrained model. Ignoring 'load_best' and 'ignore_state_keys' params.")
            state = self.load_from_pretrained(self.cfg.continue_from[len(_pretrained_prefix):])
            checkpoint_source = checkpoint.CheckpointSource.PRETRAINED
            load_best = True

        # checkpoints are not from the current xp, we only retrieve the best state
        if checkpoint_source is not None and checkpoint_source != checkpoint.CheckpointSource.CURRENT_XP:
            assert state is not None
            self.logger.info("Checkpoint source is not the current xp: Load state_dict from best state.")
            load_best = True
            state = {key: state[key] for key in self._continue_best_source_keys if key in state}
            # loaded checkpoints are FSDP checkpoints: we're reading the best state
            # from FSDP and we drop the regular best_state
            if 'fsdp_best_state' in state and state['fsdp_best_state']:
                state.pop('best_state', None)
                self.logger.info("... Loaded checkpoint has FSDP best state")
            # FSDP is enabled in the solver, if the loaded checkpoints do not have FSDP support
            # then we're initializing FSDP best state with the regular best state
            elif self.cfg.fsdp.use:
                if 'fsdp_best_state' not in state or not state['fsdp_best_state']:
                    # we swap non-FSDP checkpoints best_state to FSDP-compatible best state
                    state['fsdp_best_state'] = state.pop('best_state')
                    self.logger.info("... Loaded checkpoint does not have FSDP best state. Use regular best state")

        if state is not None:
            if load_best:
                self.logger.info("Ignoring keys when loading best %r", ignore_state_keys)
                for key in set(ignore_state_keys):
                    if key in state:
                        state.pop(key)
                has_best_state = 'best_state' in state or 'fsdp_best_state' in state
                assert has_best_state, ("Trying to load best state but neither 'best_state'",
                                        " or 'fsdp_best_state' found in checkpoints.")
            self.load_state_dict(state)

        # for FSDP, let's make extra sure nothing bad happened with out of sync
        # checkpoints across workers.
        epoch = float(self.epoch)
        avg_epoch = flashy.distrib.average_metrics({'epoch': epoch})['epoch']
        if avg_epoch != epoch:
            raise RuntimeError(
                f"Inconsistent loading of checkpoints happened, our epoch is {epoch} "
                f"but average of epochs is {avg_epoch}, at least one gpu must have a "
                "different epoch number.")

        # on load_best, properly reinitialize state_dict, best states and ema
        # otherwise we load from the current xp and don't alter anything
        if load_best:
            self.logger.info("Loading state_dict from best state.")
            if not self.cfg.fsdp.use and self.fsdp_best_state:
                # loading from an FSDP checkpoint but with FSDP deactivated
                self.logger.info("... Loading from FSDP best state dict.")
                self.best_state.load_state_dict(self.fsdp_best_state)

            # if load_best, we permanently override the regular state_dict with the best state
            if self.cfg.fsdp.use:
                self.logger.info("FSDP is used, loading from FSDP best state.")
                with fsdp.switch_to_full_state_dict(self._fsdp_modules):
                    # this might be really fragile but okay for now.
                    self.load_state_dict(self.fsdp_best_state)
            else:
                # we permanently swap the stateful objects to their best state
                self._load_new_state_dict(self.best_state.state_dict())

            # the EMA modules should also be instantiated with best state.
            # the easiest way to do so is to reinitialize a new EMA with best state loaded.
            if self.ema is not None:
                self.logger.info("Re-initializing EMA from best state")
                self.initialize_ema()

            if self.cfg.fsdp.use:
                self.logger.info("Re-initializing best state after using FSDP best state.")
                for name in self.best_state.states.keys():
                    state_source = self._get_state_source(name)
                    self.best_state.update(name, state_source)

        return state
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestRead.test_read_seek_time_wav_padded
def test_read_seek_time_wav_padded(self):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 1.
        read_duration = 1.
        for sample_rate, ch in product(sample_rates, channels):
            n_frames = int(sample_rate * duration)
            read_frames = int(sample_rate * read_duration)
            wav = get_white_noise(ch, n_frames).clamp(-0.99, 0.99)
            path = self.get_temp_path('sample_wav.wav')
            save_wav(path, wav, sample_rate)
            seek_time = torch.rand(1).item()
            seek_frames = int(sample_rate * seek_time)
            expected_frames = n_frames - seek_frames
            read_wav, read_sr = audio_read(path, seek_time, read_duration, pad=True)
            expected_pad_wav = torch.zeros(wav.shape[0], read_frames - expected_frames)
            assert read_sr == sample_rate
            assert read_wav.shape[0] == wav.shape[0]
            assert read_wav.shape[1] == read_frames
            assert torch.allclose(read_wav[..., :expected_frames], wav[..., seek_frames:], rtol=1e-03, atol=1e-04)
            assert torch.allclose(read_wav[..., expected_frames:], expected_pad_wav)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\musicgen_clapemb_32khz.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=32, partition=partitions)
    launcher.bind_(solver='musicgen/musicgen_base_32khz')
    # replace this by the desired music dataset
    launcher.bind_(dset='internal/music_400k_32khz')
    launcher.bind_(conditioner='clapemb2music')

    fsdp = {'autocast': False, 'fsdp.use': True}
    cache_path = {'conditioners.description.clap.cache_path':
                  '/fsx-audio-craft-llm/jadecopet/experiments/audiocraft/caches/clap_embed_music'}
    text_wav_training_opt = {'conditioners.description.clap.text_p': 0.5}

    launcher.bind_(fsdp)

    launcher.slurm_(gpus=32).bind_(label='32gpus')
    with launcher.job_array():
        launcher()
        launcher(text_wav_training_opt)
        launcher(cache_path)
        launcher(cache_path, text_wav_training_opt)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric.__init__
def __init__(self, bin: tp.Union[Path, str], model_path: tp.Union[Path, str],
                 format: str = "wav", batch_size: tp.Optional[int] = None,
                 log_folder: tp.Optional[tp.Union[Path, str]] = None):
        super().__init__()
        self.model_sample_rate = VGGISH_SAMPLE_RATE
        self.model_channels = VGGISH_CHANNELS
        self.model_path = AudioCraftEnvironment.resolve_reference_path(model_path)
        assert Path(self.model_path).exists(), f"Could not find provided model checkpoint path at: {self.model_path}"
        self.format = format
        self.batch_size = batch_size
        self.bin = bin
        self.tf_env = {"PYTHONPATH": str(self.bin)}
        self.python_path = os.environ.get('TF_PYTHON_EXE') or 'python'
        logger.info("Python exe for TF is  %s", self.python_path)
        if 'TF_LIBRARY_PATH' in os.environ:
            self.tf_env['LD_LIBRARY_PATH'] = os.environ['TF_LIBRARY_PATH']
        if 'TF_FORCE_GPU_ALLOW_GROWTH' in os.environ:
            self.tf_env['TF_FORCE_GPU_ALLOW_GROWTH'] = os.environ['TF_FORCE_GPU_ALLOW_GROWTH']
        logger.info("Env for TF is %r", self.tf_env)
        self.reset(log_folder)
        self.add_state("total_files", default=torch.tensor(0.), dist_reduce_fx="sum")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._compute_wav_embedding
def _compute_wav_embedding(self, wav: torch.Tensor, sample_rate: int) -> torch.Tensor:
        """Compute wav embedding, applying stem and chroma extraction."""
        # avoid 0-size tensors when we are working with null conds
        if wav.shape[-1] == 1:
            return self._extract_chroma(wav)
        stems = self._get_stemmed_wav(wav, sample_rate)
        chroma = self._extract_chroma(stems)
        return chroma
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset._create_audio_files
def _create_audio_files(self,
                            root_name: str,
                            num_examples: int,
                            durations: tp.Union[float, tp.Tuple[float, float]] = (0.1, 1.),
                            sample_rate: int = 16_000,
                            channels: int = 1):
        root_dir = self.get_temp_dir(root_name)
        for i in range(num_examples):
            if isinstance(durations, float):
                duration = durations
            elif isinstance(durations, tuple) and len(durations) == 1:
                duration = durations[0]
            elif isinstance(durations, tuple) and len(durations) == 2:
                duration = random.uniform(durations[0], durations[1])
            else:
                assert False
            n_frames = int(duration * sample_rate)
            wav = get_white_noise(channels, n_frames)
            path = os.path.join(root_dir, f'example_{i}.wav')
            save_wav(path, wav, sample_rate)
        return root_dir
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, dropout_condition
def dropout_condition(sample: ConditioningAttributes, condition_type: str, condition: str) -> ConditioningAttributes:
    """Utility function for nullifying an attribute inside an ConditioningAttributes object.
    If the condition is of type "wav", then nullify it using `nullify_condition` function.
    If the condition is of any other type, set its value to None.
    Works in-place.
    """
    if condition_type not in ['text', 'wav', 'joint_embed']:
        raise ValueError(
            "dropout_condition got an unexpected condition type!"
            f" expected 'text', 'wav' or 'joint_embed' but got '{condition_type}'"
        )

    if condition not in getattr(sample, condition_type):
        raise ValueError(
            "dropout_condition received an unexpected condition!"
            f" expected wav={sample.wav.keys()} and text={sample.text.keys()}"
            f" but got '{condition}' of type '{condition_type}'!"
        )

    if condition_type == 'wav':
        wav_cond = sample.wav[condition]
        sample.wav[condition] = nullify_wav(wav_cond)
    elif condition_type == 'joint_embed':
        embed = sample.joint_embed[condition]
        sample.joint_embed[condition] = nullify_joint_embed(embed)
    else:
        sample.text[condition] = None

    return sample
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.valid
def valid(self):
        """Valid stage."""
        return self.common_train_valid('valid')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\best_state.py
BlockTypes.METHOD, BestStateDictManager.register
def register(self, name: str, source: flashy.state.StateDictSource):
        if name in self.states:
            raise ValueError(f"{name} already present in states.")
        # Registering parameter ids for EMA and non-EMA states allows us to check that
        # there is no overlap that would create ambiguity about how to handle the best state
        param_ids = self._get_parameter_ids(source.state_dict())
        if isinstance(source, ModuleDictEMA):
            logger.debug(f"Registering to best state: ModuleDictEMA '{name}' with {len(param_ids)} params")
            self._validate_no_parameter_ids_overlap(name, param_ids)
            self.param_ids[name] = param_ids
        else:
            logger.debug(f"Registering to best state: StateDictSource '{name}' with {len(param_ids)} params")
            self._validate_no_parameter_ids_overlap('base', param_ids)
            self.param_ids['base'].update(param_ids)
        # Register state
        self.states[name] = copy_state(source.state_dict(), device=self.device, dtype=self.dtype)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, DummyPoolExecutor.__enter__
def __enter__(self):
        return self
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\specloss.py
BlockTypes.METHOD, MultiScaleMelSpectrogramLoss.forward
def forward(self, x, y):
        loss = 0.0
        self.l1s.to(x.device)
        self.l2s.to(x.device)
        for i in range(len(self.alphas)):
            s_x_1 = self.l1s[i](x)
            s_y_1 = self.l1s[i](y)
            s_x_2 = self.l2s[i](x)
            s_y_2 = self.l2s[i](y)
            loss += F.l1_loss(s_x_1, s_y_1) + self.alphas[i] * F.mse_loss(s_x_2, s_y_2)
        if self.normalized:
            loss = loss / self.total
        return loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.preprocess
def preprocess(self, x: torch.Tensor) -> tp.Tuple[torch.Tensor, tp.Optional[torch.Tensor]]:
        scale: tp.Optional[torch.Tensor]
        if self.renormalize:
            mono = x.mean(dim=1, keepdim=True)
            volume = mono.pow(2).mean(dim=2, keepdim=True).sqrt()
            scale = 1e-8 + volume
            x = x / scale
            scale = scale.view(-1, 1)
        else:
            scale = None
        return x, scale
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\ema.py
BlockTypes.METHOD, _get_named_tensors
def _get_named_tensors(module: nn.Module):
    non_persistent_buffers_set = _get_all_non_persistent_buffers_set(module)
    named_buffers = [(name, buffer) for (name, buffer) in module.named_buffers()
                     if name not in non_persistent_buffers_set]
    named_parameters = list(module.named_parameters())
    return named_parameters + named_buffers
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.build_model
def build_model(self) -> None:
        """Instantiate models and optimizer."""
        # we can potentially not use all quantizers with which the EnCodec model was trained
        # (e.g. we trained the model with quantizers dropout)
        self.compression_model = CompressionSolver.wrapped_model_from_checkpoint(
            self.cfg, self.cfg.compression_model_checkpoint, device=self.device)
        assert self.compression_model.sample_rate == self.cfg.sample_rate, (
            f"Compression model sample rate is {self.compression_model.sample_rate} but "
            f"Solver sample rate is {self.cfg.sample_rate}."
            )
        # ensure we have matching configuration between LM and compression model
        assert self.cfg.transformer_lm.card == self.compression_model.cardinality, (
            "Cardinalities of the LM and compression model don't match: ",
            f"LM cardinality is {self.cfg.transformer_lm.card} vs ",
            f"compression model cardinality is {self.compression_model.cardinality}"
        )
        assert self.cfg.transformer_lm.n_q == self.compression_model.num_codebooks, (
            "Numbers of codebooks of the LM and compression models don't match: ",
            f"LM number of codebooks is {self.cfg.transformer_lm.n_q} vs ",
            f"compression model numer of codebooks is {self.compression_model.num_codebooks}"
        )
        self.logger.info("Compression model has %d codebooks with %d cardinality, and a framerate of %d",
                         self.compression_model.num_codebooks, self.compression_model.cardinality,
                         self.compression_model.frame_rate)
        # instantiate LM model
        self.model: models.LMModel = models.builders.get_lm_model(self.cfg).to(self.device)
        if self.cfg.fsdp.use:
            assert not self.cfg.autocast, "Cannot use autocast with fsdp"
            self.model = self.wrap_with_fsdp(self.model)
        self.register_ema('model')
        # initialize optimization
        self.optimizer = builders.get_optimizer(builders.get_optim_parameter_groups(self.model), self.cfg.optim)
        self.lr_scheduler = builders.get_lr_scheduler(self.optimizer, self.cfg.schedule, self.total_updates)
        self.register_stateful('compression_model', 'model', 'optimizer', 'lr_scheduler')
        self.register_best_state('model')
        self.autocast_dtype = {
            'float16': torch.float16, 'bfloat16': torch.bfloat16
        }[self.cfg.autocast_dtype]
        self.scaler: tp.Optional[torch.cuda.amp.GradScaler] = None
        if self.cfg.fsdp.use:
            need_scaler = self.cfg.fsdp.param_dtype == 'float16'
        else:
            need_scaler = self.cfg.autocast and self.autocast_dtype is torch.float16
        if need_scaler:
            if self.cfg.fsdp.use:
                from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
                self.scaler = ShardedGradScaler()  # type: ignore
            else:
                self.scaler = torch.cuda.amp.GradScaler()
            self.register_stateful('scaler')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.forward
def forward(self, x: torch.Tensor) -> qt.QuantizedResult:
        # We don't support training with this.
        raise NotImplementedError("Forward and training with HF EncodecModel not supported.")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, MultiBandDiffusion.generate
def generate(self, emb: torch.Tensor, size: tp.Optional[torch.Size] = None,
                 step_list: tp.Optional[tp.List[int]] = None):
        """Generate Wavform audio from the latent embeddings of the compression model
        Args:
            emb (torch.Tensor): Conditioning embeddinds
            size (none torch.Size): size of the output
                if None this is computed from the typical upsampling of the model
            step_list (optional list[int]): list of Markov chain steps, defaults to 50 linearly spaced step.
        """
        if size is None:
            upsampling = int(self.codec_model.sample_rate / self.codec_model.frame_rate)
            size = torch.Size([emb.size(0), self.codec_model.channels, emb.size(-1) * upsampling])
        assert size[0] == emb.size(0)
        out = torch.zeros(size).to(self.device)
        for DP in self.DPs:
            out += DP.generate(condition=emb, step_list=step_list, initial_noise=torch.randn_like(out))
        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msstftd.py
BlockTypes.METHOD, DiscriminatorSTFT.__init__
def __init__(self, filters: int, in_channels: int = 1, out_channels: int = 1,
                 n_fft: int = 1024, hop_length: int = 256, win_length: int = 1024, max_filters: int = 1024,
                 filters_scale: int = 1, kernel_size: tp.Tuple[int, int] = (3, 9), dilations: tp.List = [1, 2, 4],
                 stride: tp.Tuple[int, int] = (1, 2), normalized: bool = True, norm: str = 'weight_norm',
                 activation: str = 'LeakyReLU', activation_params: dict = {'negative_slope': 0.2}):
        super().__init__()
        assert len(kernel_size) == 2
        assert len(stride) == 2
        self.filters = filters
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.win_length = win_length
        self.normalized = normalized
        self.activation = getattr(torch.nn, activation)(**activation_params)
        self.spec_transform = torchaudio.transforms.Spectrogram(
            n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.win_length, window_fn=torch.hann_window,
            normalized=self.normalized, center=False, pad_mode=None, power=None)
        spec_channels = 2 * self.in_channels
        self.convs = nn.ModuleList()
        self.convs.append(
            NormConv2d(spec_channels, self.filters, kernel_size=kernel_size, padding=get_2d_padding(kernel_size))
        )
        in_chs = min(filters_scale * self.filters, max_filters)
        for i, dilation in enumerate(dilations):
            out_chs = min((filters_scale ** (i + 1)) * self.filters, max_filters)
            self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride,
                                         dilation=(dilation, 1), padding=get_2d_padding(kernel_size, (dilation, 1)),
                                         norm=norm))
            in_chs = out_chs
        out_chs = min((filters_scale ** (len(dilations) + 1)) * self.filters, max_filters)
        self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=(kernel_size[0], kernel_size[0]),
                                     padding=get_2d_padding((kernel_size[0], kernel_size[0])),
                                     norm=norm))
        self.conv_post = NormConv2d(out_chs, self.out_channels,
                                    kernel_size=(kernel_size[0], kernel_size[0]),
                                    padding=get_2d_padding((kernel_size[0], kernel_size[0])),
                                    norm=norm)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\musicgen_base_cached_32khz.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=32, partition=partitions)
    launcher.bind_(solver='musicgen/musicgen_base_32khz')
    # replace this by the desired music dataset
    launcher.bind_(dset='internal/music_400k_32khz')

    fsdp = {'autocast': False, 'fsdp.use': True}
    medium = {'model/lm/model_scale': 'medium'}
    large = {'model/lm/model_scale': 'large'}

    cfg_low = {'classifier_free_guidance.training_dropout': 0.2}
    wd_low = {'conditioners.description.t5.word_dropout': 0.2}

    adam = {'optim.optimizer': 'adamw', 'optim.lr': 1e-4}

    # BEGINNING OF CACHE WRITING JOBS.
    cache_write = {
        'cache.path': '/fsx-codegen/defossez/cache/interleave_stereo_nv_32k',
        'cache.write': True,
        'generate.every': 500,
        'evaluate.every': 500,
        'logging.log_updates': 50,
    }

    cache_sub = launcher.bind({'model/lm/model_scale': 'xsmall', 'conditioner': 'none'})
    cache_sub.bind_({'deadlock.use': True})
    cache_sub.slurm_(gpus=8)
    with launcher.job_array():
        num_shards = 10  # total number of jobs running in parallel.
        for shard in range(0, num_shards):
            launcher(cache_write, {'cache.write_num_shards': num_shards, 'cache.write_shard': shard})

    # REMOVE THE FOLLOWING RETURN STATEMENT ONCE THE ABOVE JOBS ARE DONE,
    # OR SUFFICIENTLY AHEAD.
    return

    cache = {
        'cache.path': '/fsx-codegen/defossez/cache/interleave_stereo_nv_32k',
    }
    launcher.bind_(fsdp, cache)

    launcher.slurm_(gpus=32).bind_(label='32gpus')
    with launcher.job_array():
        sub = launcher.bind()
        sub()

    launcher.slurm_(gpus=64).bind_(label='64gpus')
    with launcher.job_array():
        sub = launcher.bind()
        sub(medium, adam)

    launcher.slurm_(gpus=96).bind_(label='96gpus')
    with launcher.job_array():
        sub = launcher.bind()
        sub(large, cfg_low, wd_low, adam, {'optim.max_norm': 3})
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.instance
def instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset.start_epoch
def start_epoch(self, epoch: int):
        self.current_epoch = epoch
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, CompressionModel.encode
def encode(self, x: torch.Tensor) -> tp.Tuple[torch.Tensor, tp.Optional[torch.Tensor]]:
        """See `EncodecModel.encode`."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, mse_real_loss
def mse_real_loss(x: torch.Tensor) -> torch.Tensor:
    return F.mse_loss(x, torch.tensor(1., device=x.device).expand_as(x))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, MusicLMPattern.__init__
def __init__(self, n_q: int, group_by: int = 2):
        super().__init__(n_q)
        self.group_by = group_by
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, MusicDataset.__init__
def __init__(self, *args, info_fields_required: bool = True,
                 merge_text_p: float = 0., drop_desc_p: float = 0., drop_other_p: float = 0.,
                 joint_embed_attributes: tp.List[str] = [],
                 paraphrase_source: tp.Optional[str] = None, paraphrase_p: float = 0,
                 **kwargs):
        kwargs['return_info'] = True  # We require the info for each song of the dataset.
        super().__init__(*args, **kwargs)
        self.info_fields_required = info_fields_required
        self.merge_text_p = merge_text_p
        self.drop_desc_p = drop_desc_p
        self.drop_other_p = drop_other_p
        self.joint_embed_attributes = joint_embed_attributes
        self.paraphraser = None
        if paraphrase_source is not None:
            self.paraphraser = Paraphraser(paraphrase_source, paraphrase_p)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\vq.py
BlockTypes.METHOD, ResidualVectorQuantizer.num_codebooks
def num_codebooks(self):
        return self.n_q
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.wrap_with_fsdp
def wrap_with_fsdp(self, model: torch.nn.Module, *args, **kwargs):
        model = fsdp.wrap_with_fsdp(self.cfg.fsdp, model, *args, **kwargs)
        if isinstance(model, fsdp.FSDP):
            self._fsdp_modules.append(model)
        return model
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\diffusion_schedule.py
BlockTypes.METHOD, NoiseSchedule.get_training_item
def get_training_item(self, x: torch.Tensor, tensor_step: bool = False) -> TrainingItem:
        """Create a noisy data item for diffusion model training:

        Args:
            x (torch.Tensor): clean audio data torch.tensor(bs, 1, T)
            tensor_step (bool): If tensor_step = false, only one step t is sample,
                the whole batch is diffused to the same step and t is int.
                If tensor_step = true, t is a tensor of size (x.size(0),)
                every element of the batch is diffused to a independently sampled.
        """
        step: tp.Union[int, torch.Tensor]
        if tensor_step:
            bs = x.size(0)
            step = torch.randint(0, self.num_steps, size=(bs,), device=x.device)
        else:
            step = self.rng.randrange(self.num_steps)
        alpha_bar = self.get_alpha_bar(step)  # [batch_size, n_bands, 1]

        x = self.sample_processor.project_sample(x)
        noise = torch.randn_like(x)
        noisy = (alpha_bar.sqrt() / self.rescale) * x + (1 - alpha_bar).sqrt() * noise * self.noise_scale
        return TrainingItem(noisy, noise, step)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, CompressionModel.decode
def decode(self, codes: torch.Tensor, scale: tp.Optional[torch.Tensor] = None):
        """See `EncodecModel.decode`."""
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, get_pool_executor
def get_pool_executor(num_workers: int, mp_context=None):
    return ProcessPoolExecutor(num_workers, mp_context) if num_workers > 1 else DummyPoolExecutor(1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.run_generate_step
def run_generate_step(self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
                          gen_duration: float, prompt_duration: tp.Optional[float] = None,
                          remove_prompt: bool = False,
                          **generation_params) -> dict:
        """Run generate step on a batch of optional audio tensor and corresponding attributes.

        Args:
            batch (tuple[torch.Tensor, list[SegmentWithAttributes]]):
            use_prompt (bool): Whether to do audio continuation generation with prompt from audio batch.
            gen_duration (float): Target audio duration for the generation.
            prompt_duration (float, optional): Duration for the audio prompt to use for continuation.
            remove_prompt (bool, optional): Whether to remove the prompt from the generated audio.
            generation_params: Additional generation parameters.
        Returns:
            gen_outputs (dict): Generation outputs, consisting in audio, audio tokens from both the generation
                and the prompt along with additional information.
        """
        bench_start = time.time()
        audio, meta = batch
        assert audio.size(0) == len(meta), (
            f"Mismatch between number of items in audio batch ({audio.size(0)})",
            f" and in metadata ({len(meta)})"
        )
        # prepare attributes
        attributes = [x.to_condition_attributes() for x in meta]
        # TODO: Add dropout for chroma?

        # prepare audio prompt
        if prompt_duration is None:
            prompt_audio = None
        else:
            assert prompt_duration < gen_duration, "Prompt duration must be lower than target generation duration"
            prompt_audio_frames = int(prompt_duration * self.compression_model.sample_rate)
            prompt_audio = audio[..., :prompt_audio_frames]

        # get audio tokens from compression model
        if prompt_audio is None or prompt_audio.nelement() == 0:
            num_samples = len(attributes)
            prompt_tokens = None
        else:
            num_samples = None
            prompt_audio = prompt_audio.to(self.device)
            prompt_tokens, scale = self.compression_model.encode(prompt_audio)
            assert scale is None, "Compression model in MusicGen should not require rescaling."

        # generate by sampling from the LM
        with self.autocast:
            total_gen_len = math.ceil(gen_duration * self.compression_model.frame_rate)
            gen_tokens = self.model.generate(
                prompt_tokens, attributes, max_gen_len=total_gen_len,
                num_samples=num_samples, **self.generation_params)

        # generate audio from tokens
        assert gen_tokens.dim() == 3
        gen_audio = self.compression_model.decode(gen_tokens, None)

        bench_end = time.time()
        gen_outputs = {
            'rtf': (bench_end - bench_start) / gen_duration,
            'ref_audio': audio,
            'gen_audio': gen_audio,
            'gen_tokens': gen_tokens,
            'prompt_audio': prompt_audio,
            'prompt_tokens': prompt_tokens,
        }
        return gen_outputs
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, DummyPoolExecutor.__exit__
def __exit__(self, exc_type, exc_value, exc_tb):
        return
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, HFEncodecCompressionModel.encode
def encode(self, x: torch.Tensor) -> tp.Tuple[torch.Tensor, tp.Optional[torch.Tensor]]:
        bandwidth_index = self.possible_num_codebooks.index(self.num_codebooks)
        bandwidth = self.model.config.target_bandwidths[bandwidth_index]
        res = self.model.encode(x, None, bandwidth)
        assert len(res[0]) == 1
        assert len(res[1]) == 1
        return res[0][0], res[1][0]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset.__len__
def __len__(self):
        return self.num_samples
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, ResidualVectorQuantization.encode
def encode(self, x: torch.Tensor, n_q: tp.Optional[int] = None) -> torch.Tensor:
        residual = x
        all_indices = []
        n_q = n_q or len(self.layers)
        for layer in self.layers[:n_q]:
            indices = layer.encode(residual)
            quantized = layer.decode(indices)
            residual = residual - quantized
            all_indices.append(indices)
        out_indices = torch.stack(all_indices)
        return out_indices
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.build_model
def build_model(self):
        """Build model and optimizer as well as optional Exponential Moving Average of the model.
        """
        # Model and optimizer
        self.model = models.builders.get_diffusion_model(self.cfg).to(self.device)
        self.optimizer = builders.get_optimizer(self.model.parameters(), self.cfg.optim)
        self.register_stateful('model', 'optimizer')
        self.register_best_state('model')
        self.register_ema('model')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\visqol.py
BlockTypes.METHOD, ViSQOL.visqol_model
def visqol_model(self):
        return f'{self.visqol_bin}/model/{self.model}'
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, mse_fake_loss
def mse_fake_loss(x: torch.Tensor) -> torch.Tensor:
    return F.mse_loss(x, torch.tensor(0., device=x.device).expand_as(x))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\_base_explorers.py
BlockTypes.METHOD, get_sheep_ping
def get_sheep_ping(sheep) -> tp.Optional[str]:
    """Return the amount of time since the Sheep made some update
    to its log. Returns a str using the relevant time unit."""
    ping = None
    if sheep.log is not None and sheep.log.exists():
        delta = time.time() - sheep.log.stat().st_mtime
        if delta > 3600 * 24:
            ping = f'{delta / (3600 * 24):.1f}d'
        elif delta > 3600:
            ping = f'{delta / (3600):.1f}h'
        elif delta > 60:
            ping = f'{delta / 60:.1f}m'
        else:
            ping = f'{delta:.1f}s'
    return ping
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, SoundInfo.from_dict
def from_dict(cls, dictionary: dict, fields_required: bool = False):
        _dictionary: tp.Dict[str, tp.Any] = {}

        # allow a subset of attributes to not be loaded from the dictionary
        # these attributes may be populated later
        post_init_attributes = ['self_wav']

        for _field in fields(cls):
            if _field.name in post_init_attributes:
                continue
            elif _field.name not in dictionary:
                if fields_required:
                    raise KeyError(f"Unexpected missing key: {_field.name}")
            else:
                preprocess_func: tp.Optional[tp.Callable] = cls.attribute_getter(_field.name)
                value = dictionary[_field.name]
                if preprocess_func:
                    value = preprocess_func(value)
                _dictionary[_field.name] = value
        return cls(**_dictionary)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\visqol.py
BlockTypes.METHOD, ViSQOL._prepare_files
def _prepare_files(
        self, ref_sig: torch.Tensor, deg_sig: torch.Tensor, sr: int, target_sr: int, pad_with_silence: bool = False
    ):
        # prepare files for ViSQOL evaluation.
        assert target_sr in ViSQOL.ALLOWED_SAMPLE_RATES
        assert len(ref_sig) == len(deg_sig), (
            "Expects same number of ref and degraded inputs",
            f" but ref len {len(ref_sig)} != deg len {len(deg_sig)}"
        )
        # resample audio if needed
        if sr != target_sr:
            transform = torchaudio.transforms.Resample(sr, target_sr)
            pad = int(0.5 * target_sr)
            rs_ref = []
            rs_deg = []
            for i in range(len(ref_sig)):
                rs_ref_i = transform(ref_sig[i])
                rs_deg_i = transform(deg_sig[i])
                if pad_with_silence:
                    rs_ref_i = torch.nn.functional.pad(rs_ref_i, (pad, pad), mode='constant', value=0)
                    rs_deg_i = torch.nn.functional.pad(rs_deg_i, (pad, pad), mode='constant', value=0)
                rs_ref.append(rs_ref_i)
                rs_deg.append(rs_deg_i)
            ref_sig = torch.stack(rs_ref)
            deg_sig = torch.stack(rs_deg)
        # save audio chunks to tmp dir and create csv
        tmp_dir = Path(tempfile.mkdtemp())
        try:
            tmp_input_csv_path = tmp_dir / "input.csv"
            tmp_results_csv_path = tmp_dir / "results.csv"
            tmp_debug_json_path = tmp_dir / "debug.json"
            with open(tmp_input_csv_path, "w") as csv_file:
                csv_writer = csv.writer(csv_file)
                csv_writer.writerow(["reference", "degraded"])
                for i in range(len(ref_sig)):
                    tmp_ref_filename = tmp_dir / f"ref_{i}.wav"
                    tmp_deg_filename = tmp_dir / f"deg_{i}.wav"
                    torchaudio.save(
                        tmp_ref_filename,
                        torch.clamp(ref_sig[i], min=-0.99, max=0.99),
                        sample_rate=target_sr,
                        bits_per_sample=16,
                        encoding="PCM_S"
                    )
                    torchaudio.save(
                        tmp_deg_filename,
                        torch.clamp(deg_sig[i], min=-0.99, max=0.99),
                        sample_rate=target_sr,
                        bits_per_sample=16,
                        encoding="PCM_S"
                    )
                    csv_writer.writerow([str(tmp_ref_filename), str(tmp_deg_filename)])
            return tmp_dir, tmp_input_csv_path, tmp_results_csv_path, tmp_debug_json_path
        except Exception as e:
            logger.error("Exception occurred when preparing files for ViSQOL: %s", e)
            return tmp_dir, None, None, None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, warn_once
def warn_once(logger, msg):
    """Warn about a given message only once."""
    logger.warning(msg)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\streaming.py
BlockTypes.METHOD, StreamingSequential.flush
def flush(self, x: tp.Optional[torch.Tensor] = None):
        for module in self:
            if isinstance(module, StreamingModule):
                x = module.flush(x)
            elif x is not None:
                x = module(x)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_loss
def get_loss(loss_name: str, cfg: omegaconf.DictConfig):
    """Instantiate loss from configuration."""
    klass = {
        'l1': torch.nn.L1Loss,
        'l2': torch.nn.MSELoss,
        'mel': losses.MelSpectrogramL1Loss,
        'mrstft': losses.MRSTFTLoss,
        'msspec': losses.MultiScaleMelSpectrogramLoss,
        'sisnr': losses.SISNR,
    }[loss_name]
    kwargs = dict(getattr(cfg, loss_name))
    return klass(**kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, set_efficient_attention_backend
def set_efficient_attention_backend(backend: str = 'torch'):
    # Using torch by default, it seems a bit faster on older P100 GPUs (~20% faster).
    global _efficient_attention_backend
    assert _efficient_attention_backend in ['xformers', 'torch']
    _efficient_attention_backend = backend
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset._get_sampling_probabilities
def _get_sampling_probabilities(self, normalized: bool = True):
        """Return the sampling probabilities for each file inside `self.meta`."""
        scores: tp.List[float] = []
        for file_meta in self.meta:
            score = 1.
            if self.sample_on_weight and file_meta.weight is not None:
                score *= file_meta.weight
            if self.sample_on_duration:
                score *= file_meta.duration
            scores.append(score)
        probabilities = torch.tensor(scores)
        if normalized:
            probabilities /= probabilities.sum()
        return probabilities
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\sisnr.py
BlockTypes.METHOD, SISNR.forward
def forward(self, out_sig: torch.Tensor, ref_sig: torch.Tensor) -> torch.Tensor:
        B, C, T = ref_sig.shape
        assert ref_sig.shape == out_sig.shape

        if self.segment is None:
            frame = T
            stride = T
        else:
            frame = int(self.segment * self.sample_rate)
            stride = int(frame * (1 - self.overlap))

        epsilon = self.epsilon * frame  # make epsilon prop to frame size.

        gt = _unfold(ref_sig, frame, stride)
        est = _unfold(out_sig, frame, stride)
        if self.segment is None:
            assert gt.shape[-1] == 1

        gt = _center(gt)
        est = _center(est)
        dot = torch.einsum("bcft,bcft->bcf", gt, est)

        proj = dot[:, :, :, None] * gt / (epsilon + _norm2(gt))
        noise = est - proj

        sisnr = 10 * (
            torch.log10(epsilon + _norm2(proj)) - torch.log10(epsilon + _norm2(noise))
        )
        return -1 * sisnr[..., 0].mean()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, WaveformConditioner.tokenize
def tokenize(self, x: WavCondition) -> WavCondition:
        wav, length, sample_rate, path, seek_time = x
        assert length is not None
        return WavCondition(wav.to(self.device), length.to(self.device), sample_rate, path, seek_time)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\profiler.py
BlockTypes.METHOD, Profiler.__init__
def __init__(self, module: torch.nn.Module, enabled: bool = False):
        self.profiler: tp.Optional[tp.Any] = None
        if enabled:
            from xformers.profiler import profile
            output_dir = dora.get_xp().folder / 'profiler_data'
            logger.info("Profiling activated, results with be saved to %s", output_dir)
            self.profiler = profile(output_dir=output_dir, module=module)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, hinge_real_loss
def hinge_real_loss(x: torch.Tensor) -> torch.Tensor:
    return -torch.mean(torch.min(x - 1, torch.tensor(0., device=x.device).expand_as(x)))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.train
def train(self):
        """Train stage.
        """
        if self._cached_batch_writer is not None:
            self._cached_batch_writer.start_epoch(self.epoch)
        if self._cached_batch_loader is None:
            dataset = get_dataset_from_loader(self.dataloaders['train'])
            assert isinstance(dataset, AudioDataset)
            dataset.current_epoch = self.epoch
        else:
            self._cached_batch_loader.start_epoch(self.epoch)
        return super().train()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._get_full_chroma_for_cache
def _get_full_chroma_for_cache(self, path: tp.Union[str, Path], x: WavCondition, idx: int) -> torch.Tensor:
        """Extract chroma from the whole audio waveform at the given path."""
        wav, sr = audio_read(path)
        wav = wav[None].to(self.device)
        wav = convert_audio(wav, sr, self.sample_rate, to_channels=1)
        chroma = self._compute_wav_embedding(wav, self.sample_rate)[0]
        return chroma
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, pad_for_conv1d
def pad_for_conv1d(x: torch.Tensor, kernel_size: int, stride: int, padding_total: int = 0):
    """Pad for a convolution to make sure that the last window is full.
    Extra padding is added at the end. This is required to ensure that we can rebuild
    an output of the same length, as otherwise, even with padding, some time steps
    might get removed.
    For instance, with total padding = 4, kernel size = 4, stride = 2:
        0 0 1 2 3 4 5 0 0   # (0s are padding)
        1   2   3           # (output frames of a convolution, last 0 is never used)
        0 0 1 2 3 4 5 0     # (output of tr. conv., but pos. 5 is going to get removed as padding)
            1 2 3 4         # once you removed padding, we are missing one time step !
    """
    extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)
    return F.pad(x, (0, extra_padding))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, NormConvTranspose1d.__init__
def __init__(self, *args, causal: bool = False, norm: str = 'none',
                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
        super().__init__()
        self.convtr = apply_parametrization_norm(nn.ConvTranspose1d(*args, **kwargs), norm)
        self.norm = get_norm_module(self.convtr, causal, norm, **norm_kwargs)
        self.norm_type = norm
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.reset
def reset(cls):
        """Clears the environment and forces a reload on next invocation."""
        cls._instance = None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, is_jsonable
def is_jsonable(x: tp.Any):
    """Check if an object can be serialized into a json:"""
    try:
        json.dumps(x)
        return True
    except (TypeError, OverflowError):
        return False
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\common_utils\wav_utils.py
BlockTypes.METHOD, get_white_noise
def get_white_noise(chs: int = 1, num_frames: int = 1):
    wav = torch.randn(chs, num_frames)
    return wav
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\fsdp.py
BlockTypes.METHOD, wrap_with_fsdp
def wrap_with_fsdp(cfg, model: torch.nn.Module,
                   block_classes: tp.Optional[tp.Set[tp.Type]] = None) -> FSDP:
    """Wraps a model with FSDP."""
    # Some of the typing is disabled until this gets integrated
    # into the stable version of PyTorch.
    from torch.distributed.fsdp.wrap import ModuleWrapPolicy  # type: ignore

    # we import this here to prevent circular import.
    from ..modules.transformer import StreamingTransformerLayer
    from ..modules.conditioners import ConditioningProvider

    _fix_post_backward_hook()

    assert cfg.use
    sharding_strategy_dict = {
        "no_shard": ShardingStrategy.NO_SHARD,
        "shard_grad_op": ShardingStrategy.SHARD_GRAD_OP,
        "full_shard": ShardingStrategy.FULL_SHARD,
    }

    dtype_dict = {
        "float32": torch.float32,
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
    }

    mixed_precision_config = MixedPrecision(
        param_dtype=dtype_dict[cfg.param_dtype],
        reduce_dtype=dtype_dict[cfg.reduce_dtype],
        buffer_dtype=dtype_dict[cfg.buffer_dtype],
    )

    sharding_strategy_config = sharding_strategy_dict[cfg.sharding_strategy]
    # The following is going to require being a bit smart
    # when doing LM, because this would flush the weights for every time step
    # during generation. One possiblity is to use hybrid sharding:
    # See: https://pytorch.org/docs/master/fsdp.html#torch.distributed.fsdp.ShardingStrategy
    assert sharding_strategy_config != ShardingStrategy.FULL_SHARD, \
        "Not supported at the moment, requires a bit more work."

    local_rank = dora.distrib.get_distrib_spec().local_rank
    assert local_rank < torch.cuda.device_count(), "Please upgrade Dora!"

    auto_wrap_policy = None
    if block_classes is None:
        block_classes = {StreamingTransformerLayer, ConditioningProvider}
    if cfg.per_block:
        auto_wrap_policy = ModuleWrapPolicy(block_classes)
    wrapped = _FSDPFixStateDict(
        model,
        sharding_strategy=sharding_strategy_config,
        mixed_precision=mixed_precision_config,
        device_id=local_rank,
        sync_module_states=True,
        use_orig_params=True,
        auto_wrap_policy=auto_wrap_policy,
    )  # type: ignore
    FSDP.set_state_dict_type(wrapped, StateDictType.LOCAL_STATE_DICT)  # type: ignore

    # Let the wrapped model know about the wrapping!
    # We use __dict__ to avoid it going into the state dict.
    # This is a bit dirty, but needed during generation, as otherwise
    # the wrapped model would call itself and bypass FSDP.
    for module in FSDP.fsdp_modules(wrapped):
        original = module._fsdp_wrapped_module
        original.__dict__['_fsdp'] = module
    return wrapped
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, get_full_embed
def get_full_embed(full_embed: torch.Tensor, x: tp.Any, idx: int, device: tp.Union[str, torch.device]) -> torch.Tensor:
    """Utility function for the EmbeddingCache, returning the full embedding without any chunking.
    This method can be used in case there is no need in extracting a chunk of the full embedding
    read from the cache.

    Args:
        full_embed (torch.Tensor): The full embedding.
        x (any): Batch object from which the full embedding is derived.
        idx (torch.Tensor): Index of object to consider in the batch object.
    Returns:
        full_embed (torch.Tensor): The full embedding
    """
    return full_embed.to(device)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_transformer_vs_pytorch
def test_transformer_vs_pytorch():
    torch.manual_seed(1234)
    # Check that in the non causal setting, we get the same result as
    # PyTorch Transformer encoder.
    for custom in [False, True]:
        tr = StreamingTransformer(
            16, 4, 2,
            causal=False, custom=custom, dropout=0., positional_scale=0.)
        layer = torch.nn.TransformerEncoderLayer(16, 4, dropout=0., batch_first=True)
        tr_ref = torch.nn.TransformerEncoder(layer, 2)
        tr.load_state_dict(tr_ref.state_dict())

        x = torch.randn(4, 20, 16)
        y = tr(x)
        y2 = tr_ref(x)
        delta = torch.norm(y2 - y) / torch.norm(y)
        assert delta < 1e-6, delta
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio.py
BlockTypes.METHOD, _init_av
def _init_av():
    global _av_initialized
    if _av_initialized:
        return
    logger = logging.getLogger('libav.mp3')
    logger.setLevel(logging.ERROR)
    _av_initialized = True
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider._collate_text
def _collate_text(self, samples: tp.List[ConditioningAttributes]) -> tp.Dict[str, tp.List[tp.Optional[str]]]:
        """Given a list of ConditioningAttributes objects, compile a dictionary where the keys
        are the attributes and the values are the aggregated input per attribute.
        For example:
        Input:
        [
            ConditioningAttributes(text={"genre": "Rock", "description": "A rock song with a guitar solo"}, wav=...),
            ConditioningAttributes(text={"genre": "Hip-hop", "description": "A hip-hop verse"}, wav=...),
        ]
        Output:
        {
            "genre": ["Rock", "Hip-hop"],
            "description": ["A rock song with a guitar solo", "A hip-hop verse"]
        }

        Args:
            samples (list of ConditioningAttributes): List of ConditioningAttributes samples.
        Returns:
            dict[str, list[str, optional]]: A dictionary mapping an attribute name to text batch.
        """
        out: tp.Dict[str, tp.List[tp.Optional[str]]] = defaultdict(list)
        texts = [x.text for x in samples]
        for text in texts:
            for condition in self.text_conditions:
                out[condition].append(text[condition])
        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\ema.py
BlockTypes.METHOD, ModuleDictEMA.__init__
def __init__(self, module_dict: nn.ModuleDict, decay: float = 0.999,
                 unbias: bool = True, device: tp.Union[torch.device, str] = 'cpu'):
        self.decay = decay
        self.module_dict = module_dict
        self.state: dict = defaultdict(dict)
        self.count = 0
        self.device = device
        self.unbias = unbias
        self._init()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_qk_layer_norm
def test_qk_layer_norm():
    torch.manual_seed(1234)
    tr = StreamingTransformer(
        16, 4, 2, custom=True, dropout=0., qk_layer_norm=True, bias_attn=False)
    steps = 12
    x = torch.randn(3, steps, 16)
    y = tr(x)

    tr = StreamingTransformer(
        16, 4, 2, custom=True, dropout=0., qk_layer_norm=True, cross_attention=True)
    z = torch.randn(3, 21, 16)
    y = tr(x, cross_attention_src=z)
    assert y.shape == x.shape
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, hinge_fake_loss
def hinge_fake_loss(x: torch.Tensor) -> torch.Tensor:
    return -torch.mean(torch.min(-x - 1, torch.tensor(0., device=x.device).expand_as(x)))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, StreamableConvTranspose1d.forward
def forward(self, x):
        kernel_size = self.convtr.convtr.kernel_size[0]
        stride = self.convtr.convtr.stride[0]
        padding_total = kernel_size - stride

        y = self.convtr(x)

        # We will only trim fixed padding. Extra padding from `pad_for_conv1d` would be
        # removed at the very end, when keeping only the right length for the output,
        # as removing it here would require also passing the length at the matching layer
        # in the encoder.
        if self.causal:
            # Trim the padding on the right according to the specified ratio
            # if trim_right_ratio = 1.0, trim everything from right
            padding_right = math.ceil(padding_total * self.trim_right_ratio)
            padding_left = padding_total - padding_right
            y = unpad1d(y, (padding_left, padding_right))
        else:
            # Asymmetric padding required for odd strides
            padding_right = padding_total // 2
            padding_left = padding_total - padding_right
            y = unpad1d(y, (padding_left, padding_right))
        return y
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, _call_nostderr
def _call_nostderr(*args, **kwargs):
    # Avoid ffmpeg vomiting on the logs.
    kwargs['stderr'] = sp.DEVNULL
    kwargs['stdout'] = sp.DEVNULL
    _old_call(*args, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, LMModel._sample_next_token
def _sample_next_token(self,
                           sequence: torch.Tensor,
                           cfg_conditions: CFGConditions,
                           unconditional_state: State,
                           use_sampling: bool = False,
                           temp: float = 1.0,
                           top_k: int = 0,
                           top_p: float = 0.0,
                           cfg_coef: tp.Optional[float] = None) -> torch.Tensor:
        """Sample next token from the model given a sequence and a set of conditions. The model supports
        multiple sampling strategies (greedy sampling, softmax, top-k, top-p...).

        Args:
            sequence (torch.Tensor): Current sequence of shape [B, K, S]
                with K corresponding to the number of codebooks and S the number of sequence steps.
                S = 1 in streaming mode, except for the first step that contains a bigger prompt.
            condition_tensors (dict[str, ConditionType): Set of conditions. If CFG is used,
                should be twice the batch size, being the concatenation of the conditions + null conditions.
            use_sampling (bool): Whether to use a sampling strategy or not.
            temp (float): Sampling temperature.
            top_k (int): K for "top-k" sampling.
            top_p (float): P for "top-p" sampling.
            cfg_coef (float, optional): classifier free guidance coefficient
        Returns:
            next_token (torch.Tensor): Next token tensor of shape [B, K, 1].
        """
        B = sequence.shape[0]
        cfg_coef = self.cfg_coef if cfg_coef is None else cfg_coef
        model = self if self._fsdp is None else self._fsdp
        if self.two_step_cfg and cfg_conditions != {}:
            assert isinstance(cfg_conditions, tuple), type(cfg_conditions)
            condition_tensors, null_condition_tensors = cfg_conditions
            cond_logits = model(sequence, conditions=[], condition_tensors=condition_tensors)
            state = self.get_streaming_state()
            self.set_streaming_state(unconditional_state)
            uncond_logits = model(sequence, conditions=[], condition_tensors=null_condition_tensors)
            unconditional_state.update(self.get_streaming_state())
            self.set_streaming_state(state)
            logits = uncond_logits + (cond_logits - uncond_logits) * self.cfg_coef
        else:
            assert isinstance(cfg_conditions, dict)
            condition_tensors = cfg_conditions
            if condition_tensors:
                # Preparing for CFG, predicting both conditional and unconditional logits.
                sequence = torch.cat([sequence, sequence], dim=0)
            all_logits = model(
                sequence,
                conditions=[], condition_tensors=condition_tensors)
            if condition_tensors:
                cond_logits, uncond_logits = all_logits.split(B, dim=0)  # [B, K, T, card]
                logits = uncond_logits + (cond_logits - uncond_logits) * cfg_coef
            else:
                logits = all_logits

        logits = logits.permute(0, 1, 3, 2)  # [B, K, card, T]
        logits = logits[..., -1]  # [B x K x card]

        # Apply softmax for sampling if temp > 0. Else, do greedy sampling to avoid zero division error.
        if use_sampling and temp > 0.0:
            probs = torch.softmax(logits / temp, dim=-1)
            if top_p > 0.0:
                next_token = utils.sample_top_p(probs, p=top_p)
            elif top_k > 0:
                next_token = utils.sample_top_k(probs, k=top_k)
            else:
                next_token = utils.multinomial(probs, num_samples=1)
        else:
            next_token = torch.argmax(logits, dim=-1, keepdim=True)

        return next_token
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, kmeans
def kmeans(samples, num_clusters: int, num_iters: int = 10):
    dim, dtype = samples.shape[-1], samples.dtype

    means = sample_vectors(samples, num_clusters)

    for _ in range(num_iters):
        diffs = rearrange(samples, "n d -> n () d") - rearrange(
            means, "c d -> () c d"
        )
        dists = -(diffs ** 2).sum(dim=-1)

        buckets = dists.max(dim=-1).indices
        bins = torch.bincount(buckets, minlength=num_clusters)
        zero_mask = bins == 0
        bins_min_clamped = bins.masked_fill(zero_mask, 1)

        new_means = buckets.new_zeros(num_clusters, dim, dtype=dtype)
        new_means.scatter_add_(0, repeat(buckets, "n -> n d", d=dim), samples)
        new_means = new_means / bins_min_clamped[..., None]

        means = torch.where(zero_mask[..., None], means, new_means)

    return means, bins
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_seanet.py
BlockTypes.METHOD, TestSEANetModel.test_encoder_disable_norm
def test_encoder_disable_norm(self):
        n_residuals = [0, 1, 3]
        disable_blocks = [0, 1, 2, 3, 4, 5, 6]
        norms = ['weight_norm', 'none']
        for n_res, disable_blocks, norm in product(n_residuals, disable_blocks, norms):
            encoder = SEANetEncoder(n_residual_layers=n_res, norm=norm,
                                    disable_norm_outer_blocks=disable_blocks)
            self._check_encoder_blocks_norm(encoder, disable_blocks, norm)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\chroma_cosinesim.py
BlockTypes.METHOD, ChromaCosineSimilarityMetric.compute
def compute(self) -> float:
        """Computes the average cosine similarty across all generated/target chromagrams pairs."""
        assert self.weight.item() > 0, "Unable to compute with total number of comparisons <= 0"  # type: ignore
        return (self.cosine_sum / self.weight).item()  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioMeta.test_get_audio_meta
def test_get_audio_meta(self):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 1.
        for sample_rate, ch in product(sample_rates, channels):
            n_frames = int(duration * sample_rate)
            wav = get_white_noise(ch, n_frames)
            path = self.get_temp_path('sample.wav')
            save_wav(path, wav, sample_rate)
            m = _get_audio_meta(path, minimal=True)
            assert m.path == path, 'path does not match'
            assert m.sample_rate == sample_rate, 'sample rate does not match'
            assert m.duration == duration, 'duration does not match'
            assert m.amplitude is None
            assert m.info_path is None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, _get_attention_time_dimension
def _get_attention_time_dimension() -> int:
    if _efficient_attention_backend == 'torch':
        return 2
    else:
        return 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestNormalizeAudio.test_clip_wav
def test_clip_wav(self):
        b, c, dur = 2, 1, 4.
        sr = 3
        audio = 10.0 * get_batch_white_noise(b, c, int(sr * dur))
        _clip_wav(audio)
        assert audio.abs().max() <= 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.get_team
def get_team(cls) -> str:
        """Gets the selected team as dictated by the AUDIOCRAFT_TEAM env var.
        If not defined, defaults to "labs".
        """
        return cls.instance().team
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset.from_path
def from_path(cls, root: tp.Union[str, Path], minimal_meta: bool = True,
                  exts: tp.List[str] = DEFAULT_EXTS, **kwargs):
        """Instantiate AudioDataset from a path containing (possibly nested) audio files.

        Args:
            root (str or Path): Path to root folder containing audio files.
            minimal_meta (bool): Whether to only load minimal metadata or not.
            exts (list of str): Extensions for audio files.
            kwargs: Additional keyword arguments for the AudioDataset.
        """
        root = Path(root)
        if root.is_file():
            meta = load_audio_meta(root, resolve=True)
        else:
            meta = find_audio_files(root, exts, minimal=minimal_meta, resolve=True)
        return cls(meta, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, LMModel.num_codebooks
def num_codebooks(self) -> int:
        return self.n_q
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, WaveformConditioner._get_wav_embedding
def _get_wav_embedding(self, x: WavCondition) -> torch.Tensor:
        """Gets as input a WavCondition and returns a dense embedding."""
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, mse_loss
def mse_loss(x: torch.Tensor) -> torch.Tensor:
    if x.numel() == 0:
        return torch.tensor([0.0], device=x.device)
    return F.mse_loss(x, torch.tensor(1., device=x.device).expand_as(x))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\kld.py
BlockTypes.METHOD, PasstKLDivergenceMetric._get_label_distribution
def _get_label_distribution(self, x: torch.Tensor, sizes: torch.Tensor,
                                sample_rates: torch.Tensor) -> tp.Optional[torch.Tensor]:
        """Get model output given provided input tensor.

        Args:
            x (torch.Tensor): Input audio tensor of shape [B, C, T].
            sizes (torch.Tensor): Actual audio sample length, of shape [B].
            sample_rates (torch.Tensor): Actual audio sample rate, of shape [B].
        Returns:
            probs (torch.Tensor, optional): Probabilities over labels, of shape [B, num_classes].
        """
        all_probs: tp.List[torch.Tensor] = []
        for i, wav in enumerate(x):
            sample_rate = int(sample_rates[i].item())
            wav_len = int(sizes[i].item())
            wav_segments = self._process_audio(wav, sample_rate, wav_len)
            for segment in wav_segments:
                probs = self._get_model_preds(segment).mean(dim=0)
                all_probs.append(probs)
        if len(all_probs) > 0:
            return torch.stack(all_probs, dim=0)
        else:
            return None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingTransformerLayer.forward
def forward(self, src: torch.Tensor, src_mask: tp.Optional[torch.Tensor] = None,  # type: ignore
                src_key_padding_mask: tp.Optional[torch.Tensor] = None,
                cross_attention_src: tp.Optional[torch.Tensor] = None):
        if self.cross_attention is None:
            assert cross_attention_src is None
        else:
            assert cross_attention_src is not None
        x = src
        if self.norm_first:
            x = x + self.layer_scale_1(
                self._sa_block(self.norm1(x), src_mask, src_key_padding_mask))
            if cross_attention_src is not None:
                x = x + self.layer_scale_cross(
                    self._cross_attention_block(
                        self.norm_cross(x), cross_attention_src))
            x = x + self.layer_scale_2(self._ff_block(self.norm2(x)))
        else:
            x = self.norm1(x + self.layer_scale_1(
                self._sa_block(x, src_mask, src_key_padding_mask)))
            if cross_attention_src is not None:
                x = self.norm_cross(
                    x + self.layer_scale_cross(
                        self._cross_attention_block(src, cross_attention_src)))
            x = self.norm2(x + self.layer_scale_2(self._ff_block(x)))
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\checkpoint.py
BlockTypes.METHOD, is_sharded_checkpoint
def is_sharded_checkpoint(path: Path) -> bool:
    """Whether the checkpoint at the given path corresponds to a sharded checkpoint across rank."""
    return re.search(r'\.th\.\d+$', path.name) is not None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\musicgen_base_32khz.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=32, partition=partitions)
    launcher.bind_(solver='musicgen/musicgen_base_32khz')
    # replace this by the desired music dataset
    launcher.bind_(dset='internal/music_400k_32khz')

    fsdp = {'autocast': False, 'fsdp.use': True}
    medium = {'model/lm/model_scale': 'medium'}
    large = {'model/lm/model_scale': 'large'}

    cfg_low = {'classifier_free_guidance.training_dropout': 0.2}
    wd_low = {'conditioners.description.t5.word_dropout': 0.2}

    adam = {'optim.optimizer': 'adamw', 'optim.lr': 1e-4}

    launcher.bind_(fsdp)

    launcher.slurm_(gpus=32).bind_(label='32gpus')
    with launcher.job_array():
        sub = launcher.bind()
        sub()

    launcher.slurm_(gpus=64).bind_(label='64gpus')
    with launcher.job_array():
        sub = launcher.bind()
        sub(medium, adam)

    launcher.slurm_(gpus=96).bind_(label='96gpus')
    with launcher.job_array():
        sub = launcher.bind()
        sub(large, cfg_low, wd_low, adam, {'optim.max_norm': 3})
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_balancer
def get_balancer(loss_weights: tp.Dict[str, float], cfg: omegaconf.DictConfig) -> losses.Balancer:
    """Instantiate loss balancer from configuration for the provided weights."""
    kwargs: tp.Dict[str, tp.Any] = dict_from_config(cfg)
    return losses.Balancer(loss_weights, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\common_utils\wav_utils.py
BlockTypes.METHOD, get_batch_white_noise
def get_batch_white_noise(bs: int = 1, chs: int = 1, num_frames: int = 1):
    wav = torch.randn(bs, chs, num_frames)
    return wav
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.best_metric_name
def best_metric_name(self) -> tp.Optional[str]:
        # best model is the last for the compression model
        return None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.build_pattern_sequence
def build_pattern_sequence(self, z: torch.Tensor, special_token: int, keep_only_valid_steps: bool = False):
        """Build sequence corresponding to the pattern from the input tensor z.
        The sequence is built using up to sequence_steps if specified, and non-pattern
        coordinates are filled with the special token.

        Args:
            z (torch.Tensor): Input tensor of multi-codebooks sequence, of shape [B, K, T].
            special_token (int): Special token used to fill non-pattern coordinates in the new sequence.
            keep_only_valid_steps (bool): Build a sequence from the pattern up to valid (= fully defined) steps.
                Steps that are beyond valid steps will be replaced by the special_token in that case.
        Returns:
            values (torch.Tensor): Interleaved sequence matching the pattern, of shape [B, K, S] with S
                corresponding either to the sequence_steps if provided, otherwise to the length of the pattern.
            indexes (torch.Tensor): Indexes corresponding to the interleaved sequence, of shape [K, S].
            mask (torch.Tensor): Mask corresponding to indexes that matches valid indexes of shape [K, S].
        """
        B, K, T = z.shape
        indexes, mask = self._build_pattern_sequence_scatter_indexes(
            T, K, keep_only_valid_steps=keep_only_valid_steps, device=str(z.device)
        )
        z = z.view(B, -1)
        # we append the special token as the last index of our flattened z tensor
        z = torch.cat([z, torch.zeros_like(z[:, :1]) + special_token], dim=1)
        values = z[:, indexes.view(-1)]
        values = values.view(B, K, indexes.shape[-1])
        return values, indexes, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\dadam.py
BlockTypes.METHOD, to_real
def to_real(x):
    if torch.is_complex(x):
        return x.real
    else:
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, slugify
def slugify(value: tp.Any, allow_unicode: bool = False):
    """Process string for safer file naming.

    Taken from https://github.com/django/django/blob/master/django/utils/text.py

    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated
    dashes to single dashes. Remove characters that aren't alphanumerics,
    underscores, or hyphens. Convert to lowercase. Also strip leading and
    trailing whitespace, dashes, and underscores.
    """
    value = str(value)
    if allow_unicode:
        value = unicodedata.normalize("NFKC", value)
    else:
        value = (
            unicodedata.normalize("NFKD", value)
            .encode("ascii", "ignore")
            .decode("ascii")
        )
    value = re.sub(r"[^\w\s-]", "", value.lower())
    return re.sub(r"[-\s]+", "-", value).strip("-_")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, pad1d
def pad1d(x: torch.Tensor, paddings: tp.Tuple[int, int], mode: str = 'constant', value: float = 0.):
    """Tiny wrapper around F.pad, just to allow for reflect padding on small input.
    If this is the case, we insert extra 0 padding to the right before the reflection happen.
    """
    length = x.shape[-1]
    padding_left, padding_right = paddings
    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)
    if mode == 'reflect':
        max_pad = max(padding_left, padding_right)
        extra_pad = 0
        if length <= max_pad:
            extra_pad = max_pad - length + 1
            x = F.pad(x, (0, extra_pad))
        padded = F.pad(x, paddings, mode, value)
        end = padded.shape[-1] - extra_pad
        return padded[..., :end]
    else:
        return F.pad(x, paddings, mode, value)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.__init__
def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__(cfg)
        self.rng: torch.Generator  # set at each epoch
        self.adv_losses = builders.get_adversarial_losses(self.cfg)
        self.aux_losses = nn.ModuleDict()
        self.info_losses = nn.ModuleDict()
        assert not cfg.fsdp.use, "FSDP not supported by CompressionSolver."
        loss_weights = dict()
        for loss_name, weight in self.cfg.losses.items():
            if loss_name in ['adv', 'feat']:
                for adv_name, _ in self.adv_losses.items():
                    loss_weights[f'{loss_name}_{adv_name}'] = weight
            elif weight > 0:
                self.aux_losses[loss_name] = builders.get_loss(loss_name, self.cfg)
                loss_weights[loss_name] = weight
            else:
                self.info_losses[loss_name] = builders.get_loss(loss_name, self.cfg)
        self.balancer = builders.get_balancer(loss_weights, self.cfg.balancer)
        self.register_stateful('adv_losses')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, LMModel.__init__
def __init__(self, pattern_provider: CodebooksPatternProvider, condition_provider: ConditioningProvider,
                 fuser: ConditionFuser, n_q: int = 8, card: int = 1024, dim: int = 128, num_heads: int = 8,
                 hidden_scale: int = 4, norm: str = 'layer_norm', norm_first: bool = False,
                 emb_lr: tp.Optional[float] = None, bias_proj: bool = True,
                 weight_init: tp.Optional[str] = None, depthwise_init: tp.Optional[str] = None,
                 zero_bias_init: bool = False, cfg_dropout: float = 0, cfg_coef: float = 1.0,
                 attribute_dropout: tp.Dict[str, tp.Dict[str, float]] = {}, two_step_cfg: bool = False,
                 **kwargs):
        super().__init__()
        self.cfg_coef = cfg_coef
        self.cfg_dropout = ClassifierFreeGuidanceDropout(p=cfg_dropout)
        self.att_dropout = AttributeDropout(p=attribute_dropout)
        self.condition_provider = condition_provider
        self.fuser = fuser
        self.card = card
        embed_dim = self.card + 1
        self.n_q = n_q
        self.dim = dim
        self.pattern_provider = pattern_provider
        self.two_step_cfg = two_step_cfg
        self.emb = nn.ModuleList([ScaledEmbedding(embed_dim, dim, lr=emb_lr) for _ in range(n_q)])
        if 'activation' in kwargs:
            kwargs['activation'] = get_activation_fn(kwargs['activation'])
        self.transformer = StreamingTransformer(
            d_model=dim, num_heads=num_heads, dim_feedforward=int(hidden_scale * dim),
            norm=norm, norm_first=norm_first, **kwargs)
        self.out_norm: tp.Optional[nn.Module] = None
        if norm_first:
            self.out_norm = create_norm_fn(norm, dim)
        self.linears = nn.ModuleList([nn.Linear(dim, self.card, bias=bias_proj) for _ in range(n_q)])
        self._init_weights(weight_init, depthwise_init, zero_bias_init)
        self._fsdp: tp.Optional[nn.Module]
        self.__dict__['_fsdp'] = None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen._prepare_tokens_and_attributes
def _prepare_tokens_and_attributes(
            self,
            descriptions: tp.Sequence[tp.Optional[str]],
            prompt: tp.Optional[torch.Tensor],
            melody_wavs: tp.Optional[MelodyList] = None,
    ) -> tp.Tuple[tp.List[ConditioningAttributes], tp.Optional[torch.Tensor]]:
        """Prepare model inputs.

        Args:
            descriptions (list of str): A list of strings used as text conditioning.
            prompt (torch.Tensor): A batch of waveforms used for continuation.
            melody_wavs (torch.Tensor, optional): A batch of waveforms
                used as melody conditioning. Defaults to None.
        """
        attributes = [
            ConditioningAttributes(text={'description': description})
            for description in descriptions]

        if melody_wavs is None:
            for attr in attributes:
                attr.wav['self_wav'] = WavCondition(
                    torch.zeros((1, 1, 1), device=self.device),
                    torch.tensor([0], device=self.device),
                    sample_rate=[self.sample_rate],
                    path=[None])
        else:
            if 'self_wav' not in self.lm.condition_provider.conditioners:
                raise RuntimeError("This model doesn't support melody conditioning. "
                                   "Use the `melody` model.")
            assert len(melody_wavs) == len(descriptions), \
                f"number of melody wavs must match number of descriptions! " \
                f"got melody len={len(melody_wavs)}, and descriptions len={len(descriptions)}"
            for attr, melody in zip(attributes, melody_wavs):
                if melody is None:
                    attr.wav['self_wav'] = WavCondition(
                        torch.zeros((1, 1, 1), device=self.device),
                        torch.tensor([0], device=self.device),
                        sample_rate=[self.sample_rate],
                        path=[None])
                else:
                    attr.wav['self_wav'] = WavCondition(
                        melody[None].to(device=self.device),
                        torch.tensor([melody.shape[-1]], device=self.device),
                        sample_rate=[self.sample_rate],
                        path=[None],
                    )

        if prompt is not None:
            if descriptions is not None:
                assert len(descriptions) == len(prompt), "Prompt and nb. descriptions doesn't match"
            prompt = prompt.to(self.device)
            prompt_tokens, scale = self.compression_model.encode(prompt)
            assert scale is None
        else:
            prompt_tokens = None
        return attributes, prompt_tokens
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingMultiheadAttention._apply_rope
def _apply_rope(self, query: torch.Tensor, key: torch.Tensor):
        # TODO: fix and verify layout.
        assert _efficient_attention_backend == 'xformers', "Rope not supported with torch attn."
        # Apply rope embeddings to query and key tensors.
        assert self.rope is not None
        if 'past_keys' in self._streaming_state:
            past_keys_offset = self._streaming_state['past_keys'].shape[1]
        else:
            past_keys_offset = 0
        if 'offset' in self._streaming_state:
            past_context_offset = int(self._streaming_state['offset'].item())
        else:
            past_context_offset = 0
        streaming_offset = past_context_offset + past_keys_offset
        return self.rope.rotate_qk(query, key, start=streaming_offset)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, LMModel.forward
def forward(self, sequence: torch.Tensor,
                conditions: tp.List[ConditioningAttributes],
                condition_tensors: tp.Optional[ConditionTensors] = None) -> torch.Tensor:
        """Apply language model on sequence and conditions.
        Given a tensor of sequence of shape [B, K, S] with K the number of codebooks and
        S the sequence steps, return the logits with shape [B, card, K, S].

        Args:
            indices (torch.Tensor): Indices of the codes to model.
            conditions (list of ConditioningAttributes): Conditions to use when modeling
                the given codes. Note that when evaluating multiple time with the same conditioning
                you should pre-compute those and pass them as `condition_tensors`.
            condition_tensors (dict[str, ConditionType], optional): Pre-computed conditioning
                tensors, see `conditions`.
        Returns:
            torch.Tensor: Logits.
        """
        B, K, S = sequence.shape
        assert K == self.num_codebooks, "Sequence shape must match the specified number of codebooks"
        input_ = sum([self.emb[k](sequence[:, k]) for k in range(K)])
        if condition_tensors is None:
            assert not self._is_streaming, "Conditions tensors should be precomputed when streaming."
            # apply dropout modules
            conditions = self.cfg_dropout(conditions)
            conditions = self.att_dropout(conditions)
            tokenized = self.condition_provider.tokenize(conditions)
            # encode conditions and fuse, both have a streaming cache to not recompute when generating.
            condition_tensors = self.condition_provider(tokenized)
        else:
            assert not conditions, "Shouldn't pass both conditions and condition_tensors."

        input_, cross_attention_input = self.fuser(input_, condition_tensors)

        out = self.transformer(input_, cross_attention_src=cross_attention_input)
        if self.out_norm:
            out = self.out_norm(out)
        logits = torch.stack([self.linears[k](out) for k in range(K)], dim=1)  # [B, K, S, card]

        # remove the prefix from the model outputs
        if len(self.fuser.fuse2cond['prepend']) > 0:
            logits = logits[:, :, -S:]

        return logits  # [B, K, S, card]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_meta_duration_filter_all
def test_meta_duration_filter_all(self):
        meta = [
           AudioMeta(path='a', duration=5, sample_rate=1, weight=2),
           AudioMeta(path='b', duration=10, sample_rate=1, weight=None),
           AudioMeta(path='c', duration=5, sample_rate=1, weight=0),
        ]
        try:
            AudioDataset(meta, segment_duration=11, min_segment_ratio=1)
            assert False
        except AssertionError:
            assert True
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, EmbeddingCache.__init__
def __init__(self, cache_path: tp.Union[Path], device: tp.Union[str, torch.device],
                 compute_embed_fn: tp.Callable[[Path, tp.Any, int], torch.Tensor],
                 extract_embed_fn: tp.Optional[tp.Callable[[torch.Tensor, tp.Any, int], torch.Tensor]] = None):
        self.cache_path = Path(cache_path)
        self.device = device
        self._compute_embed_fn = compute_embed_fn
        self._extract_embed_fn: tp.Callable[[torch.Tensor, tp.Any, int], torch.Tensor]
        if extract_embed_fn is not None:
            self._extract_embed_fn = extract_embed_fn
        else:
            self._extract_embed_fn = partial(get_full_embed, device=device)
        if self.cache_path is not None:
            self.cache_path.mkdir(exist_ok=True, parents=True)
            logger.info(f"Cache instantiated at: {self.cache_path}")
            self.pool = ThreadPoolExecutor(8)
            self.pool.__enter__()
        self._current_batch_cache: dict = {}
        self._memory_cache: dict = {}
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric._parallel_create_embedding_beams
def _parallel_create_embedding_beams(self, num_of_gpus: int):
        assert num_of_gpus > 0
        logger.info("Creating embeddings beams in a parallel manner on different GPUs")
        tests_beams_process, tests_beams_log_file = self._create_embedding_beams(is_background=False, gpu_index=0)
        bg_beams_process, bg_beams_log_file = self._create_embedding_beams(is_background=True, gpu_index=1)
        tests_beams_code = tests_beams_process.wait()
        bg_beams_code = bg_beams_process.wait()
        self._log_process_result(tests_beams_code, tests_beams_log_file, is_background=False)
        self._log_process_result(bg_beams_code, bg_beams_log_file, is_background=True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, NormConvTranspose1d.forward
def forward(self, x):
        x = self.convtr(x)
        x = self.norm(x)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, WaveformConditioner._downsampling_factor
def _downsampling_factor(self):
        """Returns the downsampling factor of the embedding model."""
        raise NotImplementedError()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.get_cluster
def get_cluster(cls) -> str:
        """Gets the detected cluster.
        This value can be overridden by the AUDIOCRAFT_CLUSTER env var.
        """
        return cls.instance().cluster
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_utils.py
BlockTypes.METHOD, f32_pcm
def f32_pcm(wav: torch.Tensor) -> torch.Tensor:
    """Convert audio to float 32 bits PCM format.
    """
    if wav.dtype.is_floating_point:
        return wav
    elif wav.dtype == torch.int16:
        return wav.float() / 2**15
    elif wav.dtype == torch.int32:
        return wav.float() / 2**31
    raise ValueError(f"Unsupported wav dtype: {wav.dtype}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_quantizer
def get_quantizer(quantizer: str, cfg: omegaconf.DictConfig, dimension: int) -> qt.BaseQuantizer:
    klass = {
        'no_quant': qt.DummyQuantizer,
        'rvq': qt.ResidualVectorQuantizer
    }[quantizer]
    kwargs = dict_from_config(getattr(cfg, quantizer))
    if quantizer != 'no_quant':
        kwargs['dimension'] = dimension
    return klass(**kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.postprocess
def postprocess(self,
                    x: torch.Tensor,
                    scale: tp.Optional[torch.Tensor] = None) -> torch.Tensor:
        if scale is not None:
            assert self.renormalize
            x = x * scale.view(-1, 1, 1)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\common_utils\wav_utils.py
BlockTypes.METHOD, save_wav
def save_wav(path: str, wav: torch.Tensor, sample_rate: int):
    fp = Path(path)
    kwargs: tp.Dict[str, tp.Any] = {}
    if fp.suffix == '.wav':
        kwargs['encoding'] = 'PCM_S'
        kwargs['bits_per_sample'] = 16
    elif fp.suffix == '.mp3':
        kwargs['compression'] = 320
    torchaudio.save(str(fp), wav, sample_rate, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestNormalizeAudio.test_normalize_audio_clip
def test_normalize_audio_clip(self):
        b, c, dur = 2, 1, 4.
        sr = 3
        audio = 10.0 * get_batch_white_noise(b, c, int(sr * dur))
        norm_audio = normalize_audio(audio, strategy='clip')
        assert norm_audio.abs().max() <= 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset._create_audio_dataset
def _create_audio_dataset(self,
                              root_name: str,
                              total_num_examples: int,
                              durations: tp.Union[float, tp.Tuple[float, float]] = (0.1, 1.),
                              sample_rate: int = 16_000,
                              channels: int = 1,
                              segment_duration: tp.Optional[float] = None,
                              num_examples: int = 10,
                              shuffle: bool = True,
                              return_info: bool = False):
        root_dir = self._create_audio_files(root_name, total_num_examples, durations, sample_rate, channels)
        dataset = AudioDataset.from_path(root_dir,
                                         minimal_meta=True,
                                         segment_duration=segment_duration,
                                         num_samples=num_examples,
                                         sample_rate=sample_rate,
                                         channels=channels,
                                         shuffle=shuffle,
                                         return_info=return_info)
        return dataset
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, load_clap_state_dict
def load_clap_state_dict(clap_model, path: tp.Union[str, Path]):
    """Wrapper around state dict loading of CLAP model
    addressing compatibility issues between CLAP and AudioCraft
    HuggingFace transformer version.
    See: https://github.com/LAION-AI/CLAP/issues/118
    """
    from clap_module.factory import load_state_dict  # type: ignore
    pkg = load_state_dict(path)
    pkg.pop('text_branch.embeddings.position_ids', None)
    clap_model.model.load_state_dict(pkg)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, _is_profiled
def _is_profiled() -> bool:
    # Return true if we are currently running with a xformers profiler activated.
    try:
        from xformers.profiler import profiler
    except ImportError:
        return False
    return profiler._Profiler._CURRENT_PROFILER is not None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._extract_chroma_chunk
def _extract_chroma_chunk(self, full_chroma: torch.Tensor, x: WavCondition, idx: int) -> torch.Tensor:
        """Extract a chunk of chroma from the full chroma derived from the full waveform."""
        wav_length = x.wav.shape[-1]
        seek_time = x.seek_time[idx]
        assert seek_time is not None, (
            "WavCondition seek_time is required "
            "when extracting chroma chunks from pre-computed chroma.")
        full_chroma = full_chroma.float()
        frame_rate = self.sample_rate / self._downsampling_factor()
        target_length = int(frame_rate * wav_length / self.sample_rate)
        index = int(frame_rate * seek_time)
        out = full_chroma[index: index + target_length]
        out = F.pad(out[None], (0, 0, 0, target_length - out.shape[0]))[0]
        return out.to(self.device)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, test_unpad1d
def test_unpad1d():
    x = torch.randn(1, 1, 20)

    u1 = unpad1d(x, (5, 5))
    assert u1.shape[-1] == 10
    u2 = unpad1d(x, (0, 5))
    assert u2.shape[-1] == 15
    u3 = unpad1d(x, (5, 0))
    assert u3.shape[-1] == 15
    u4 = unpad1d(x, (0, 0))
    assert u4.shape[-1] == x.shape[-1]

    with pytest.raises(AssertionError):
        unpad1d(x, (-1, 0))

    with pytest.raises(AssertionError):
        unpad1d(x, (0, -1))

    with pytest.raises(AssertionError):
        unpad1d(x, (-1, -1))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_adversary
def get_adversary(name: str, cfg: omegaconf.DictConfig) -> nn.Module:
    """Initialize adversary from config."""
    klass = {
        'msd': adversarial.MultiScaleDiscriminator,
        'mpd': adversarial.MultiPeriodDiscriminator,
        'msstftd': adversarial.MultiScaleSTFTDiscriminator,
    }[name]
    adv_cfg: tp.Dict[str, tp.Any] = dict(getattr(cfg, name))
    return klass(**adv_cfg)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, VectorQuantization.__init__
def __init__(
        self,
        dim: int,
        codebook_size: int,
        codebook_dim: tp.Optional[int] = None,
        decay: float = 0.8,
        epsilon: float = 1e-5,
        kmeans_init: bool = False,
        kmeans_iters: int = 10,
        threshold_ema_dead_code: int = 2,
        channels_last: bool = False,
        commitment_weight: float = 1.,
        orthogonal_reg_weight: float = 0.0,
        orthogonal_reg_active_codes_only: bool = False,
        orthogonal_reg_max_codes: tp.Optional[int] = None,
    ):
        super().__init__()
        _codebook_dim: int = default(codebook_dim, dim)

        requires_projection = _codebook_dim != dim
        self.project_in = (nn.Linear(dim, _codebook_dim) if requires_projection else nn.Identity())
        self.project_out = (nn.Linear(_codebook_dim, dim) if requires_projection else nn.Identity())

        self.epsilon = epsilon
        self.commitment_weight = commitment_weight

        self.orthogonal_reg_weight = orthogonal_reg_weight
        self.orthogonal_reg_active_codes_only = orthogonal_reg_active_codes_only
        self.orthogonal_reg_max_codes = orthogonal_reg_max_codes

        self._codebook = EuclideanCodebook(dim=_codebook_dim, codebook_size=codebook_size,
                                           kmeans_init=kmeans_init, kmeans_iters=kmeans_iters,
                                           decay=decay, epsilon=epsilon,
                                           threshold_ema_dead_code=threshold_ema_dead_code)
        self.codebook_size = codebook_size

        self.channels_last = channels_last
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\profiler.py
BlockTypes.METHOD, Profiler.step
def step(self):
        if self.profiler is not None:
            self.profiler.step()  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.get_dora_dir
def get_dora_dir(cls) -> Path:
        """Gets the path to the dora directory for the current team and cluster.
        Value is overridden by the AUDIOCRAFT_DORA_DIR env var.
        """
        cluster_config = cls.instance()._get_cluster_config()
        dora_dir = os.getenv("AUDIOCRAFT_DORA_DIR", cluster_config["dora_dir"])
        logger.warning(f"Dora directory: {dora_dir}")
        return Path(dora_dir)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, hinge_loss
def hinge_loss(x: torch.Tensor) -> torch.Tensor:
    if x.numel() == 0:
        return torch.tensor([0.0], device=x.device)
    return -x.mean()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_conditioner_provider
def get_conditioner_provider(output_dim: int, cfg: omegaconf.DictConfig) -> ConditioningProvider:
    """Instantiate a conditioning model."""
    device = cfg.device
    duration = cfg.dataset.segment_duration
    cfg = getattr(cfg, 'conditioners')
    dict_cfg = {} if cfg is None else dict_from_config(cfg)
    conditioners: tp.Dict[str, BaseConditioner] = {}
    condition_provider_args = dict_cfg.pop('args', {})
    condition_provider_args.pop('merge_text_conditions_p', None)
    condition_provider_args.pop('drop_desc_p', None)

    for cond, cond_cfg in dict_cfg.items():
        model_type = cond_cfg['model']
        model_args = cond_cfg[model_type]
        if model_type == 't5':
            conditioners[str(cond)] = T5Conditioner(output_dim=output_dim, device=device, **model_args)
        elif model_type == 'lut':
            conditioners[str(cond)] = LUTConditioner(output_dim=output_dim, **model_args)
        elif model_type == 'chroma_stem':
            conditioners[str(cond)] = ChromaStemConditioner(
                output_dim=output_dim,
                duration=duration,
                device=device,
                **model_args
            )
        elif model_type == 'clap':
            conditioners[str(cond)] = CLAPEmbeddingConditioner(
                output_dim=output_dim,
                device=device,
                **model_args
            )
        else:
            raise ValueError(f"Unrecognized conditioning model: {model_type}")
    conditioner = ConditioningProvider(conditioners, device=device, **condition_provider_args)
    return conditioner
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, interrupt
def interrupt():
    global INTERRUPTING
    INTERRUPTING = True
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\inverse_sqrt_lr_scheduler.py
BlockTypes.METHOD, InverseSquareRootLRScheduler.__init__
def __init__(self, optimizer: Optimizer, warmup_steps: int, warmup_init_lr: tp.Optional[float] = 0):
        self.warmup_steps = warmup_steps
        self.warmup_init_lr = warmup_init_lr
        super().__init__(optimizer)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen._generate_tokens
def _generate_tokens(self, attributes: tp.List[ConditioningAttributes],
                         prompt_tokens: tp.Optional[torch.Tensor], progress: bool = False) -> torch.Tensor:
        """Generate discrete audio tokens given audio prompt and/or conditions.

        Args:
            attributes (list of ConditioningAttributes): Conditions used for generation (here text).
            prompt_tokens (torch.Tensor, optional): Audio prompt used for continuation.
            progress (bool, optional): Flag to display progress of the generation process. Defaults to False.
        Returns:
            torch.Tensor: Generated audio, of shape [B, C, T], T is defined by the generation params.
        """
        total_gen_len = int(self.duration * self.frame_rate)
        max_prompt_len = int(min(self.duration, self.max_duration) * self.frame_rate)
        current_gen_offset: int = 0

        def _progress_callback(generated_tokens: int, tokens_to_generate: int):
            generated_tokens += current_gen_offset
            if self._progress_callback is not None:
                # Note that total_gen_len might be quite wrong depending on the
                # codebook pattern used, but with delay it is almost accurate.
                self._progress_callback(generated_tokens, total_gen_len)
            else:
                print(f'{generated_tokens: 6d} / {total_gen_len: 6d}', end='\r')

        if prompt_tokens is not None:
            assert max_prompt_len >= prompt_tokens.shape[-1], \
                "Prompt is longer than audio to generate"

        callback = None
        if progress:
            callback = _progress_callback

        if self.duration <= self.max_duration:
            # generate by sampling from LM, simple case.
            with self.autocast:
                gen_tokens = self.lm.generate(
                    prompt_tokens, attributes,
                    callback=callback, max_gen_len=total_gen_len, **self.generation_params)

        else:
            all_tokens = []
            if prompt_tokens is None:
                prompt_length = 0
            else:
                all_tokens.append(prompt_tokens)
                prompt_length = prompt_tokens.shape[-1]

            stride_tokens = int(self.frame_rate * self.extend_stride)
            while current_gen_offset + prompt_length < total_gen_len:
                time_offset = current_gen_offset / self.frame_rate
                chunk_duration = min(self.duration - time_offset, self.max_duration)
                max_gen_len = int(chunk_duration * self.frame_rate)
                with self.autocast:
                    gen_tokens = self.lm.generate(
                        prompt_tokens, attributes,
                        callback=callback, max_gen_len=max_gen_len, **self.generation_params)
                if prompt_tokens is None:
                    all_tokens.append(gen_tokens)
                else:
                    all_tokens.append(gen_tokens[:, :, prompt_tokens.shape[-1]:])
                prompt_tokens = gen_tokens[:, :, stride_tokens:]
                prompt_length = prompt_tokens.shape[-1]
                current_gen_offset += stride_tokens

            gen_tokens = torch.cat(all_tokens, dim=-1)

        # generate audio
        assert gen_tokens.dim() == 3
        with torch.no_grad():
            gen_audio = self.compression_model.decode(gen_tokens, None)
        return gen_audio
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, WaveformConditioner.forward
def forward(self, x: WavCondition) -> ConditionType:
        """Extract condition embedding and mask from a waveform and its metadata.
        Args:
            x (WavCondition): Waveform condition containing raw waveform and metadata.
        Returns:
            ConditionType: a dense vector representing the conditioning along with its mask
        """
        wav, lengths, *_ = x
        with torch.no_grad():
            embeds = self._get_wav_embedding(x)
        embeds = embeds.to(self.output_proj.weight)
        embeds = self.output_proj(embeds)

        if lengths is not None:
            lengths = lengths / self._downsampling_factor()
            mask = length_to_mask(lengths, max_len=embeds.shape[1]).int()  # type: ignore
        else:
            mask = torch.ones_like(embeds)
        embeds = (embeds * mask.unsqueeze(2).to(self.device))

        return embeds, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\train.py
BlockTypes.METHOD, resolve_config_dset_paths
def resolve_config_dset_paths(cfg):
    """Enable Dora to load manifest from git clone repository."""
    # manifest files for the different splits
    for key, value in cfg.datasource.items():
        if isinstance(value, str):
            cfg.datasource[key] = git_save.to_absolute_path(value)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\deadlock.py
BlockTypes.METHOD, DeadlockDetect.__init__
def __init__(self, use: bool = False, timeout: float = 120.):
        self.use = use
        self.timeout = timeout
        self._queue: Queue = Queue()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\dadam.py
BlockTypes.METHOD, DAdaptAdam.__init__
def __init__(self, params, lr=1.0,
                 betas=(0.9, 0.999),
                 eps=1e-8,
                 weight_decay=0,
                 log_every=0,
                 decouple=True,
                 d0=1e-6,
                 growth_rate=float('inf')):
        if not 0.0 < d0:
            raise ValueError("Invalid d0 value: {}".format(d0))
        if not 0.0 < lr:
            raise ValueError("Invalid learning rate: {}".format(lr))
        if not 0.0 < eps:
            raise ValueError("Invalid epsilon value: {}".format(eps))
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))

        if decouple:
            logger.info("Using decoupled weight decay")

        from .fsdp import is_fsdp_used
        fsdp_in_use = is_fsdp_used()
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay,
                        d=d0,
                        k=0,
                        gsq_weighted=0.0,
                        log_every=log_every,
                        decouple=decouple,
                        growth_rate=growth_rate,
                        fsdp_in_use=fsdp_in_use)

        super().__init__(params, defaults)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_dataset_samples
def test_dataset_samples(self):
        total_examples = 1
        num_samples = 2
        audio_duration = 1.
        segment_duration = 1.
        sample_rate = 16_000
        channels = 1

        create_dataset = partial(
            self._create_audio_dataset,
            'dset', total_examples, durations=audio_duration, sample_rate=sample_rate,
            channels=channels, segment_duration=segment_duration, num_examples=num_samples,
        )

        dataset = create_dataset(shuffle=True)
        # when shuffle = True, we have different inputs for the same index across epoch
        sample_1 = dataset[0]
        sample_2 = dataset[0]
        assert not torch.allclose(sample_1, sample_2)

        dataset_noshuffle = create_dataset(shuffle=False)
        # when shuffle = False, we have same inputs for the same index across epoch
        sample_1 = dataset_noshuffle[0]
        sample_2 = dataset_noshuffle[0]
        assert torch.allclose(sample_1, sample_2)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.wrapped_model_from_checkpoint
def wrapped_model_from_checkpoint(cfg: omegaconf.DictConfig,
                                      checkpoint_path: tp.Union[Path, str],
                                      device: tp.Union[torch.device, str] = 'cpu') -> models.CompressionModel:
        """Instantiate a wrapped CompressionModel from a given checkpoint path or dora sig.

        Args:
            cfg (omegaconf.DictConfig): Configuration to read from for wrapped mode.
            checkpoint_path (Path or str): Path to checkpoint or dora sig from where the checkpoint is resolved.
            use_ema (bool): Use EMA variant of the model instead of the actual model.
            device (torch.device or str): Device on which the model is loaded.
        """
        compression_model = CompressionSolver.model_from_checkpoint(checkpoint_path, device)
        compression_model = models.builders.get_wrapped_compression_model(compression_model, cfg)
        return compression_model
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\loaders.py
BlockTypes.METHOD, get_audiocraft_cache_dir
def get_audiocraft_cache_dir() -> tp.Optional[str]:
    return os.environ.get('AUDIOCRAFT_CACHE_DIR', None)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\best_state.py
BlockTypes.METHOD, BestStateDictManager.state_dict
def state_dict(self) -> flashy.state.StateDict:
        return self.states
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, _match_stable_samples
def _match_stable_samples(samples_per_xp: tp.List[tp.Set[Sample]]) -> tp.Dict[str, tp.List[Sample]]:
    # Create a dictionary of stable id -> sample per XP
    stable_samples_per_xp = [{
        sample.id: sample for sample in samples
        if sample.prompt is not None or sample.conditioning
    } for samples in samples_per_xp]
    # Set of all stable ids
    stable_ids = {id for samples in stable_samples_per_xp for id in samples.keys()}
    # Dictionary of stable id -> list of samples. If an XP does not have it, assign None
    stable_samples = {id: [xp.get(id) for xp in stable_samples_per_xp] for id in stable_ids}
    # Filter out ids that contain None values (we only want matched samples after all)
    # cast is necessary to avoid mypy linter errors.
    return {id: tp.cast(tp.List[Sample], samples) for id, samples in stable_samples.items() if None not in samples}
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_seanet.py
BlockTypes.METHOD, TestSEANetModel._check_decoder_blocks_norm
def _check_decoder_blocks_norm(self, decoder: SEANetDecoder, n_disable_blocks: int, norm: str):
        n_blocks = 0
        for layer in decoder.model:
            if isinstance(layer, StreamableConv1d):
                n_blocks += 1
                assert layer.conv.norm_type == 'none' if (decoder.n_blocks - n_blocks) < n_disable_blocks else norm
            elif isinstance(layer, StreamableConvTranspose1d):
                n_blocks += 1
                assert layer.convtr.norm_type == 'none' if (decoder.n_blocks - n_blocks) < n_disable_blocks else norm
            elif isinstance(layer, SEANetResnetBlock):
                for resnet_layer in layer.block:
                    if isinstance(resnet_layer, StreamableConv1d):
                        assert resnet_layer.conv.norm_type == 'none' \
                            if (decoder.n_blocks - n_blocks) < n_disable_blocks else norm
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, DropoutModule.__init__
def __init__(self, seed: int = 1234):
        super().__init__()
        self.rng = torch.Generator()
        self.rng.manual_seed(seed)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\ema.py
BlockTypes.METHOD, ModuleDictEMA._init
def _init(self):
        for module_name, module in self.module_dict.items():
            for key, val in _get_named_tensors(module):
                if not val.is_floating_point():
                    continue
                device = self.device or val.device
                if key not in self.state[module_name]:
                    self.state[module_name][key] = val.detach().to(device, copy=True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\_base_explorers.py
BlockTypes.METHOD, BaseExplorer.stages
def stages(self):
        return ["train", "valid", "evaluate"]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, ResidualVectorQuantization.decode
def decode(self, q_indices: torch.Tensor) -> torch.Tensor:
        quantized_out = torch.tensor(0.0, device=q_indices.device)
        for i, indices in enumerate(q_indices):
            layer = self.layers[i]
            quantized = layer.decode(indices)
            quantized_out = quantized_out + quantized
        return quantized_out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_dataset_full
def test_dataset_full(self):
        total_examples = 10
        min_duration, max_duration = 1., 4.
        sample_rate = 16_000
        channels = 1
        dataset = self._create_audio_dataset(
            'dset', total_examples, durations=(min_duration, max_duration),
            sample_rate=sample_rate, channels=channels, segment_duration=None)
        assert len(dataset) == total_examples
        assert dataset.sample_rate == sample_rate
        assert dataset.channels == channels
        for idx in range(len(dataset)):
            sample = dataset[idx]
            assert sample.shape[0] == channels
            assert sample.shape[1] <= int(max_duration * sample_rate)
            assert sample.shape[1] >= int(min_duration * sample_rate)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, MusicDataset.__getitem__
def __getitem__(self, index):
        wav, info = super().__getitem__(index)
        info_data = info.to_dict()
        music_info_path = Path(info.meta.path).with_suffix('.json')

        if Path(music_info_path).exists():
            with open(music_info_path, 'r') as json_file:
                music_data = json.load(json_file)
                music_data.update(info_data)
                music_info = MusicInfo.from_dict(music_data, fields_required=self.info_fields_required)
            if self.paraphraser is not None:
                music_info.description = self.paraphraser.sample(music_info.meta.path, music_info.description)
            if self.merge_text_p:
                music_info = augment_music_info_description(
                    music_info, self.merge_text_p, self.drop_desc_p, self.drop_other_p)
        else:
            music_info = MusicInfo.from_dict(info_data, fields_required=False)

        music_info.self_wav = WavCondition(
            wav=wav[None], length=torch.tensor([info.n_frames]),
            sample_rate=[info.sample_rate], path=[info.meta.path], seek_time=[info.seek_time])

        for att in self.joint_embed_attributes:
            att_value = getattr(music_info, att)
            joint_embed_cond = JointEmbedCondition(
                wav[None], [att_value], torch.tensor([info.n_frames]),
                sample_rate=[info.sample_rate], path=[info.meta.path], seek_time=[info.seek_time])
            music_info.joint_embed[att] = joint_embed_cond

        return wav, music_info
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.evaluate_audio_generation
def evaluate_audio_generation(self) -> dict:
        """Evaluate audio generation with off-the-shelf metrics."""
        evaluate_stage_name = f'{self.current_stage}_generation'
        # instantiate evaluation metrics, if at least one metric is defined, run audio generation evaluation
        fad: tp.Optional[eval_metrics.FrechetAudioDistanceMetric] = None
        kldiv: tp.Optional[eval_metrics.KLDivergenceMetric] = None
        text_consistency: tp.Optional[eval_metrics.TextConsistencyMetric] = None
        chroma_cosine: tp.Optional[eval_metrics.ChromaCosineSimilarityMetric] = None
        should_run_eval = False
        eval_chroma_wavs: tp.Optional[torch.Tensor] = None
        if self.cfg.evaluate.metrics.fad:
            fad = builders.get_fad(self.cfg.metrics.fad).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.kld:
            kldiv = builders.get_kldiv(self.cfg.metrics.kld).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.text_consistency:
            text_consistency = builders.get_text_consistency(self.cfg.metrics.text_consistency).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.chroma_cosine:
            chroma_cosine = builders.get_chroma_cosine_similarity(self.cfg.metrics.chroma_cosine).to(self.device)
            # if we have predefind wavs for chroma we should purge them for computing the cosine metric
            has_predefined_eval_chromas = 'self_wav' in self.model.condition_provider.conditioners and \
                                          self.model.condition_provider.conditioners['self_wav'].has_eval_wavs()
            if has_predefined_eval_chromas:
                warn_once(self.logger, "Attempting to run cosine eval for config with pre-defined eval chromas! "
                                       'Resetting eval chromas to None for evaluation.')
                eval_chroma_wavs = self.model.condition_provider.conditioners.self_wav.eval_wavs  # type: ignore
                self.model.condition_provider.conditioners.self_wav.reset_eval_wavs(None)  # type: ignore
            should_run_eval = True

        def get_compressed_audio(audio: torch.Tensor) -> torch.Tensor:
            audio_tokens, scale = self.compression_model.encode(audio.to(self.device))
            compressed_audio = self.compression_model.decode(audio_tokens, scale)
            return compressed_audio[..., :audio.shape[-1]]

        metrics: dict = {}
        if should_run_eval:
            loader = self.dataloaders['evaluate']
            updates = len(loader)
            lp = self.log_progress(f'{evaluate_stage_name} inference', loader, total=updates, updates=self.log_updates)
            average = flashy.averager()
            dataset = get_dataset_from_loader(loader)
            assert isinstance(dataset, AudioDataset)
            self.logger.info(f"Computing evaluation metrics on {len(dataset)} samples")

            for idx, batch in enumerate(lp):
                audio, meta = batch
                assert all([self.cfg.sample_rate == m.sample_rate for m in meta])

                target_duration = audio.shape[-1] / self.cfg.sample_rate
                if self.cfg.evaluate.fixed_generation_duration:
                    target_duration = self.cfg.evaluate.fixed_generation_duration

                gen_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration,
                    **self.generation_params
                )
                y_pred = gen_outputs['gen_audio'].detach()
                y_pred = y_pred[..., :audio.shape[-1]]

                normalize_kwargs = dict(self.cfg.generate.audio)
                normalize_kwargs.pop('format', None)
                y_pred = torch.stack([normalize_audio(w, **normalize_kwargs) for w in y_pred], dim=0).cpu()
                y = audio.cpu()  # should already be on CPU but just in case
                sizes = torch.tensor([m.n_frames for m in meta])  # actual sizes without padding
                sample_rates = torch.tensor([m.sample_rate for m in meta])  # sample rates for audio samples
                audio_stems = [Path(m.meta.path).stem + f"_{m.seek_time}" for m in meta]

                if fad is not None:
                    if self.cfg.metrics.fad.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    fad.update(y_pred, y, sizes, sample_rates, audio_stems)
                if kldiv is not None:
                    if self.cfg.metrics.kld.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    kldiv.update(y_pred, y, sizes, sample_rates)
                if text_consistency is not None:
                    texts = [m.description for m in meta]
                    if self.cfg.metrics.text_consistency.use_gt:
                        y_pred = y
                    text_consistency.update(y_pred, texts, sizes, sample_rates)
                if chroma_cosine is not None:
                    if self.cfg.metrics.chroma_cosine.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    chroma_cosine.update(y_pred, y, sizes, sample_rates)
                    # restore chroma conditioner's eval chroma wavs
                    if eval_chroma_wavs is not None:
                        self.model.condition_provider.conditioners['self_wav'].reset_eval_wavs(eval_chroma_wavs)

            flashy.distrib.barrier()
            if fad is not None:
                metrics['fad'] = fad.compute()
            if kldiv is not None:
                kld_metrics = kldiv.compute()
                metrics.update(kld_metrics)
            if text_consistency is not None:
                metrics['text_consistency'] = text_consistency.compute()
            if chroma_cosine is not None:
                metrics['chroma_cosine'] = chroma_cosine.compute()
            metrics = average(metrics)
            metrics = flashy.distrib.average_metrics(metrics, len(loader))

        return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\fsdp.py
BlockTypes.METHOD, _FSDPFixStateDict._name_without_fsdp_prefix
def _name_without_fsdp_prefix(name: str) -> str:
        from torch.distributed.fsdp._common_utils import FSDP_WRAPPED_MODULE  # type: ignore
        parts = name.split('.')
        new_parts = [part for part in parts if part != FSDP_WRAPPED_MODULE]
        return '.'.join(new_parts)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msd.py
BlockTypes.METHOD, ScaleDiscriminator.forward
def forward(self, x: torch.Tensor):
        fmap = []
        for layer in self.convs:
            x = layer(x)
            x = self.activation(x)
            fmap.append(x)
        x = self.conv_post(x)
        fmap.append(x)
        # x = torch.flatten(x, 1, -1)
        return x, fmap
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, _stft
def _stft(x: torch.Tensor, fft_size: int, hop_length: int, win_length: int,
          window: tp.Optional[torch.Tensor], normalized: bool) -> torch.Tensor:
    """Perform STFT and convert to magnitude spectrogram.

    Args:
        x: Input signal tensor (B, C, T).
        fft_size (int): FFT size.
        hop_length (int): Hop size.
        win_length (int): Window length.
        window (torch.Tensor or None): Window function type.
        normalized (bool): Whether to normalize the STFT or not.

    Returns:
        torch.Tensor: Magnitude spectrogram (B, C, #frames, fft_size // 2 + 1).
    """
    B, C, T = x.shape
    x_stft = torch.stft(
        x.view(-1, T), fft_size, hop_length, win_length, window,
        normalized=normalized, return_complex=True,
    )
    x_stft = x_stft.view(B, C, *x_stft.shape[1:])
    real = x_stft.real
    imag = x_stft.imag

    # NOTE(kan-bayashi): clamp is needed to avoid nan or inf
    return torch.sqrt(torch.clamp(real ** 2 + imag ** 2, min=1e-7)).transpose(2, 1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestInfo.test_info_mp3
def test_info_mp3(self):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 1.
        for sample_rate, ch in product(sample_rates, channels):
            wav = get_white_noise(ch, int(sample_rate * duration))
            path = self.get_temp_path('sample_wav.mp3')
            save_wav(path, wav, sample_rate)
            info = audio_info(path)
            assert info.sample_rate == sample_rate
            assert info.channels == ch
            # we cannot trust torchaudio for num_frames, so we don't check
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_meta_duration_filter_long
def test_meta_duration_filter_long(self):
        meta = [
           AudioMeta(path='a', duration=5, sample_rate=1, weight=2),
           AudioMeta(path='b', duration=10, sample_rate=1, weight=None),
           AudioMeta(path='c', duration=5, sample_rate=1, weight=0),
        ]
        dataset = AudioDataset(meta, segment_duration=None, min_segment_ratio=1, max_audio_duration=7)
        assert len(dataset) == 2
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_utils.py
BlockTypes.METHOD, i16_pcm
def i16_pcm(wav: torch.Tensor) -> torch.Tensor:
    """Convert audio to int 16 bits PCM format.

    ..Warning:: There exist many formula for doing this conversion. None are perfect
    due to the asymmetry of the int16 range. One either have possible clipping, DC offset,
    or inconsistencies with f32_pcm. If the given wav doesn't have enough headroom,
    it is possible that `i16_pcm(f32_pcm)) != Identity`.
    """
    if wav.dtype.is_floating_point:
        assert wav.abs().max() <= 1
        candidate = (wav * 2 ** 15).round()
        if candidate.max() >= 2 ** 15:  # clipping would occur
            candidate = (wav * (2 ** 15 - 1)).round()
        return candidate.short()
    else:
        assert wav.dtype == torch.int16
        return wav
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\best_state.py
BlockTypes.METHOD, BestStateDictManager.load_state_dict
def load_state_dict(self, state: flashy.state.StateDict):
        for name, sub_state in state.items():
            for k, v in sub_state.items():
                self.states[name][k].copy_(v)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, MusicInfo.has_music_meta
def has_music_meta(self) -> bool:
        return self.name is not None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\loaders.py
BlockTypes.METHOD, _get_state_dict
def _get_state_dict(
    file_or_url_or_id: tp.Union[Path, str],
    filename: tp.Optional[str] = None,
    device='cpu',
    cache_dir: tp.Optional[str] = None,
):
    if cache_dir is None:
        cache_dir = get_audiocraft_cache_dir()
    # Return the state dict either from a file or url
    file_or_url_or_id = str(file_or_url_or_id)
    assert isinstance(file_or_url_or_id, str)

    if os.path.isfile(file_or_url_or_id):
        return torch.load(file_or_url_or_id, map_location=device)

    if os.path.isdir(file_or_url_or_id):
        file = f"{file_or_url_or_id}/{filename}"
        return torch.load(file, map_location=device)

    elif file_or_url_or_id.startswith('https://'):
        return torch.hub.load_state_dict_from_url(file_or_url_or_id, map_location=device, check_hash=True)

    else:
        assert filename is not None, "filename needs to be defined if using HF checkpoints"

        file = hf_hub_download(repo_id=file_or_url_or_id, filename=filename, cache_dir=cache_dir)
        return torch.load(file, map_location=device)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, NormConvTranspose2d.__init__
def __init__(self, *args, norm: str = 'none', norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
        super().__init__()
        self.convtr = apply_parametrization_norm(nn.ConvTranspose2d(*args, **kwargs), norm)
        self.norm = get_norm_module(self.convtr, causal=False, norm=norm, **norm_kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\common_utils\temp_utils.py
BlockTypes.METHOD, TempDirMixin.tearDownClass
def tearDownClass(cls):
        if cls.temp_dir_ is not None:
            try:
                cls.temp_dir_.cleanup()
                cls.temp_dir_ = None
            except PermissionError:
                # On Windows there is a know issue with `shutil.rmtree`,
                # which fails intermittently.
                # https://github.com/python/cpython/issues/74168
                # Following the above thread, we ignore it.
                pass
        super().tearDownClass()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\_base_explorers.py
BlockTypes.METHOD, BaseExplorer.get_grid_meta
def get_grid_meta(self):
        """Returns the list of Meta information to display for each XP/job.
        """
        return [
            tt.leaf("index", align=">"),
            tt.leaf("name", wrap=140),
            tt.leaf("state"),
            tt.leaf("sig", align=">"),
            tt.leaf("sid", align="<"),
        ]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, create_norm_fn
def create_norm_fn(norm_type: str, dim: int, **kwargs) -> nn.Module:
    """Create normalization module for transformer encoder layer.

    Args:
        norm_type (str): Normalization method.
        dim (int): Dimension of the normalized layer.
        **kwargs (dict): Additional parameters for normalization layer.
    Returns:
        nn.Module: Normalization module.
    """
    if norm_type == 'layer_norm':
        return nn.LayerNorm(dim, eps=1e-5, **kwargs)
    else:
        raise ValueError(f"Unknown norm type: {norm_type}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.get_reference_dir
def get_reference_dir(cls) -> Path:
        """Gets the path to the reference directory for the current team and cluster.
        Value is overridden by the AUDIOCRAFT_REFERENCE_DIR env var.
        """
        cluster_config = cls.instance()._get_cluster_config()
        return Path(os.getenv("AUDIOCRAFT_REFERENCE_DIR", cluster_config["reference_dir"]))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.forward
def forward(self, x: torch.Tensor) -> qt.QuantizedResult:
        assert x.dim() == 3
        length = x.shape[-1]
        x, scale = self.preprocess(x)

        emb = self.encoder(x)
        q_res = self.quantizer(emb, self.frame_rate)
        out = self.decoder(q_res.x)

        # remove extra padding added by the encoder and decoder
        assert out.shape[-1] >= length, (out.shape[-1], length)
        out = out[..., :length]

        q_res.x = self.postprocess(out, scale)

        return q_res
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider._collate_wavs
def _collate_wavs(self, samples: tp.List[ConditioningAttributes]) -> tp.Dict[str, WavCondition]:
        """Generate a dict where the keys are attributes by which we fetch similar wavs,
        and the values are Tensors of wavs according to said attributes.

        *Note*: by the time the samples reach this function, each sample should have some waveform
        inside the "wav" attribute. It should be either:
        1. A real waveform
        2. A null waveform due to the sample having no similar waveforms (nullified by the dataset)
        3. A null waveform due to it being dropped in a dropout module (nullified by dropout)

        Args:
            samples (list of ConditioningAttributes): List of ConditioningAttributes samples.
        Returns:
            dict[str, WavCondition]: A dictionary mapping an attribute name to wavs.
        """
        wavs = defaultdict(list)
        lengths = defaultdict(list)
        sample_rates = defaultdict(list)
        paths = defaultdict(list)
        seek_times = defaultdict(list)
        out: tp.Dict[str, WavCondition] = {}

        for sample in samples:
            for attribute in self.wav_conditions:
                wav, length, sample_rate, path, seek_time = sample.wav[attribute]
                assert wav.dim() == 3, f"Got wav with dim={wav.dim()}, but expected 3 [1, C, T]"
                assert wav.size(0) == 1, f"Got wav [B, C, T] with shape={wav.shape}, but expected B == 1"
                # mono-channel conditioning
                wav = wav.mean(1, keepdim=True)  # [1, 1, T]
                wavs[attribute].append(wav.flatten())  # [T]
                lengths[attribute].append(length)
                sample_rates[attribute].extend(sample_rate)
                paths[attribute].extend(path)
                seek_times[attribute].extend(seek_time)

        # stack all wavs to a single tensor
        for attribute in self.wav_conditions:
            stacked_wav, _ = collate(wavs[attribute], dim=0)
            out[attribute] = WavCondition(
                stacked_wav.unsqueeze(1), torch.cat(lengths[attribute]), sample_rates[attribute],
                paths[attribute], seek_times[attribute])

        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, FileCleaner.__init__
def __init__(self, file_lifetime: float = 3600):
        self.file_lifetime = file_lifetime
        self.files = []
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_adversarial_losses
def get_adversarial_losses(cfg) -> nn.ModuleDict:
    """Initialize dict of adversarial losses from config."""
    device = cfg.device
    adv_cfg = getattr(cfg, 'adversarial')
    adversaries = adv_cfg.get('adversaries', [])
    adv_loss_name = adv_cfg['adv_loss']
    feat_loss_name = adv_cfg.get('feat_loss')
    normalize = adv_cfg.get('normalize', True)
    feat_loss: tp.Optional[adversarial.FeatureMatchingLoss] = None
    if feat_loss_name:
        assert feat_loss_name in ['l1', 'l2'], f"Feature loss only support L1 or L2 but {feat_loss_name} found."
        loss = get_loss(feat_loss_name, cfg)
        feat_loss = adversarial.FeatureMatchingLoss(loss, normalize)
    loss = adversarial.get_adv_criterion(adv_loss_name)
    loss_real = adversarial.get_real_criterion(adv_loss_name)
    loss_fake = adversarial.get_fake_criterion(adv_loss_name)
    adv_losses = nn.ModuleDict()
    for adv_name in adversaries:
        adversary = get_adversary(adv_name, cfg).to(device)
        optimizer = get_optimizer(adversary.parameters(), cfg.optim)
        adv_loss = adversarial.AdversarialLoss(
            adversary,
            optimizer,
            loss=loss,
            loss_real=loss_real,
            loss_fake=loss_fake,
            loss_feat=feat_loss,
            normalize=normalize
        )
        adv_losses[adv_name] = adv_loss
    return adv_losses
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\checkpoint.py
BlockTypes.METHOD, resolve_checkpoint_path
def resolve_checkpoint_path(sig_or_path: tp.Union[Path, str], name: tp.Optional[str] = None,
                            use_fsdp: bool = False) -> tp.Optional[Path]:
    """Resolve a given checkpoint path for a provided dora sig or path.

    Args:
        sig_or_path (Path or str): Checkpoint path or dora signature.
        name (str, optional): Name suffix for the checkpoint file stem.
        rank (optional, int): Rank for distributed processing, retrieved with flashy if not provided.
        use_fsdp (bool): Whether the calling solver relies on FSDP.
    Returns:
        Path, optional: Resolved checkpoint path, if it exists.
    """
    from audiocraft import train
    xps_root = train.main.dora.dir / 'xps'
    sig_or_path = str(sig_or_path)
    if sig_or_path.startswith('//sig/'):
        sig = sig_or_path[len('//sig/'):]
        path = xps_root / sig
    else:
        path = Path(sig_or_path)
        path = AudioCraftEnvironment.resolve_reference_path(path)

    if path.is_dir():
        path = path / checkpoint_name(name, use_fsdp=use_fsdp)

    if path.exists():
        return path
    else:
        return None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_encodec_model.py
BlockTypes.METHOD, TestEncodecModel._create_encodec_model
def _create_encodec_model(self,
                              sample_rate: int,
                              channels: int,
                              dim: int = 5,
                              n_filters: int = 3,
                              n_residual_layers: int = 1,
                              ratios: list = [5, 4, 3, 2],
                              **kwargs):
        frame_rate = np.prod(ratios)
        encoder = SEANetEncoder(channels=channels, dimension=dim, n_filters=n_filters,
                                n_residual_layers=n_residual_layers, ratios=ratios)
        decoder = SEANetDecoder(channels=channels, dimension=dim, n_filters=n_filters,
                                n_residual_layers=n_residual_layers, ratios=ratios)
        quantizer = DummyQuantizer()
        model = EncodecModel(encoder, decoder, quantizer, frame_rate=frame_rate,
                             sample_rate=sample_rate, channels=channels, **kwargs)
        return model
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestNormalizeAudio.test_normalize_audio_rms
def test_normalize_audio_rms(self):
        b, c, dur = 2, 1, 4.
        sr = 3
        audio = 10.0 * get_batch_white_noise(b, c, int(sr * dur))
        norm_audio = normalize_audio(audio, strategy='rms')
        assert norm_audio.abs().max() <= 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\inverse_sqrt_lr_scheduler.py
BlockTypes.METHOD, InverseSquareRootLRScheduler._get_sched_lr
def _get_sched_lr(self, lr: float, step: int):
        if step < self.warmup_steps:
            warmup_init_lr = self.warmup_init_lr or 0
            lr_step = (lr - warmup_init_lr) / self.warmup_steps
            lr = warmup_init_lr + step * lr_step
        else:
            decay_factor = lr * self.warmup_steps**0.5
            lr = decay_factor * step**-0.5
        return lr
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.regenerate
def regenerate(self, wav: torch.Tensor, step_list: tp.Optional[list] = None):
        """Regenerate the given waveform."""
        condition = self.get_condition(wav)
        initial = self.schedule.get_initial_noise(self.data_processor.process_data(wav))  # sampling rate changes.
        result = self.schedule.generate_subsampled(self.model, initial=initial, condition=condition,
                                                   step_list=step_list)
        result = self.data_processor.inverse_process(result)
        return result
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\seanet.py
BlockTypes.METHOD, SEANetResnetBlock.forward
def forward(self, x):
        return self.shortcut(x) + self.block(x)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, hinge2_loss
def hinge2_loss(x: torch.Tensor) -> torch.Tensor:
    if x.numel() == 0:
        return torch.tensor([0.0])
    return -torch.mean(torch.min(x - 1, torch.tensor(0., device=x.device).expand_as(x)))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\deadlock.py
BlockTypes.METHOD, DeadlockDetect.update
def update(self, stage: str):
        if self.use:
            self._queue.put(stage)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\profiler.py
BlockTypes.METHOD, Profiler.__enter__
def __enter__(self):
        if self.profiler is not None:
            return self.profiler.__enter__()  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_encodec_autoencoder
def get_encodec_autoencoder(encoder_name: str, cfg: omegaconf.DictConfig):
    if encoder_name == 'seanet':
        kwargs = dict_from_config(getattr(cfg, 'seanet'))
        encoder_override_kwargs = kwargs.pop('encoder')
        decoder_override_kwargs = kwargs.pop('decoder')
        encoder_kwargs = {**kwargs, **encoder_override_kwargs}
        decoder_kwargs = {**kwargs, **decoder_override_kwargs}
        encoder = audiocraft.modules.SEANetEncoder(**encoder_kwargs)
        decoder = audiocraft.modules.SEANetDecoder(**decoder_kwargs)
        return encoder, decoder
    else:
        raise KeyError(f"Unexpected compression model {cfg.compression_model}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.frame_rate
def frame_rate(self) -> float:
        """Roughly the number of AR steps per seconds."""
        return self.compression_model.frame_rate
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern._build_reverted_sequence_scatter_indexes
def _build_reverted_sequence_scatter_indexes(self, sequence_steps: int, n_q: int,
                                                 keep_only_valid_steps: bool = False,
                                                 is_model_output: bool = False,
                                                 device: tp.Union[torch.device, str] = 'cpu'):
        """Builds scatter indexes required to retrieve the original multi-codebook sequence
        from interleaving pattern.

        Args:
            sequence_steps (int): Sequence steps.
            n_q (int): Number of codebooks.
            keep_only_valid_steps (bool): Build a sequence from the pattern up to valid (= fully defined) steps.
                Steps that are beyond valid steps will be replaced by the special_token in that case.
            is_model_output (bool): Whether to keep the sequence item corresponding to initial special token or not.
            device (torch.device or str): Device for created tensors.
        Returns:
            indexes (torch.Tensor): Indexes for reconstructing the output, of shape [K, T].
            mask (torch.Tensor): Mask corresponding to indexes that matches valid indexes of shape [K, T].
        """
        ref_layout = self.valid_layout if keep_only_valid_steps else self.layout
        # TODO(jade): Do we want to further truncate to only valid timesteps here as well?
        timesteps = self.timesteps
        assert n_q == self.n_q, f"invalid number of codebooks for the sequence and the pattern: {n_q} != {self.n_q}"
        assert sequence_steps <= len(ref_layout), \
            f"sequence to revert is longer than the defined pattern: {sequence_steps} > {len(ref_layout)}"

        # ensure we take the appropriate indexes to keep the model output from the first special token as well
        if is_model_output:
            ref_layout = ref_layout[1:]

        # single item indexing being super slow with pytorch vs. numpy, so we use numpy here
        indexes = torch.zeros(n_q, timesteps, dtype=torch.long).numpy()
        mask = torch.zeros(n_q, timesteps, dtype=torch.bool).numpy()
        # fill indexes with last sequence step value that will correspond to our special token
        indexes[:] = n_q * sequence_steps
        for s, sequence_codes in enumerate(ref_layout):
            if s < sequence_steps:
                for code in sequence_codes:
                    if code.t < timesteps:
                        indexes[code.q, code.t] = s + code.q * sequence_steps
                        mask[code.q, code.t] = 1
        indexes = torch.from_numpy(indexes).to(device)
        mask = torch.from_numpy(mask).to(device)
        return indexes, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_dataset_collate_fn
def test_dataset_collate_fn(self):
        total_examples = 10
        num_samples = 20
        min_duration, max_duration = 1., 4.
        segment_duration = 1.
        sample_rate = 16_000
        channels = 1
        dataset = self._create_audio_dataset(
            'dset', total_examples, durations=(min_duration, max_duration), sample_rate=sample_rate,
            channels=channels, segment_duration=segment_duration, num_examples=num_samples, return_info=False)
        batch_size = 4
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            num_workers=0
        )
        for idx, batch in enumerate(dataloader):
            assert batch.shape[0] == batch_size
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio.py
BlockTypes.METHOD, _av_info
def _av_info(filepath: tp.Union[str, Path]) -> AudioFileInfo:
    _init_av()
    with av.open(str(filepath)) as af:
        stream = af.streams.audio[0]
        sample_rate = stream.codec_context.sample_rate
        duration = float(stream.duration * stream.time_base)
        channels = stream.channels
        return AudioFileInfo(sample_rate, duration, channels)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, MusicInfo.to_condition_attributes
def to_condition_attributes(self) -> ConditioningAttributes:
        out = ConditioningAttributes()
        for _field in fields(self):
            key, value = _field.name, getattr(self, _field.name)
            if key == 'self_wav':
                out.wav[key] = value
            elif key == 'joint_embed':
                for embed_attribute, embed_cond in value.items():
                    out.joint_embed[embed_attribute] = embed_cond
            else:
                if isinstance(value, list):
                    value = ' '.join(value)
                out.text[key] = value
        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_optimizer
def get_optimizer(params: tp.Union[nn.Module, tp.Iterable[torch.Tensor]], cfg: omegaconf.DictConfig) -> Optimizer:
    """Build torch optimizer from config and set of parameters.
    Supported optimizers: Adam, AdamW

    Args:
        params (nn.Module or iterable of torch.Tensor): Parameters to optimize.
        cfg (DictConfig): Optimization-related configuration.
    Returns:
        torch.optim.Optimizer.
    """
    if 'optimizer' not in cfg:
        if getattr(cfg, 'optim', None) is not None:
            raise KeyError("Optimizer not found in config. Try instantiating optimizer from cfg.optim?")
        else:
            raise KeyError("Optimizer not found in config.")

    parameters = get_optim_parameter_groups(params) if isinstance(params, nn.Module) else params
    optimizer: torch.optim.Optimizer
    if cfg.optimizer == 'adam':
        optimizer = torch.optim.Adam(parameters, lr=cfg.lr, **cfg.adam)
    elif cfg.optimizer == 'adamw':
        optimizer = torch.optim.AdamW(parameters, lr=cfg.lr, **cfg.adam)
    elif cfg.optimizer == 'dadam':
        optimizer = optim.DAdaptAdam(parameters, lr=cfg.lr, **cfg.adam)
    else:
        raise ValueError(f"Unsupported LR Scheduler: {cfg.lr_scheduler}")
    return optimizer
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, SoundDataset.__init__
def __init__(
        self,
        *args,
        info_fields_required: bool = True,
        external_metadata_source: tp.Optional[str] = None,
        aug_p: float = 0.,
        mix_p: float = 0.,
        mix_snr_low: int = -5,
        mix_snr_high: int = 5,
        mix_min_overlap: float = 0.5,
        **kwargs
    ):
        kwargs['return_info'] = True  # We require the info for each song of the dataset.
        super().__init__(*args, **kwargs)
        self.info_fields_required = info_fields_required
        self.external_metadata_source = external_metadata_source
        self.aug_p = aug_p
        self.mix_p = mix_p
        if self.aug_p > 0:
            assert self.mix_p > 0, "Expecting some mixing proportion mix_p if aug_p > 0"
            assert self.channels == 1, "SoundDataset with audio mixing considers only monophonic audio"
        self.mix_snr_low = mix_snr_low
        self.mix_snr_high = mix_snr_high
        self.mix_min_overlap = mix_min_overlap
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_cross_attention_compat
def test_cross_attention_compat():
    torch.manual_seed(1234)
    num_heads = 2
    dim = num_heads * 64
    with pytest.raises(AssertionError):
        StreamingMultiheadAttention(dim, num_heads, causal=True, cross_attention=True)

    cross_attn = StreamingMultiheadAttention(
        dim, num_heads, dropout=0, cross_attention=True, custom=True)
    ref_attn = torch.nn.MultiheadAttention(dim, num_heads, dropout=0, batch_first=True)

    # We can load the regular attention state dict
    # so we have compat when loading old checkpoints.
    cross_attn.load_state_dict(ref_attn.state_dict())

    queries = torch.randn(3, 7, dim)
    keys = torch.randn(3, 9, dim)
    values = torch.randn(3, 9, dim)

    y = cross_attn(queries, keys, values)[0]
    y_ref = ref_attn(queries, keys, values)[0]
    assert torch.allclose(y, y_ref, atol=1e-7), (y - y_ref).norm() / y_ref.norm()

    # Now let's check that streaming is working properly.
    with cross_attn.streaming():
        ys = []
        for step in range(queries.shape[1]):
            ys.append(cross_attn(queries[:, step: step + 1], keys, values)[0])
    y_streaming = torch.cat(ys, dim=1)
    assert torch.allclose(y_streaming, y, atol=1e-7)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, get_init_fn
def get_init_fn(method: str, input_dim: int, init_depth: tp.Optional[int] = None):
    """LM layer initialization.
    Inspired from xlformers: https://github.com/fairinternal/xlformers

    Args:
        method (str): Method name for init function. Valid options are:
            'gaussian', 'uniform'.
        input_dim (int): Input dimension of the initialized module.
        init_depth (int, optional): Optional init depth value used to rescale
            the standard deviation if defined.
    """
    # Compute std
    std = 1 / math.sqrt(input_dim)
    # Rescale with depth
    if init_depth is not None:
        std = std / math.sqrt(2 * init_depth)

    if method == 'gaussian':
        return partial(
            torch.nn.init.trunc_normal_, mean=0.0, std=std, a=-3 * std, b=3 * std
        )
    elif method == 'uniform':
        bound = math.sqrt(3) * std  # ensure the standard deviation is `std`
        return partial(torch.nn.init.uniform_, a=-bound, b=bound)
    else:
        raise ValueError("Unsupported layer initialization method")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, ParallelPatternProvider.__init__
def __init__(self, n_q: int):
        super().__init__(n_q, [0] * n_q)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\_base_explorers.py
BlockTypes.METHOD, BaseExplorer.process_sheep
def process_sheep(self, sheep, history):
        train = {
            "epoch": len(history),
        }
        parts = {"train": train}
        for metrics in history:
            for key, sub in metrics.items():
                part = parts.get(key, {})
                if 'duration' in sub:
                    # Convert to minutes for readability.
                    sub['duration'] = sub['duration'] / 60.
                part.update(sub)
                parts[key] = part
        ping = get_sheep_ping(sheep)
        if ping is not None:
            for name in self.stages():
                if name not in parts:
                    parts[name] = {}
                # Add the ping to each part for convenience.
                parts[name]['ping'] = ping
        return parts
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, CachedBatchLoader.__iter__
def __iter__(self):
        """This will yields tuples, exactly as provided to the
        `CachedBatchWriter.save` method.
        """
        pool = ThreadPoolExecutor(self.num_workers)
        next_index = 0
        queue = deque()

        def _get_next():
            nonlocal next_index
            r = queue.popleft().result()
            if r is None:
                return None
            else:
                queue.append(pool.submit(self._load_one, next_index))
                next_index += 1
            return r

        with pool:
            # fill the buffer of fetching jobs.
            for _ in range(2 * self.num_workers):
                queue.append(pool.submit(self._load_one, next_index))
                next_index += 1
            while True:
                batch = _get_next()
                if batch is None:
                    return
                yield batch
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, FileCleaner.add
def add(self, path: tp.Union[str, Path]):
        self._cleanup()
        self.files.append((time.time(), Path(path)))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\common_utils\temp_utils.py
BlockTypes.METHOD, TempDirMixin.get_base_temp_dir
def get_base_temp_dir(cls):
        # If AUDIOCRAFT_TEST_DIR is set, use it instead of temporary directory.
        # this is handy for debugging.
        key = "AUDIOCRAFT_TEST_DIR"
        if key in os.environ:
            return os.environ[key]
        if cls.temp_dir_ is None:
            cls.temp_dir_ = tempfile.TemporaryDirectory()
        return cls.temp_dir_.name
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, main
def main():
    logging.basicConfig(stream=sys.stderr, level=logging.INFO)
    parser = argparse.ArgumentParser(
        prog='audio_dataset',
        description='Generate .jsonl files by scanning a folder.')
    parser.add_argument('root', help='Root folder with all the audio files')
    parser.add_argument('output_meta_file',
                        help='Output file to store the metadata, ')
    parser.add_argument('--complete',
                        action='store_false', dest='minimal', default=True,
                        help='Retrieve all metadata, even the one that are expansive '
                             'to compute (e.g. normalization).')
    parser.add_argument('--resolve',
                        action='store_true', default=False,
                        help='Resolve the paths to be absolute and with no symlinks.')
    parser.add_argument('--workers',
                        default=10, type=int,
                        help='Number of workers.')
    args = parser.parse_args()
    meta = find_audio_files(args.root, DEFAULT_EXTS, progress=True,
                            resolve=args.resolve, minimal=args.minimal, workers=args.workers)
    save_audio_meta(args.output_meta_file, meta)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, _match_unstable_samples
def _match_unstable_samples(samples_per_xp: tp.List[tp.Set[Sample]]) -> tp.Dict[str, tp.List[Sample]]:
    # For unstable ids, we use a sorted list since we'll match them in order
    unstable_samples_per_xp = [[
        sample for sample in sorted(samples, key=lambda x: x.id)
        if sample.prompt is None and not sample.conditioning
    ] for samples in samples_per_xp]
    # Trim samples per xp so all samples can have a match
    min_len = min([len(samples) for samples in unstable_samples_per_xp])
    unstable_samples_per_xp = [samples[:min_len] for samples in unstable_samples_per_xp]
    # Dictionary of index -> list of matched samples
    return {
        f'noinput_{i}': [samples[i] for samples in unstable_samples_per_xp] for i in range(min_len)
    }
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.sample_rate
def sample_rate(self) -> int:
        """Sample rate of the generated audio."""
        return self.compression_model.sample_rate
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, AttributeDropout.__init__
def __init__(self, p: tp.Dict[str, tp.Dict[str, float]], active_on_eval: bool = False, seed: int = 1234):
        super().__init__(seed=seed)
        self.active_on_eval = active_on_eval
        # construct dict that return the values from p otherwise 0
        self.p = {}
        for condition_type, probs in p.items():
            self.p[condition_type] = defaultdict(lambda: 0, probs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, NormConvTranspose2d.forward
def forward(self, x):
        x = self.convtr(x)
        x = self.norm(x)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_streaming_api
def test_streaming_api():
    tr = StreamingTransformer(16, 4, 2, causal=True, dropout=0.)
    tr.eval()
    steps = 12
    x = torch.randn(1, steps, 16)

    with torch.no_grad():
        with tr.streaming():
            _ = tr(x[:, :1])
            state = {k: v.clone() for k, v in tr.get_streaming_state().items()}
            y = tr(x[:, 1:2])
            tr.set_streaming_state(state)
            y2 = tr(x[:, 1:2])
            assert torch.allclose(y, y2), (y - y2).norm()
            assert tr.flush() is None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\export_legacy.py
BlockTypes.METHOD, _clean_lm_cfg
def _clean_lm_cfg(cfg: DictConfig):
    OmegaConf.set_struct(cfg, False)
    # This used to be set automatically in the LM solver, need a more robust solution
    # for the future.
    cfg['transformer_lm']['card'] = 2048
    cfg['transformer_lm']['n_q'] = 4
    # Experimental params no longer supported.
    bad_params = ['spectral_norm_attn_iters', 'spectral_norm_ff_iters',
                  'residual_balancer_attn', 'residual_balancer_ff', 'layer_drop']
    for name in bad_params:
        del cfg['transformer_lm'][name]
    OmegaConf.set_struct(cfg, True)
    return cfg
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingMultiheadAttention.forward
def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                key_padding_mask=None, need_weights=False, attn_mask=None,
                average_attn_weights=True, is_causal=False):
        assert attn_mask is None
        assert not is_causal, ("New param added in torch 2.0.1 not supported, "
                               "use the causal args in the constructor.")

        time_dim = _get_attention_time_dimension()
        if time_dim == 2:
            layout = "b h t d"
        else:
            layout = "b t h d"
        dtype = query.dtype
        if self._is_streaming:
            assert self.causal or self.cross_attention, \
                "Streaming only available for causal or cross attention"

        if self.causal:
            # At the moment we specialize only for the self-attention case.
            assert query.shape[1] == key.shape[1], "Causal only for same length query / key / value"
            assert value.shape[1] == key.shape[1], "Causal only for same length query / key / value"
            attn_mask = self._get_mask(query.shape[1], query.device, query.dtype)

        if self.custom:
            # custom implementation
            assert need_weights is False
            assert key_padding_mask is None
            if self.cross_attention:
                # Different queries, keys, values, we have to spit manually the weights
                # before applying the linear.
                dim = self.in_proj_weight.shape[0] // 3
                if self.in_proj_bias is None:
                    bias_q, bias_k, bias_v = None, None, None
                else:
                    bias_q = self.in_proj_bias[:dim]
                    bias_k = self.in_proj_bias[dim: 2 * dim]
                    bias_v = self.in_proj_bias[2 * dim:]
                q = nn.functional.linear(query, self.in_proj_weight[:dim], bias_q)
                # todo: when streaming, we could actually save k, v and check the shape actually match.
                k = nn.functional.linear(key, self.in_proj_weight[dim: 2 * dim], bias_k)
                v = nn.functional.linear(value, self.in_proj_weight[2 * dim:], bias_v)
                if self.qk_layer_norm is True:
                    q = self.q_layer_norm(q)
                    k = self.k_layer_norm(k)
                q, k, v = [rearrange(x, f"b t (h d) -> {layout}", h=self.num_heads) for x in [q, k, v]]
            else:
                if not _is_profiled():
                    # profiling breaks that propertysomehow.
                    assert query is key, "specialized implementation"
                    assert value is key, "specialized implementation"
                projected = nn.functional.linear(query, self.in_proj_weight, self.in_proj_bias)
                if self.kv_repeat == 1:
                    if time_dim == 2:
                        bound_layout = "b h p t d"
                    else:
                        bound_layout = "b t p h d"
                    packed = rearrange(projected, f"b t (p h d) -> {bound_layout}", p=3, h=self.num_heads)
                    q, k, v = ops.unbind(packed, dim=2)
                else:
                    embed_dim = self.embed_dim
                    per_head_dim = (embed_dim // self.num_heads)
                    kv_heads = self.num_heads // self.kv_repeat
                    q = projected[:, :, :embed_dim]
                    start = embed_dim
                    end = start + per_head_dim * kv_heads
                    k = projected[:, :, start: end]
                    v = projected[:, :, end:]
                    q = rearrange(q, f"b t (h d) -> {layout}", h=self.num_heads)
                    k = rearrange(k, f"b t (h d) -> {layout}", h=kv_heads)
                    v = rearrange(v, f"b t (h d) -> {layout}", h=kv_heads)

                if self.qk_layer_norm is True:
                    assert self.kv_repeat == 1
                    q, k = [rearrange(x, f"{layout} -> b t (h d)") for x in [q, k]]
                    q = self.q_layer_norm(q)
                    k = self.k_layer_norm(k)
                    q, k = [rearrange(x, f"b t (h d) -> {layout}", h=self.num_heads) for x in [q, k]]
                if self.rope:
                    q, k = self._apply_rope(q, k)
                k, v = self._complete_kv(k, v)
                if self.kv_repeat > 1:
                    k = expand_repeated_kv(k, self.kv_repeat)
                    v = expand_repeated_kv(v, self.kv_repeat)
            if self.attention_as_float32:
                q, k, v = [x.float() for x in [q, k, v]]
            if self.memory_efficient:
                p = self.dropout if self.training else 0
                if _efficient_attention_backend == 'torch':
                    x = torch.nn.functional.scaled_dot_product_attention(
                        q, k, v, is_causal=attn_mask is not None, dropout_p=p)
                else:
                    x = ops.memory_efficient_attention(q, k, v, attn_mask, p=p)
            else:
                # We include the dot product as float32, for consistency
                # with the other implementations that include that step
                # as part of the attention. Note that when using `autocast`,
                # the einsums would be done as bfloat16, but the softmax
                # would be done as bfloat16, so `attention_as_float32` will
                # extend a bit the range of operations done in float32,
                # although this should make no difference.
                q = q / q.shape[-1] ** 0.5
                key_layout = layout.replace('t', 'k')
                query_layout = layout
                if self._is_streaming and self.safe_streaming and q.device.type == 'cuda':
                    with torch.autocast(device_type=q.device.type, dtype=torch.float32):
                        pre_w = torch.einsum(f"{query_layout},{key_layout}-> b h t k", q, k)
                else:
                    pre_w = torch.einsum(f"{query_layout},{key_layout}-> b h t k", q, k)
                if attn_mask is not None:
                    pre_w = pre_w + attn_mask
                w = torch.softmax(pre_w, dim=-1)
                w = F.dropout(w, self.dropout, training=self.training).to(v)
                # Key and value have the same format.
                x = torch.einsum(f"b h t k, {key_layout} -> {layout}", w, v)
            x = x.to(dtype)
            x = rearrange(x, f"{layout} -> b t (h d)", h=self.num_heads)
            x = self.out_proj(x)
        else:
            key, value = self._complete_kv(key, value)
            if self.attention_as_float32:
                query, key, value = [x.float() for x in [query, key, value]]
            x, _ = self.mha(
                query, key, value, key_padding_mask,
                need_weights, attn_mask, average_attn_weights)
            x = x.to(dtype)

        return x, None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio.py
BlockTypes.METHOD, audio_read
def audio_read(filepath: tp.Union[str, Path], seek_time: float = 0.,
               duration: float = -1., pad: bool = False) -> tp.Tuple[torch.Tensor, int]:
    """Read audio by picking the most appropriate backend tool based on the audio format.

    Args:
        filepath (str or Path): Path to audio file to read.
        seek_time (float): Time at which to start reading in the file.
        duration (float): Duration to read from the file. If set to -1, the whole file is read.
        pad (bool): Pad output audio if not reaching expected duration.
    Returns:
        tuple of torch.Tensor, int: Tuple containing audio data and sample rate.
    """
    fp = Path(filepath)
    if fp.suffix in ['.flac', '.ogg']:  # TODO: check if we can safely use av_read for .ogg
        # There is some bug with ffmpeg and reading flac
        info = _soundfile_info(filepath)
        frames = -1 if duration <= 0 else int(duration * info.sample_rate)
        frame_offset = int(seek_time * info.sample_rate)
        wav, sr = soundfile.read(filepath, start=frame_offset, frames=frames, dtype=np.float32)
        assert info.sample_rate == sr, f"Mismatch of sample rates {info.sample_rate} {sr}"
        wav = torch.from_numpy(wav).t().contiguous()
        if len(wav.shape) == 1:
            wav = torch.unsqueeze(wav, 0)
    elif (
        fp.suffix in ['.wav', '.mp3'] and fp.suffix[1:] in ta.utils.sox_utils.list_read_formats()
        and duration <= 0 and seek_time == 0
    ):
        # Torchaudio is faster if we load an entire file at once.
        wav, sr = ta.load(fp)
    else:
        wav, sr = _av_read(filepath, seek_time, duration)
    if pad and duration > 0:
        expected_frames = int(duration * sr)
        wav = F.pad(wav, (0, expected_frames - wav.shape[-1]))
    return wav, sr
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, CompressionModel.forward
def forward(self, x: torch.Tensor) -> qt.QuantizedResult:
        ...
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\seanet.py
BlockTypes.METHOD, SEANetEncoder.__init__
def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 3,
                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},
                 norm: str = 'none', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,
                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,
                 pad_mode: str = 'reflect', true_skip: bool = True, compress: int = 2, lstm: int = 0,
                 disable_norm_outer_blocks: int = 0):
        super().__init__()
        self.channels = channels
        self.dimension = dimension
        self.n_filters = n_filters
        self.ratios = list(reversed(ratios))
        del ratios
        self.n_residual_layers = n_residual_layers
        self.hop_length = np.prod(self.ratios)
        self.n_blocks = len(self.ratios) + 2  # first and last conv + residual blocks
        self.disable_norm_outer_blocks = disable_norm_outer_blocks
        assert self.disable_norm_outer_blocks >= 0 and self.disable_norm_outer_blocks <= self.n_blocks, \
            "Number of blocks for which to disable norm is invalid." \
            "It should be lower or equal to the actual number of blocks in the network and greater or equal to 0."

        act = getattr(nn, activation)
        mult = 1
        model: tp.List[nn.Module] = [
            StreamableConv1d(channels, mult * n_filters, kernel_size,
                             norm='none' if self.disable_norm_outer_blocks >= 1 else norm,
                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)
        ]
        # Downsample to raw audio scale
        for i, ratio in enumerate(self.ratios):
            block_norm = 'none' if self.disable_norm_outer_blocks >= i + 2 else norm
            # Add residual layers
            for j in range(n_residual_layers):
                model += [
                    SEANetResnetBlock(mult * n_filters, kernel_sizes=[residual_kernel_size, 1],
                                      dilations=[dilation_base ** j, 1],
                                      norm=block_norm, norm_params=norm_params,
                                      activation=activation, activation_params=activation_params,
                                      causal=causal, pad_mode=pad_mode, compress=compress, true_skip=true_skip)]

            # Add downsampling layers
            model += [
                act(**activation_params),
                StreamableConv1d(mult * n_filters, mult * n_filters * 2,
                                 kernel_size=ratio * 2, stride=ratio,
                                 norm=block_norm, norm_kwargs=norm_params,
                                 causal=causal, pad_mode=pad_mode),
            ]
            mult *= 2

        if lstm:
            model += [StreamableLSTM(mult * n_filters, num_layers=lstm)]

        model += [
            act(**activation_params),
            StreamableConv1d(mult * n_filters, dimension, last_kernel_size,
                             norm='none' if self.disable_norm_outer_blocks == self.n_blocks else norm,
                             norm_kwargs=norm_params, causal=causal, pad_mode=pad_mode)
        ]

        self.model = nn.Sequential(*model)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, create_sin_embedding
def create_sin_embedding(positions: torch.Tensor, dim: int, max_period: float = 10000,
                         dtype: torch.dtype = torch.float32) -> torch.Tensor:
    """Create sinusoidal positional embedding, with shape `[B, T, C]`.

    Args:
        positions (torch.Tensor): LongTensor of positions.
        dim (int): Dimension of the embedding.
        max_period (float): Maximum period of the cosine/sine functions.
        dtype (torch.dtype or str): dtype to use to generate the embedding.
    Returns:
        torch.Tensor: Sinusoidal positional embedding.
    """
    # We aim for BTC format
    assert dim % 2 == 0
    half_dim = dim // 2
    positions = positions.to(dtype)
    adim = torch.arange(half_dim, device=positions.device, dtype=dtype).view(1, 1, -1)
    max_period_tensor = torch.full([], max_period, device=positions.device, dtype=dtype)  # avoid sync point
    phase = positions / (max_period_tensor ** (adim / (half_dim - 1)))
    return torch.cat([torch.cos(phase), torch.sin(phase)], dim=-1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingTransformerLayer.__init__
def __init__(self, d_model: int, num_heads: int, dim_feedforward: int = 2048, dropout: float = 0.1,
                 bias_ff: bool = True, bias_attn: bool = True, causal: bool = False,
                 past_context: tp.Optional[int] = None, custom: bool = False,
                 memory_efficient: bool = False, attention_as_float32: bool = False,
                 qk_layer_norm: bool = False, qk_layer_norm_cross: bool = False,
                 cross_attention: bool = False, layer_scale: tp.Optional[float] = None,
                 rope: tp.Optional[RotaryEmbedding] = None, attention_dropout: tp.Optional[float] = None,
                 kv_repeat: int = 1, norm: str = 'layer_norm', device=None, dtype=None, **kwargs):
        super().__init__(d_model, num_heads, dim_feedforward, dropout,
                         device=device, dtype=dtype, batch_first=True, **kwargs)
        factory_kwargs = {'device': device, 'dtype': dtype}
        # Redefine self_attn to our streaming multi-head attention
        attn_kwargs: tp.Dict[str, tp.Any] = {
            'embed_dim': d_model,
            'num_heads': num_heads,
            'dropout': dropout if attention_dropout is None else attention_dropout,
            'bias': bias_attn,
            'custom': custom,
            'memory_efficient': memory_efficient,
            'attention_as_float32': attention_as_float32,
        }
        self.self_attn: StreamingMultiheadAttention = StreamingMultiheadAttention(
            causal=causal, past_context=past_context, rope=rope, qk_layer_norm=qk_layer_norm,
            kv_repeat=kv_repeat, **attn_kwargs, **factory_kwargs)  # type: ignore
        # Redefine feedforward layers to expose bias parameter
        self.linear1 = nn.Linear(d_model, dim_feedforward, bias=bias_ff, **factory_kwargs)
        self.linear2 = nn.Linear(dim_feedforward, d_model, bias=bias_ff, **factory_kwargs)

        self.layer_scale_1: nn.Module
        self.layer_scale_2: nn.Module
        if layer_scale is None:
            self.layer_scale_1 = nn.Identity()
            self.layer_scale_2 = nn.Identity()
        else:
            self.layer_scale_1 = LayerScale(d_model, layer_scale, **factory_kwargs)
            self.layer_scale_2 = LayerScale(d_model, layer_scale, **factory_kwargs)

        self.cross_attention: tp.Optional[nn.Module] = None
        if cross_attention:
            self.cross_attention = StreamingMultiheadAttention(
                cross_attention=True, qk_layer_norm=qk_layer_norm_cross,
                **attn_kwargs, **factory_kwargs)
            # Norm and dropout
            self.dropout_cross = nn.Dropout(dropout)
            # eps value matching that used in PyTorch reference implementation.
            self.norm_cross = nn.LayerNorm(d_model, eps=1e-5, **factory_kwargs)
            self.layer_scale_cross: nn.Module
            if layer_scale is None:
                self.layer_scale_cross = nn.Identity()
            else:
                self.layer_scale_cross = LayerScale(d_model, layer_scale, **factory_kwargs)
        self.norm1 = create_norm_fn(norm, d_model, **factory_kwargs)  # type: ignore
        self.norm2 = create_norm_fn(norm, d_model, **factory_kwargs)  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\fsdp.py
BlockTypes.METHOD, _FSDPFixStateDict.state_dict
def state_dict(self) -> tp.Dict[str, tp.Any]:  # type: ignore
        state = dict(super().state_dict())
        for key, value in list(state.items()):
            if is_sharded_tensor(value):
                del state[key]
        return state
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\deadlock.py
BlockTypes.METHOD, DeadlockDetect.__enter__
def __enter__(self):
        if self.use:
            self._thread = threading.Thread(target=self._detector_thread)
            self._thread.start()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\train.py
BlockTypes.METHOD, get_solver
def get_solver(cfg):
    from . import solvers
    # Convert batch size to batch size for each GPU
    assert cfg.dataset.batch_size % flashy.distrib.world_size() == 0
    cfg.dataset.batch_size //= flashy.distrib.world_size()
    for split in ['train', 'valid', 'evaluate', 'generate']:
        if hasattr(cfg.dataset, split) and hasattr(cfg.dataset[split], 'batch_size'):
            assert cfg.dataset[split].batch_size % flashy.distrib.world_size() == 0
            cfg.dataset[split].batch_size //= flashy.distrib.world_size()
    resolve_config_dset_paths(cfg)
    solver = solvers.get_solver(cfg)
    return solver
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_seanet.py
BlockTypes.METHOD, TestSEANetModel.test_decoder_disable_norm
def test_decoder_disable_norm(self):
        n_residuals = [0, 1, 3]
        disable_blocks = [0, 1, 2, 3, 4, 5, 6]
        norms = ['weight_norm', 'none']
        for n_res, disable_blocks, norm in product(n_residuals, disable_blocks, norms):
            decoder = SEANetDecoder(n_residual_layers=n_res, norm=norm,
                                    disable_norm_outer_blocks=disable_blocks)
            self._check_decoder_blocks_norm(decoder, disable_blocks, norm)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_transformer_causal_streaming
def test_transformer_causal_streaming():
    torch.manual_seed(1234)

    for context, custom in product([None, 10], [False, True]):
        # Test that causality and receptive fields are properly handled.
        # looking at the gradients
        tr = StreamingTransformer(
            16, 4, 1 if context else 2,
            causal=True, past_context=context, custom=custom,
            dropout=0.)
        steps = 20
        for k in [0, 10, 15, 19]:
            x = torch.randn(4, steps, 16, requires_grad=True)
            y = tr(x)
            y[:, k].abs().sum().backward()
            if k + 1 < steps:
                assert torch.allclose(x.grad[:, k + 1:], torch.tensor(0.)), x.grad[:, k + 1:].norm()
            assert not torch.allclose(x.grad[:, :k + 1], torch.tensor(0.)), x.grad[:, :k + 1].norm()
            if context is not None and k > context:
                limit = k - context - 1
                assert torch.allclose(x.grad[:, :limit],
                                      torch.tensor(0.)), x.grad[:, :limit].norm()

        # Now check that streaming gives the same result at batch eval.
        x = torch.randn(4, steps, 16)
        y = tr(x)
        ys = []
        with tr.streaming():
            for k in range(steps):
                chunk = x[:, k:k + 1, :]
                ys.append(tr(chunk))
        y_stream = torch.cat(ys, dim=1)
        delta = torch.norm(y_stream - y) / torch.norm(y)
        assert delta < 1e-6, delta
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.get_slurm_exclude
def get_slurm_exclude(cls) -> tp.Optional[str]:
        """Get the list of nodes to exclude for that cluster."""
        cluster_config = cls.instance()._get_cluster_config()
        return cluster_config.get("slurm_exclude")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, T5Conditioner.tokenize
def tokenize(self, x: tp.List[tp.Optional[str]]) -> tp.Dict[str, torch.Tensor]:
        # if current sample doesn't have a certain attribute, replace with empty string
        entries: tp.List[str] = [xi if xi is not None else "" for xi in x]
        if self.normalize_text:
            _, _, entries = self.text_normalizer(entries, return_text=True)
        if self.word_dropout > 0. and self.training:
            new_entries = []
            for entry in entries:
                words = [word for word in entry.split(" ") if random.random() >= self.word_dropout]
                new_entries.append(" ".join(words))
            entries = new_entries

        empty_idx = torch.LongTensor([i for i, xi in enumerate(entries) if xi == ""])

        inputs = self.t5_tokenizer(entries, return_tensors='pt', padding=True).to(self.device)
        mask = inputs['attention_mask']
        mask[empty_idx, :] = 0  # zero-out index where the input is non-existant
        return inputs
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_audiogen.py
BlockTypes.METHOD, TestAudioGenModel.get_audiogen
def get_audiogen(self):
        ag = AudioGen.get_pretrained(name='debug', device='cpu')
        ag.set_generation_params(duration=2.0, extend_stride=2.)
        return ag
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, AdversarialLoss.__init__
def __init__(self,
                 adversary: nn.Module,
                 optimizer: torch.optim.Optimizer,
                 loss: AdvLossType,
                 loss_real: AdvLossType,
                 loss_fake: AdvLossType,
                 loss_feat: tp.Optional[FeatLossType] = None,
                 normalize: bool = True):
        super().__init__()
        self.adversary: nn.Module = adversary
        flashy.distrib.broadcast_model(self.adversary)
        self.optimizer = optimizer
        self.loss = loss
        self.loss_real = loss_real
        self.loss_fake = loss_fake
        self.loss_feat = loss_feat
        self.normalize = normalize
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\profiler.py
BlockTypes.METHOD, Profiler.__exit__
def __exit__(self, exc_type, exc_value, exc_tb):
        if self.profiler is not None:
            return self.profiler.__exit__(exc_type, exc_value, exc_tb)  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\compression\encodec_musicgen_32khz.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=8, partition=partitions)
    # use configuration for MusicGen's EnCodec model trained on monophonic audio sampled at 32 kHz
    # MusicGen's EnCodec is trained with a total stride of 640 leading to a frame rate of 50 hz
    launcher.bind_(solver='compression/encodec_musicgen_32khz')
    # replace this by the desired music dataset
    launcher.bind_(dset='internal/music_400k_32khz')
    # launch xp
    launcher()
    launcher({
        'metrics.visqol.bin': '/data/home/jadecopet/local/usr/opt/visqol',
        'label': 'visqol',
        'evaluate.metrics.visqol': True
    })
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, orthogonal_loss_fn
def orthogonal_loss_fn(t):
    # eq (2) from https://arxiv.org/abs/2112.00384
    n = t.shape[0]
    normed_codes = l2norm(t)
    identity = torch.eye(n, device=t.device)
    cosine_sim = einsum("i d, j d -> i j", normed_codes, normed_codes)
    return ((cosine_sim - identity) ** 2).sum() / (n ** 2)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, FileCleaner._cleanup
def _cleanup(self):
        now = time.time()
        for time_added, path in list(self.files):
            if now - time_added > self.file_lifetime:
                if path.exists():
                    path.unlink()
                self.files.pop(0)
            else:
                break
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset.sample_file
def sample_file(self, index: int, rng: torch.Generator) -> AudioMeta:
        """Sample a given file from `self.meta`. Can be overridden in subclasses.
        This is only called if `segment_duration` is not None.

        You must use the provided random number generator `rng` for reproducibility.
        You can further make use of the index accessed.
        """
        if self.permutation_on_files:
            assert self.current_epoch is not None
            total_index = self.current_epoch * len(self) + index
            permutation_index = total_index // len(self.meta)
            relative_index = total_index % len(self.meta)
            permutation = AudioDataset._get_file_permutation(
                len(self.meta), permutation_index, self.shuffle_seed)
            file_index = permutation[relative_index]
            return self.meta[file_index]

        if not self.sample_on_weight and not self.sample_on_duration:
            file_index = int(torch.randint(len(self.sampling_probabilities), (1,), generator=rng).item())
        else:
            file_index = int(torch.multinomial(self.sampling_probabilities, 1, generator=rng).item())

        return self.meta[file_index]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_utils.py
BlockTypes.METHOD, TestNormalizeAudio.test_normalize_audio_peak
def test_normalize_audio_peak(self):
        b, c, dur = 2, 1, 4.
        sr = 3
        audio = 10.0 * get_batch_white_noise(b, c, int(sr * dur))
        norm_audio = normalize_audio(audio, strategy='peak')
        assert norm_audio.abs().max() <= 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.audio_channels
def audio_channels(self) -> int:
        """Audio channels of the generated audio."""
        return self.compression_model.channels
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, UnrolledPatternProvider.__init__
def __init__(self, n_q: int, flattening: tp.Optional[tp.List[int]] = None,
                 delays: tp.Optional[tp.List[int]] = None):
        super().__init__(n_q)
        if flattening is None:
            flattening = list(range(n_q))
        if delays is None:
            delays = [0] * n_q
        assert len(flattening) == n_q
        assert len(delays) == n_q
        assert sorted(flattening) == flattening
        assert sorted(delays) == delays
        self._flattened_codebooks = self._build_flattened_codebooks(delays, flattening)
        self.max_delay = max(delays)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\polynomial_decay_lr_scheduler.py
BlockTypes.METHOD, PolynomialDecayLRScheduler.__init__
def __init__(self, optimizer: Optimizer, warmup_steps: int, total_steps: int,
                 end_lr: float = 0., zero_lr_warmup_steps: int = 0, power: float = 1.):
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.end_lr = end_lr
        self.zero_lr_warmup_steps = zero_lr_warmup_steps
        self.power = power
        super().__init__(optimizer)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, FeatureMatchingLoss.__init__
def __init__(self, loss: nn.Module = torch.nn.L1Loss(), normalize: bool = True):
        super().__init__()
        self.loss = loss
        self.normalize = normalize
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.__init__
def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__()
        self.logger.info(f"Instantiating solver {self.__class__.__name__} for XP {self.xp.sig}")
        self.logger.info(f"All XP logs are stored in {self.xp.folder}")
        self.cfg = cfg
        self.device = cfg.device
        self.model: nn.Module
        self._continue_best_source_keys = ['best_state', 'fsdp_best_state']
        self._fsdp_modules: tp.List[fsdp.FSDP] = []
        self._ema_sources: nn.ModuleDict = nn.ModuleDict()
        self.ema: tp.Optional[optim.ModuleDictEMA] = None
        self.dataloaders: tp.Dict[str, torch.utils.data.DataLoader] = dict()
        self._log_updates = self.cfg.logging.get('log_updates', 10)
        if self.cfg.logging.log_tensorboard:
            self.init_tensorboard(**self.cfg.get('tensorboard'))
        if self.cfg.logging.log_wandb and self:
            self.init_wandb(**self.cfg.get('wandb'))
        # keep a copy of the best performing state for stateful objects
        # used for evaluation and generation stages
        dtype_best: tp.Optional[torch.dtype] = None
        if self.cfg.fsdp.use:
            dtype_best = getattr(torch, self.cfg.fsdp.param_dtype)  # type: ignore
            assert isinstance(dtype_best, torch.dtype)
        elif self.cfg.autocast:
            dtype_best = getattr(torch, self.cfg.autocast_dtype)  # type: ignore
            assert isinstance(dtype_best, torch.dtype)
        self.best_state: BestStateDictManager = BestStateDictManager(dtype=dtype_best)
        # Hacky support for keeping a copy of the full best state in rank0.
        self.fsdp_best_state: tp.Dict[str, tp.Any] = {}
        self.register_stateful('best_state', 'fsdp_best_state')  # register best_state object to keep it in state_dict
        self._new_best_state: bool = False  # should save a new checkpoint
        # instantiate datasets and appropriate number of updates per epoch
        self.build_dataloaders()
        if self.cfg.execute_only is None:
            assert 'train' in self.dataloaders, "The train dataset split must be provided."
            assert 'valid' in self.dataloaders, "The valid dataset split must be provided."
        self.train_updates_per_epoch = len(self.dataloaders['train']) if 'train' in self.dataloaders else 0
        if self.cfg.optim.updates_per_epoch:
            self.train_updates_per_epoch = self.cfg.optim.updates_per_epoch
        self.total_updates = self.train_updates_per_epoch * self.cfg.optim.epochs
        # instantiate model & exponential moving average on the model
        self.build_model()
        self.logger.info("Model hash: %s", model_hash(self.model))
        assert 'model' in self.stateful.sources, \
            "Please register the model to stateful with self.register_stateful('model') in build_model."
        self.profiler = Profiler(self.model, **self.cfg.profiler)
        self.initialize_ema()
        self.register_stateful('ema')
        assert self.ema is None or 'ema' in self.stateful.sources, \
            "Please register the ema to stateful with self.register_stateful('ema') in build_model."
        self.deadlock_detect = DeadlockDetect(**self.cfg.deadlock)
        # basic statistics on the trained model
        model_size = sum(p.numel() for p in self.model.parameters() if p.requires_grad) / 1e6
        # one copy of grad, one copy of momentum, one copy of denominator and model weights.
        # and 4 bytes for each float!
        mem_usage = model_size * 4 * 4 / 1000
        self.logger.info("Model size: %.2f M params", model_size)
        self.logger.info("Base memory usage, with model, grad and optim: %.2f GB", mem_usage)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\linear_warmup_lr_scheduler.py
BlockTypes.METHOD, LinearWarmupLRScheduler.__init__
def __init__(self, optimizer: Optimizer, warmup_steps: int, warmup_init_lr: tp.Optional[float] = 0):
        self.warmup_steps = warmup_steps
        self.warmup_init_lr = warmup_init_lr
        super().__init__(optimizer)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\compression\debug.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=2, partition=partitions)
    launcher.bind_(solver='compression/debug')

    with launcher.job_array():
        # base debug task using config from solver=compression/debug
        launcher()
        # we can override parameters in the grid to launch additional xps
        launcher({'rvq.bins': 2048, 'rvq.n_q': 4})
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingTransformer.__init__
def __init__(self, d_model: int, num_heads: int, num_layers: int, dim_feedforward: int = 2048,
                 dropout: float = 0.1, bias_ff: bool = True, bias_attn: bool = True,
                 causal: bool = False, past_context: tp.Optional[int] = None,
                 custom: bool = False, memory_efficient: bool = False, attention_as_float32: bool = False,
                 cross_attention: bool = False, layer_scale: tp.Optional[float] = None,
                 positional_embedding: str = 'sin', max_period: float = 10_000, positional_scale: float = 1.,
                 xpos: bool = False, lr: tp.Optional[float] = None, weight_decay: tp.Optional[float] = None,
                 layer_class: tp.Type[StreamingTransformerLayer] = StreamingTransformerLayer,
                 checkpointing: str = 'none', device=None, dtype=None, **kwargs):
        super().__init__()
        assert d_model % num_heads == 0

        self.positional_embedding = positional_embedding
        self.max_period = max_period
        self.positional_scale = positional_scale
        self.weight_decay = weight_decay
        self.lr = lr

        assert positional_embedding in ['sin', 'rope', 'sin_rope']
        self.rope: tp.Optional[RotaryEmbedding] = None
        if self.positional_embedding in ['rope', 'sin_rope']:
            assert _is_custom(custom, memory_efficient)
            self.rope = RotaryEmbedding(d_model // num_heads, max_period=max_period,
                                        xpos=xpos, scale=positional_scale, device=device)

        self.checkpointing = checkpointing

        assert checkpointing in ['none', 'torch', 'xformers_default', 'xformers_mm']
        if self.checkpointing.startswith('xformers'):
            _verify_xformers_internal_compat()

        self.layers = nn.ModuleList()
        for idx in range(num_layers):
            self.layers.append(
                layer_class(
                    d_model=d_model, num_heads=num_heads, dim_feedforward=dim_feedforward,
                    dropout=dropout, bias_ff=bias_ff, bias_attn=bias_attn,
                    causal=causal, past_context=past_context, custom=custom,
                    memory_efficient=memory_efficient, attention_as_float32=attention_as_float32,
                    cross_attention=cross_attention, layer_scale=layer_scale, rope=self.rope,
                    device=device, dtype=dtype, **kwargs))

        if self.checkpointing != 'none':
            for layer in self.layers:
                # see audiocraft/optim/fsdp.py, magic signal to indicate this requires fixing the
                # backward hook inside of FSDP...
                layer._magma_checkpointed = True  # type: ignore
                assert layer.layer_drop == 0., "Need further checking"  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_codebooks_patterns.py
BlockTypes.METHOD, TestPattern._get_pattern_providers
def _get_pattern_providers(self, n_q: int):
        pattern_provider_1 = ParallelPatternProvider(n_q)
        pattern_provider_2 = DelayedPatternProvider(n_q, list(range(n_q)))
        pattern_provider_3 = DelayedPatternProvider(n_q, [0] + [1] * (n_q - 1))
        pattern_provider_4 = UnrolledPatternProvider(
            n_q, flattening=list(range(n_q)), delays=[0] * n_q
        )
        pattern_provider_5 = UnrolledPatternProvider(
            n_q, flattening=[0] + [1] * (n_q - 1), delays=[0] * n_q
        )
        pattern_provider_6 = UnrolledPatternProvider(
            n_q, flattening=[0] + [1] * (n_q - 1), delays=[0] + [5] * (n_q - 1)
        )
        return [
            pattern_provider_1,
            pattern_provider_2,
            pattern_provider_3,
            pattern_provider_4,
            pattern_provider_5,
            pattern_provider_6,
        ]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, test_get_extra_padding_for_conv1d
def test_get_extra_padding_for_conv1d():
    # TODO: Implement me!
    pass
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\chroma_cosinesim.py
BlockTypes.METHOD, ChromaCosineSimilarityMetric.__init__
def __init__(self, sample_rate: int, n_chroma: int, radix2_exp: int, argmax: bool, eps: float = 1e-8):
        super().__init__()
        self.chroma_sample_rate = sample_rate
        self.n_chroma = n_chroma
        self.eps = eps
        self.chroma_extractor = ChromaExtractor(sample_rate=self.chroma_sample_rate, n_chroma=self.n_chroma,
                                                radix2_exp=radix2_exp, argmax=argmax)
        self.add_state("cosine_sum", default=torch.tensor(0.), dist_reduce_fx="sum")
        self.add_state("weight", default=torch.tensor(0.), dist_reduce_fx="sum")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen._generate_tokens
def _generate_tokens(self, attributes: tp.List[ConditioningAttributes],
                         prompt_tokens: tp.Optional[torch.Tensor], progress: bool = False) -> torch.Tensor:
        """Generate discrete audio tokens given audio prompt and/or conditions.

        Args:
            attributes (list of ConditioningAttributes): Conditions used for generation (text/melody).
            prompt_tokens (torch.Tensor, optional): Audio prompt used for continuation.
            progress (bool, optional): Flag to display progress of the generation process. Defaults to False.
        Returns:
            torch.Tensor: Generated audio, of shape [B, C, T], T is defined by the generation params.
        """
        total_gen_len = int(self.duration * self.frame_rate)
        max_prompt_len = int(min(self.duration, self.max_duration) * self.frame_rate)
        current_gen_offset: int = 0

        def _progress_callback(generated_tokens: int, tokens_to_generate: int):
            generated_tokens += current_gen_offset
            if self._progress_callback is not None:
                # Note that total_gen_len might be quite wrong depending on the
                # codebook pattern used, but with delay it is almost accurate.
                self._progress_callback(generated_tokens, total_gen_len)
            else:
                print(f'{generated_tokens: 6d} / {total_gen_len: 6d}', end='\r')

        if prompt_tokens is not None:
            assert max_prompt_len >= prompt_tokens.shape[-1], \
                "Prompt is longer than audio to generate"

        callback = None
        if progress:
            callback = _progress_callback

        if self.duration <= self.max_duration:
            # generate by sampling from LM, simple case.
            with self.autocast:
                gen_tokens = self.lm.generate(
                    prompt_tokens, attributes,
                    callback=callback, max_gen_len=total_gen_len, **self.generation_params)

        else:
            # now this gets a bit messier, we need to handle prompts,
            # melody conditioning etc.
            ref_wavs = [attr.wav['self_wav'] for attr in attributes]
            all_tokens = []
            if prompt_tokens is None:
                prompt_length = 0
            else:
                all_tokens.append(prompt_tokens)
                prompt_length = prompt_tokens.shape[-1]

            stride_tokens = int(self.frame_rate * self.extend_stride)

            while current_gen_offset + prompt_length < total_gen_len:
                time_offset = current_gen_offset / self.frame_rate
                chunk_duration = min(self.duration - time_offset, self.max_duration)
                max_gen_len = int(chunk_duration * self.frame_rate)
                for attr, ref_wav in zip(attributes, ref_wavs):
                    wav_length = ref_wav.length.item()
                    if wav_length == 0:
                        continue
                    # We will extend the wav periodically if it not long enough.
                    # we have to do it here rather than in conditioners.py as otherwise
                    # we wouldn't have the full wav.
                    initial_position = int(time_offset * self.sample_rate)
                    wav_target_length = int(self.max_duration * self.sample_rate)
                    positions = torch.arange(initial_position,
                                             initial_position + wav_target_length, device=self.device)
                    attr.wav['self_wav'] = WavCondition(
                        ref_wav[0][..., positions % wav_length],
                        torch.full_like(ref_wav[1], wav_target_length),
                        [self.sample_rate] * ref_wav[0].size(0),
                        [None], [0.])
                with self.autocast:
                    gen_tokens = self.lm.generate(
                        prompt_tokens, attributes,
                        callback=callback, max_gen_len=max_gen_len, **self.generation_params)
                if prompt_tokens is None:
                    all_tokens.append(gen_tokens)
                else:
                    all_tokens.append(gen_tokens[:, :, prompt_tokens.shape[-1]:])
                prompt_tokens = gen_tokens[:, :, stride_tokens:]
                prompt_length = prompt_tokens.shape[-1]
                current_gen_offset += stride_tokens

            gen_tokens = torch.cat(all_tokens, dim=-1)
        return gen_tokens
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_streaming_memory_efficient
def test_streaming_memory_efficient():
    for backend in ['torch', 'xformers']:
        torch.manual_seed(1234)
        set_efficient_attention_backend(backend)
        tr = StreamingTransformer(16, 4, 2, causal=True, dropout=0., custom=True)
        tr_mem_efficient = StreamingTransformer(
            16, 4, 2, dropout=0., memory_efficient=True, causal=True)
        tr.load_state_dict(tr_mem_efficient.state_dict())
        tr.eval()
        tr_mem_efficient.eval()
        steps = 12
        x = torch.randn(3, steps, 16)

        ref = tr(x)

        with tr_mem_efficient.streaming():
            outs = []
            # frame_sizes = [2] + [1] * (steps - 2)
            frame_sizes = [1] * steps

            for frame_size in frame_sizes:
                frame = x[:, :frame_size]
                x = x[:, frame_size:]
                outs.append(tr_mem_efficient(frame))

        out = torch.cat(outs, dim=1)
        delta = torch.norm(out - ref) / torch.norm(out)
        assert delta < 1e-6, delta
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\loaders.py
BlockTypes.METHOD, load_compression_model_ckpt
def load_compression_model_ckpt(file_or_url_or_id: tp.Union[Path, str], cache_dir: tp.Optional[str] = None):
    return _get_state_dict(file_or_url_or_id, filename="compression_state_dict.bin", cache_dir=cache_dir)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\inverse_sqrt_lr_scheduler.py
BlockTypes.METHOD, InverseSquareRootLRScheduler.get_lr
def get_lr(self):
        return [self._get_sched_lr(base_lr, self._step_count) for base_lr in self.base_lrs]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, evaluate_audio_reconstruction
def evaluate_audio_reconstruction(y_pred: torch.Tensor, y: torch.Tensor, cfg: omegaconf.DictConfig) -> dict:
    """Audio reconstruction evaluation method that can be conveniently pickled."""
    metrics = {}
    if cfg.evaluate.metrics.visqol:
        visqol = builders.get_visqol(cfg.metrics.visqol)
        metrics['visqol'] = visqol(y_pred, y, cfg.sample_rate)
    sisnr = builders.get_loss('sisnr', cfg)
    metrics['sisnr'] = sisnr(y_pred, y)
    return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\mos.py
BlockTypes.METHOD, audio
def audio(path: str):
    full_path = Path('/') / path
    assert full_path.suffix in [".mp3", ".wav"]
    return full_path.read_bytes(), {'Content-Type': 'audio/mpeg'}
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\common_utils\temp_utils.py
BlockTypes.METHOD, TempDirMixin.id
def id(self):
        return self.__class__.__name__
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\lstm.py
BlockTypes.METHOD, StreamableLSTM.__init__
def __init__(self, dimension: int, num_layers: int = 2, skip: bool = True):
        super().__init__()
        self.skip = skip
        self.lstm = nn.LSTM(dimension, dimension, num_layers)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.generate
def generate(self):
        """Generate stage."""
        sample_manager = SampleManager(self.xp)
        self.model.eval()
        generate_stage_name = f'{self.current_stage}'

        loader = self.dataloaders['generate']
        updates = len(loader)
        lp = self.log_progress(generate_stage_name, loader, total=updates, updates=self.log_updates)

        for batch in lp:
            reference, _ = batch
            reference = reference.to(self.device)
            estimate = self.regenerate(reference)
            reference = reference.cpu()
            estimate = estimate.cpu()
            sample_manager.add_samples(estimate, self.epoch, ground_truth_wavs=reference)
        flashy.distrib.barrier()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, test_pad1d_zeros
def test_pad1d_zeros():
    x = torch.randn(1, 1, 20)

    xp1 = pad1d(x, (0, 5), mode='constant', value=0.)
    assert xp1.shape[-1] == 25
    xp2 = pad1d(x, (5, 5), mode='constant', value=0.)
    assert xp2.shape[-1] == 30
    xp3 = pad1d(x, (0, 0), mode='constant', value=0.)
    assert xp3.shape[-1] == 20
    xp4 = pad1d(x, (10, 30), mode='constant', value=0.)
    assert xp4.shape[-1] == 60

    with pytest.raises(AssertionError):
        pad1d(x, (-1, 0), mode='constant', value=0.)

    with pytest.raises(AssertionError):
        pad1d(x, (0, -1), mode='constant', value=0.)

    with pytest.raises(AssertionError):
        pad1d(x, (-1, -1), mode='constant', value=0.)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, unpad1d
def unpad1d(x: torch.Tensor, paddings: tp.Tuple[int, int]):
    """Remove padding from x, handling properly zero padding. Only for 1d!"""
    padding_left, padding_right = paddings
    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)
    assert (padding_left + padding_right) <= x.shape[-1]
    end = x.shape[-1] - padding_right
    return x[..., padding_left: end]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestAvRead.test_avread_seek_base
def test_avread_seek_base(self):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 2.
        for sample_rate, ch in product(sample_rates, channels):
            n_frames = int(sample_rate * duration)
            wav = get_white_noise(ch, n_frames)
            path = self.get_temp_path(f'reference_a_{sample_rate}_{ch}.wav')
            save_wav(path, wav, sample_rate)
            for _ in range(100):
                # seek will always load a full duration segment in the file
                seek_time = random.uniform(0.0, 1.0)
                seek_duration = random.uniform(0.001, 1.0)
                read_wav, read_sr = _av_read(path, seek_time, seek_duration)
                assert read_sr == sample_rate
                assert read_wav.shape[0] == wav.shape[0]
                assert read_wav.shape[-1] == int(seek_duration * sample_rate)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.get_slurm_partitions
def get_slurm_partitions(cls, partition_types: tp.Optional[tp.List[str]] = None) -> str:
        """Gets the requested partitions for the current team and cluster as a comma-separated string.

        Args:
            partition_types (list[str], optional): partition types to retrieve. Values must be
                from ['global', 'team']. If not provided, the global partition is returned.
        """
        if not partition_types:
            partition_types = ["global"]

        cluster_config = cls.instance()._get_cluster_config()
        partitions = [
            cluster_config["partitions"][partition_type]
            for partition_type in partition_types
        ]
        return ",".join(partitions)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, SpectralConvergenceLoss.__init__
def __init__(self, epsilon: float = torch.finfo(torch.float32).eps):
        super().__init__()
        self.epsilon = epsilon
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.get_pretrained
def get_pretrained(name: str = 'facebook/musicgen-melody', device=None):
        """Return pretrained model, we provide four models:
        - facebook/musicgen-small (300M), text to music,
          # see: https://huggingface.co/facebook/musicgen-small
        - facebook/musicgen-medium (1.5B), text to music,
          # see: https://huggingface.co/facebook/musicgen-medium
        - facebook/musicgen-melody (1.5B) text to music and text+melody to music,
          # see: https://huggingface.co/facebook/musicgen-melody
        - facebook/musicgen-large (3.3B), text to music,
          # see: https://huggingface.co/facebook/musicgen-large
        """
        if device is None:
            if torch.cuda.device_count():
                device = 'cuda'
            else:
                device = 'cpu'

        if name == 'debug':
            # used only for unit tests
            compression_model = get_debug_compression_model(device)
            lm = get_debug_lm_model(device)
            return MusicGen(name, compression_model, lm, max_duration=30)

        if name in _HF_MODEL_CHECKPOINTS_MAP:
            warnings.warn(
                "MusicGen pretrained model relying on deprecated checkpoint mapping. " +
                f"Please use full pre-trained id instead: facebook/musicgen-{name}")
            name = _HF_MODEL_CHECKPOINTS_MAP[name]

        lm = load_lm_model(name, device=device)
        compression_model = load_compression_model(name, device=device)
        if 'self_wav' in lm.condition_provider.conditioners:
            lm.condition_provider.conditioners['self_wav'].match_len_on_eval = True

        return MusicGen(name, compression_model, lm)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\common_utils\temp_utils.py
BlockTypes.METHOD, TempDirMixin.get_temp_path
def get_temp_path(self, *paths):
        temp_dir = os.path.join(self.get_base_temp_dir(), self.id)
        path = os.path.join(temp_dir, *paths)
        os.makedirs(os.path.dirname(path), exist_ok=True)
        return path
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, StreamableConv1d.__init__
def __init__(self, in_channels: int, out_channels: int,
                 kernel_size: int, stride: int = 1, dilation: int = 1,
                 groups: int = 1, bias: bool = True, causal: bool = False,
                 norm: str = 'none', norm_kwargs: tp.Dict[str, tp.Any] = {},
                 pad_mode: str = 'reflect'):
        super().__init__()
        # warn user on unusual setup between dilation and stride
        if stride > 1 and dilation > 1:
            warnings.warn("StreamableConv1d has been initialized with stride > 1 and dilation > 1"
                          f" (kernel_size={kernel_size} stride={stride}, dilation={dilation}).")
        self.conv = NormConv1d(in_channels, out_channels, kernel_size, stride,
                               dilation=dilation, groups=groups, bias=bias, causal=causal,
                               norm=norm, norm_kwargs=norm_kwargs)
        self.causal = causal
        self.pad_mode = pad_mode
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, EmbeddingCache._get_cache_path
def _get_cache_path(self, path: tp.Union[Path, str]):
        """Get cache path for the given file path."""
        sig = sha1(str(path).encode()).hexdigest()
        return self.cache_path / sig
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_audiogen.py
BlockTypes.METHOD, TestAudioGenModel.test_base
def test_base(self):
        ag = self.get_audiogen()
        assert ag.frame_rate == 25
        assert ag.sample_rate == 16000
        assert ag.audio_channels == 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\deadlock.py
BlockTypes.METHOD, DeadlockDetect.__exit__
def __exit__(self, exc_type, exc_val, exc_tb):
        if self.use:
            self._queue.put(None)
            self._thread.join()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._get_wav_embedding
def _get_wav_embedding(self, x: WavCondition) -> torch.Tensor:
        """Get the wav embedding from the WavCondition.
        The conditioner will either extract the embedding on-the-fly computing it from the condition wav directly
        or will rely on the embedding cache to load the pre-computed embedding if relevant.
        """
        sampled_wav: tp.Optional[torch.Tensor] = None
        if not self.training and self.eval_wavs is not None:
            warn_once(logger, "Using precomputed evaluation wavs!")
            sampled_wav = self._sample_eval_wavs(len(x.wav))

        no_undefined_paths = all(p is not None for p in x.path)
        no_nullified_cond = x.wav.shape[-1] > 1
        if sampled_wav is not None:
            chroma = self._compute_wav_embedding(sampled_wav, self.sample_rate)
        elif self.cache is not None and no_undefined_paths and no_nullified_cond:
            paths = [Path(p) for p in x.path if p is not None]
            chroma = self.cache.get_embed_from_cache(paths, x)
        else:
            assert all(sr == x.sample_rate[0] for sr in x.sample_rate), "All sample rates in batch should be equal."
            chroma = self._compute_wav_embedding(x.wav, x.sample_rate[0])

        if self.match_len_on_eval:
            B, T, C = chroma.shape
            if T > self.chroma_len:
                chroma = chroma[:, :self.chroma_len]
                logger.debug(f"Chroma was truncated to match length! ({T} -> {chroma.shape[1]})")
            elif T < self.chroma_len:
                n_repeat = int(math.ceil(self.chroma_len / T))
                chroma = chroma.repeat(1, n_repeat, 1)
                chroma = chroma[:, :self.chroma_len]
                logger.debug(f"Chroma was repeated to match length! ({T} -> {chroma.shape[1]})")

        return chroma
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, VectorQuantization.codebook
def codebook(self):
        return self._codebook.embed
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._tokenizer
def _tokenizer(self, texts: tp.Union[str, tp.List[str]]) -> dict:
        # we use the default params from CLAP module here as well
        return self.clap_tokenize(texts, padding="max_length", truncation=True, max_length=77, return_tensors="pt")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, MusicInfo.attribute_getter
def attribute_getter(attribute):
        if attribute == 'bpm':
            preprocess_func = get_bpm
        elif attribute == 'key':
            preprocess_func = get_musical_key
        elif attribute in ['moods', 'keywords']:
            preprocess_func = get_keyword_list
        elif attribute in ['genre', 'name', 'instrument']:
            preprocess_func = get_keyword
        elif attribute in ['title', 'artist', 'description']:
            preprocess_func = get_string
        else:
            preprocess_func = None
        return preprocess_func
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\ema.py
BlockTypes.METHOD, ModuleDictEMA.step
def step(self):
        if self.unbias:
            self.count = self.count * self.decay + 1
            w = 1 / self.count
        else:
            w = 1 - self.decay
        for module_name, module in self.module_dict.items():
            for key, val in _get_named_tensors(module):
                if not val.is_floating_point():
                    continue
                device = self.device or val.device
                self.state[module_name][key].mul_(1 - w)
                self.state[module_name][key].add_(val.detach().to(device), alpha=w)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, FeatureMatchingLoss.forward
def forward(self, fmap_fake: tp.List[torch.Tensor], fmap_real: tp.List[torch.Tensor]) -> torch.Tensor:
        assert len(fmap_fake) == len(fmap_real) and len(fmap_fake) > 0
        feat_loss = torch.tensor(0., device=fmap_fake[0].device)
        feat_scale = torch.tensor(0., device=fmap_fake[0].device)
        n_fmaps = 0
        for (feat_fake, feat_real) in zip(fmap_fake, fmap_real):
            assert feat_fake.shape == feat_real.shape
            n_fmaps += 1
            feat_loss += self.loss(feat_fake, feat_real)
            feat_scale += torch.mean(torch.abs(feat_real))

        if self.normalize:
            feat_loss /= n_fmaps

        return feat_loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio.py
BlockTypes.METHOD, _soundfile_info
def _soundfile_info(filepath: tp.Union[str, Path]) -> AudioFileInfo:
    info = soundfile.info(filepath)
    return AudioFileInfo(info.samplerate, info.duration, info.channels)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, get_model
def get_model(cfg, channels: int, side: int, num_steps: int):
    if cfg.model == 'unet':
        return DiffusionUnet(
            chin=channels, num_steps=num_steps, **cfg.diffusion_unet)
    else:
        raise RuntimeError('Not Implemented')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_rope.py
BlockTypes.METHOD, test_rope_io_dtypes
def test_rope_io_dtypes():
    set_efficient_attention_backend('xformers')
    B, T, H, C = 8, 75, 16, 128

    rope_32 = RotaryEmbedding(dim=C, dtype=torch.float32)
    rope_64 = RotaryEmbedding(dim=C, dtype=torch.float64)

    # Test bfloat16 inputs w/ both 32 and 64 precision rope.
    xq_16 = torch.rand((B, T, H, C)).to(torch.bfloat16)
    xk_16 = torch.rand((B, T, H, C)).to(torch.bfloat16)
    xq_out, xk_out = rope_32.rotate_qk(xq_16, xk_16)
    assert xq_out.dtype == torch.bfloat16
    xq_out, xk_out = rope_64.rotate_qk(xq_16, xk_16)
    assert xq_out.dtype == torch.bfloat16

    # Test float32 inputs w/ both 32 and 64 precision rope.
    xq_32 = torch.rand((B, T, H, C)).to(torch.float32)
    xk_32 = torch.rand((B, T, H, C)).to(torch.float32)
    xq_out, xk_out = rope_32.rotate_qk(xq_32, xk_32)
    assert xq_out.dtype == torch.float32
    xq_out, xk_out = rope_64.rotate_qk(xq_32, xk_32)
    assert xq_out.dtype == torch.float32
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\diffusion\4_bands_base_32khz.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    launcher.slurm_(gpus=4, partition='learnfair')

    launcher.bind_({'solver': 'diffusion/default',
                    'dset': 'internal/music_10k_32khz'})

    with launcher.job_array():
        launcher({'filter.use': True, 'filter.idx_band': 0, "processor.use": False, 'processor.power_std': 0.4})
        launcher({'filter.use': True, 'filter.idx_band': 1, "processor.use": False, 'processor.power_std': 0.4})
        launcher({'filter.use': True, 'filter.idx_band': 2, "processor.use": True, 'processor.power_std': 0.4})
        launcher({'filter.use': True, 'filter.idx_band': 3, "processor.use": True, 'processor.power_std': 0.75})
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\linear_warmup_lr_scheduler.py
BlockTypes.METHOD, LinearWarmupLRScheduler._get_sched_lr
def _get_sched_lr(self, lr: float, step: int):
        if step < self.warmup_steps:
            warmup_init_lr = self.warmup_init_lr or 0
            lr_step = (lr - warmup_init_lr) / self.warmup_steps
            lr = warmup_init_lr + step * lr_step
        return lr
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.encode
def encode(self, x: torch.Tensor) -> tp.Tuple[torch.Tensor, tp.Optional[torch.Tensor]]:
        """Encode the given input tensor to quantized representation along with scale parameter.

        Args:
            x (torch.Tensor): Float tensor of shape [B, C, T]

        Returns:
            codes, scale (tuple of torch.Tensor, torch.Tensor): Tuple composed of:
                codes a float tensor of shape [B, K, T] with K the number of codebooks used and T the timestep.
                scale a float tensor containing the scale for audio renormalizealization.
        """
        assert x.dim() == 3
        x, scale = self.preprocess(x)
        emb = self.encoder(x)
        codes = self.quantizer.encode(emb)
        return codes, scale
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_lr_scheduler
def get_lr_scheduler(optimizer: torch.optim.Optimizer,
                     cfg: omegaconf.DictConfig,
                     total_updates: int) -> tp.Optional[LRScheduler]:
    """Build torch learning rate scheduler from config and associated optimizer.
    Supported learning rate schedulers: ExponentialLRScheduler, PlateauLRScheduler

    Args:
        optimizer (torch.optim.Optimizer): Optimizer.
        cfg (DictConfig): Schedule-related configuration.
        total_updates (int): Total number of updates.
    Returns:
        torch.optim.Optimizer.
    """
    if 'lr_scheduler' not in cfg:
        raise KeyError("LR Scheduler not found in config")

    lr_sched: tp.Optional[LRScheduler] = None
    if cfg.lr_scheduler == 'step':
        lr_sched = torch.optim.lr_scheduler.StepLR(optimizer, **cfg.step)
    elif cfg.lr_scheduler == 'exponential':
        lr_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=cfg.exponential)
    elif cfg.lr_scheduler == 'cosine':
        kwargs = dict_from_config(cfg.cosine)
        warmup_steps = kwargs.pop('warmup')
        lr_sched = optim.CosineLRScheduler(
            optimizer, warmup_steps=warmup_steps, total_steps=total_updates, **kwargs)
    elif cfg.lr_scheduler == 'polynomial_decay':
        kwargs = dict_from_config(cfg.polynomial_decay)
        warmup_steps = kwargs.pop('warmup')
        lr_sched = optim.PolynomialDecayLRScheduler(
            optimizer, warmup_steps=warmup_steps, total_steps=total_updates, **kwargs)
    elif cfg.lr_scheduler == 'inverse_sqrt':
        kwargs = dict_from_config(cfg.inverse_sqrt)
        warmup_steps = kwargs.pop('warmup')
        lr_sched = optim.InverseSquareRootLRScheduler(optimizer, warmup_steps=warmup_steps, **kwargs)
    elif cfg.lr_scheduler == 'linear_warmup':
        kwargs = dict_from_config(cfg.linear_warmup)
        warmup_steps = kwargs.pop('warmup')
        lr_sched = optim.LinearWarmupLRScheduler(optimizer, warmup_steps=warmup_steps, **kwargs)
    elif cfg.lr_scheduler is not None:
        raise ValueError(f"Unsupported LR Scheduler: {cfg.lr_scheduler}")
    return lr_sched
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._compute_text_embedding
def _compute_text_embedding(self, text: tp.List[str]) -> torch.Tensor:
        """Compute text embedding from CLAP model on a given a batch of text.

        Args:
            text (list[str]): List of text for the batch, with B items.
        Returns:
            torch.Tensor: CLAP embedding derived from text, of shape [B, 1, D], with D the CLAP embedding dimension.
        """
        with torch.no_grad():
            embed = self.clap.get_text_embedding(text, tokenizer=self._tokenizer, use_tensor=True)
            return embed.view(embed.size(0), 1, embed.size(-1))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\mos.py
BlockTypes.METHOD, mean
def mean(x):
    return sum(x) / len(x)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_compression_model
def get_compression_model(cfg: omegaconf.DictConfig) -> CompressionModel:
    """Instantiate a compression model."""
    if cfg.compression_model == 'encodec':
        kwargs = dict_from_config(getattr(cfg, 'encodec'))
        encoder_name = kwargs.pop('autoencoder')
        quantizer_name = kwargs.pop('quantizer')
        encoder, decoder = get_encodec_autoencoder(encoder_name, cfg)
        quantizer = get_quantizer(quantizer_name, cfg, encoder.dimension)
        frame_rate = kwargs['sample_rate'] // encoder.hop_length
        renormalize = kwargs.pop('renormalize', False)
        # deprecated params
        kwargs.pop('renorm', None)
        return EncodecModel(encoder, decoder, quantizer,
                            frame_rate=frame_rate, renormalize=renormalize, **kwargs).to(cfg.device)
    else:
        raise KeyError(f"Unexpected compression model {cfg.compression_model}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, VectorQuantization.inited
def inited(self):
        return self._codebook.inited
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.build_model
def build_model(self):
        """Instantiate model and optimizer."""
        # Model and optimizer
        self.model = models.builders.get_compression_model(self.cfg).to(self.device)
        self.optimizer = builders.get_optimizer(self.model.parameters(), self.cfg.optim)
        self.register_stateful('model', 'optimizer')
        self.register_best_state('model')
        self.register_ema('model')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, get_samples_for_xps
def get_samples_for_xps(xps: tp.List[dora.XP], **kwargs) -> tp.Dict[str, tp.List[Sample]]:
    """Gets a dictionary of matched samples across the given XPs.
    Each dictionary entry maps a sample id to a list of samples for that id. The number of samples per id
    will always match the number of XPs provided and will correspond to each XP in the same order given.
    In other words, only samples that can be match across all provided XPs will be returned
    in order to satisfy this rule.

    There are two types of ids that can be returned: stable and unstable.
    * Stable IDs are deterministic ids that were computed by the SampleManager given a sample's inputs
      (prompts/conditioning). This is why we can match them across XPs.
    * Unstable IDs are of the form "noinput_{idx}" and are generated on-the-fly, in order to map samples
      that used non-deterministic, random ids. This is the case for samples that did not use prompts or
      conditioning for their generation. This function will sort these samples by their id and match them
      by their index.

    Args:
        xps: a list of XPs to match samples from.
        start_epoch (int): If provided, only return samples corresponding to this epoch or newer.
        end_epoch (int): If provided, only return samples corresponding to this epoch or older.
        exclude_prompted (bool): If True, does not include samples that used a prompt.
        exclude_unprompted (bool): If True, does not include samples that did not use a prompt.
        exclude_conditioned (bool): If True, excludes samples that used conditioning.
        exclude_unconditioned (bool): If True, excludes samples that did not use conditioning.
    """
    managers = [SampleManager(xp) for xp in xps]
    samples_per_xp = [manager.get_samples(**kwargs) for manager in managers]
    stable_samples = _match_stable_samples(samples_per_xp)
    unstable_samples = _match_unstable_samples(samples_per_xp)
    return dict(stable_samples, **unstable_samples)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msd.py
BlockTypes.METHOD, MultiScaleDiscriminator.__init__
def __init__(self, in_channels: int = 1, out_channels: int = 1, downsample_factor: int = 2,
                 scale_norms: tp.Sequence[str] = ['weight_norm', 'weight_norm', 'weight_norm'], **kwargs):
        super().__init__()
        self.discriminators = nn.ModuleList([
            ScaleDiscriminator(in_channels, out_channels, norm=norm, **kwargs) for norm in scale_norms
        ])
        self.downsample = nn.AvgPool1d(downsample_factor * 2, downsample_factor, padding=downsample_factor)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, SpectralConvergenceLoss.forward
def forward(self, x_mag: torch.Tensor, y_mag: torch.Tensor):
        """Calculate forward propagation.

        Args:
            x_mag: Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).
            y_mag: Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).
        Returns:
            torch.Tensor: Spectral convergence loss value.
        """
        return torch.norm(y_mag - x_mag, p="fro") / (torch.norm(y_mag, p="fro") + self.epsilon)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, AttributeDropout.forward
def forward(self, samples: tp.List[ConditioningAttributes]) -> tp.List[ConditioningAttributes]:
        """
        Args:
            samples (list[ConditioningAttributes]): List of conditions.
        Returns:
            list[ConditioningAttributes]: List of conditions after certain attributes were set to None.
        """
        if not self.training and not self.active_on_eval:
            return samples

        samples = deepcopy(samples)
        for condition_type, ps in self.p.items():  # for condition types [text, wav]
            for condition, p in ps.items():  # for attributes of each type (e.g., [artist, genre])
                if torch.rand(1, generator=self.rng).item() < p:
                    for sample in samples:
                        dropout_condition(sample, condition_type, condition)
        return samples
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\lstm.py
BlockTypes.METHOD, StreamableLSTM.forward
def forward(self, x):
        x = x.permute(2, 0, 1)
        y, _ = self.lstm(x)
        if self.skip:
            y = y + x
        y = y.permute(1, 2, 0)
        return y
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern._validate_layout
def _validate_layout(self):
        """Runs checks on the layout to ensure a valid pattern is defined.
        A pattern is considered invalid if:
            - Multiple timesteps for a same codebook are defined in the same sequence step
            - The timesteps for a given codebook are not in ascending order as we advance in the sequence
              (this would mean that we have future timesteps before past timesteps).
        """
        q_timesteps = {q: 0 for q in range(self.n_q)}
        for s, seq_coords in enumerate(self.layout):
            if len(seq_coords) > 0:
                qs = set()
                for coord in seq_coords:
                    qs.add(coord.q)
                    last_q_timestep = q_timesteps[coord.q]
                    assert coord.t >= last_q_timestep, \
                        f"Past timesteps are found in the sequence for codebook = {coord.q} at step {s}"
                    q_timesteps[coord.q] = coord.t
                # each sequence step contains at max 1 coordinate per codebook
                assert len(qs) == len(seq_coords), \
                    f"Multiple entries for a same codebook are found at step {s}"
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_seanet.py
BlockTypes.METHOD, TestSEANetModel.test_disable_norm_raises_exception
def test_disable_norm_raises_exception(self):
        # Invalid disable_norm_outer_blocks values raise exceptions
        with pytest.raises(AssertionError):
            SEANetEncoder(disable_norm_outer_blocks=-1)

        with pytest.raises(AssertionError):
            SEANetEncoder(ratios=[1, 1, 2, 2], disable_norm_outer_blocks=7)

        with pytest.raises(AssertionError):
            SEANetDecoder(disable_norm_outer_blocks=-1)

        with pytest.raises(AssertionError):
            SEANetDecoder(ratios=[1, 1, 2, 2], disable_norm_outer_blocks=7)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestInfo._test_info_format
def _test_info_format(self, ext: str):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 1.
        for sample_rate, ch in product(sample_rates, channels):
            n_frames = int(sample_rate * duration)
            wav = get_white_noise(ch, n_frames)
            path = self.get_temp_path(f'sample_wav{ext}')
            save_wav(path, wav, sample_rate)
            info = audio_info(path)
            assert info.sample_rate == sample_rate
            assert info.channels == ch
            assert np.isclose(info.duration, duration, atol=1e-5)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\rvm.py
BlockTypes.METHOD, db_to_scale
def db_to_scale(volume: tp.Union[float, torch.Tensor]):
    return 10 ** (volume / 20)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.__init__
def __init__(
        self,
        dim: int,
        codebook_size: int,
        kmeans_init: int = False,
        kmeans_iters: int = 10,
        decay: float = 0.8,
        epsilon: float = 1e-5,
        threshold_ema_dead_code: int = 2,
    ):
        super().__init__()
        self.decay = decay
        init_fn: tp.Union[tp.Callable[..., torch.Tensor], tp.Any] = uniform_init if not kmeans_init else torch.zeros
        embed = init_fn(codebook_size, dim)

        self.codebook_size = codebook_size

        self.kmeans_iters = kmeans_iters
        self.epsilon = epsilon
        self.threshold_ema_dead_code = threshold_ema_dead_code

        self.register_buffer("inited", torch.Tensor([not kmeans_init]))
        self.register_buffer("cluster_size", torch.zeros(codebook_size))
        self.register_buffer("embed", embed)
        self.register_buffer("embed_avg", embed.clone())
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_rope.py
BlockTypes.METHOD, test_rope
def test_rope():
    set_efficient_attention_backend('xformers')
    B, T, H, C = 8, 75, 16, 128

    rope = RotaryEmbedding(dim=C)
    xq = torch.rand((B, T, H, C))
    xk = torch.rand((B, T, H, C))
    xq_out, xk_out = rope.rotate_qk(xq, xk, start=7)

    assert list(xq_out.shape) == [B, T, H, C]
    assert list(xk_out.shape) == [B, T, H, C]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio.py
BlockTypes.METHOD, audio_info
def audio_info(filepath: tp.Union[str, Path]) -> AudioFileInfo:
    # torchaudio no longer returns useful duration informations for some formats like mp3s.
    filepath = Path(filepath)
    if filepath.suffix in ['.flac', '.ogg']:  # TODO: Validate .ogg can be safely read with av_info
        # ffmpeg has some weird issue with flac.
        return _soundfile_info(filepath)
    else:
        return _av_info(filepath)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\fsdp.py
BlockTypes.METHOD, _FSDPFixStateDict.load_state_dict
def load_state_dict(self, state: tp.Dict[str, tp.Any]):  # type: ignore
        if self._state_dict_type is StateDictType.FULL_STATE_DICT:
            super().load_state_dict(state)
            purge_fsdp(self)
            return
        # Fix FSDP load state dict in all situation.
        # Use this only with LOCAL_STATE_DICT !!!
        current_state = dict(super().state_dict())
        for key, value in state.items():
            key = _FSDPFixStateDict._name_without_fsdp_prefix(key)
            if key not in current_state:
                # Emulate strict loading manually.
                raise RuntimeError(f"Unknown state key {key}")
            current_state[key].copy_(value)

        # Purging cached weights from previous forward.
        purge_fsdp(self)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\mos.py
BlockTypes.METHOD, std
def std(x):
    m = mean(x)
    return math.sqrt(sum((i - m)**2 for i in x) / len(x))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_dataset_segment
def test_dataset_segment(self):
        total_examples = 10
        num_samples = 20
        min_duration, max_duration = 1., 4.
        segment_duration = 1.
        sample_rate = 16_000
        channels = 1
        dataset = self._create_audio_dataset(
            'dset', total_examples, durations=(min_duration, max_duration), sample_rate=sample_rate,
            channels=channels, segment_duration=segment_duration, num_examples=num_samples)
        assert len(dataset) == num_samples
        assert dataset.sample_rate == sample_rate
        assert dataset.channels == channels
        for idx in range(len(dataset)):
            sample = dataset[idx]
            assert sample.shape[0] == channels
            assert sample.shape[1] == int(segment_duration * sample_rate)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\polynomial_decay_lr_scheduler.py
BlockTypes.METHOD, PolynomialDecayLRScheduler._get_sched_lr
def _get_sched_lr(self, lr: float, step: int):
        if self.zero_lr_warmup_steps > 0 and step <= self.zero_lr_warmup_steps:
            lr = 0
        elif self.warmup_steps > 0 and step <= self.warmup_steps + self.zero_lr_warmup_steps:
            lr_ratio = (step - self.zero_lr_warmup_steps) / float(self.warmup_steps)
            lr = lr_ratio * lr
        elif step >= self.total_steps:
            lr = self.end_lr
        else:
            total_warmup_steps = self.warmup_steps + self.zero_lr_warmup_steps
            lr_range = lr - self.end_lr
            pct_remaining = 1 - (step - total_warmup_steps) / (self.total_steps - total_warmup_steps)
            lr = lr_range * pct_remaining ** self.power + self.end_lr
        return lr
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, VectorQuantization._preprocess
def _preprocess(self, x):
        if not self.channels_last:
            x = rearrange(x, "b d n -> b n d")
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\train.py
BlockTypes.METHOD, get_solver_from_xp
def get_solver_from_xp(xp: XP, override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                       restore: bool = True, load_best: bool = True,
                       ignore_state_keys: tp.List[str] = [], disable_fsdp: bool = True):
    """Given a XP, return the Solver object.

    Args:
        xp (XP): Dora experiment for which to retrieve the solver.
        override_cfg (dict or None): If not None, should be a dict used to
            override some values in the config of `xp`. This will not impact
            the XP signature or folder. The format is different
            than the one used in Dora grids, nested keys should actually be nested dicts,
            not flattened, e.g. `{'optim': {'batch_size': 32}}`.
        restore (bool): If `True` (the default), restore state from the last checkpoint.
        load_best (bool): If `True` (the default), load the best state from the checkpoint.
        ignore_state_keys (list[str]): List of sources to ignore when loading the state, e.g. `optimizer`.
        disable_fsdp (bool): if True, disables FSDP entirely. This will
            also automatically skip loading the EMA. For solver specific
            state sources, like the optimizer, you might want to
            use along `ignore_state_keys=['optimizer']`. Must be used with `load_best=True`.
    """
    logger.info(f"Loading solver from XP {xp.sig}. "
                f"Overrides used: {xp.argv}")
    cfg = xp.cfg
    if override_cfg is not None:
        cfg = omegaconf.OmegaConf.merge(cfg, omegaconf.DictConfig(override_cfg))
    if disable_fsdp and cfg.fsdp.use:
        cfg.fsdp.use = False
        assert load_best is True
        # ignoring some keys that were FSDP sharded like model, ema, and best_state.
        # fsdp_best_state will be used in that case. When using a specific solver,
        # one is responsible for adding the relevant keys, e.g. 'optimizer'.
        # We could make something to automatically register those inside the solver, but that
        # seem overkill at this point.
        ignore_state_keys = ignore_state_keys + ['model', 'ema', 'best_state']

    try:
        with xp.enter():
            solver = get_solver(cfg)
            if restore:
                solver.restore(load_best=load_best, ignore_state_keys=ignore_state_keys)
        return solver
    finally:
        hydra.core.global_hydra.GlobalHydra.instance().clear()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\common_utils\temp_utils.py
BlockTypes.METHOD, TempDirMixin.get_temp_dir
def get_temp_dir(self, *paths):
        temp_dir = os.path.join(self.get_base_temp_dir(), self.id)
        path = os.path.join(temp_dir, *paths)
        os.makedirs(path, exist_ok=True)
        return path
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_activations.py
BlockTypes.METHOD, TestActivations.test_custom_glu_calculation
def test_custom_glu_calculation(self):

        activation = CustomGLU(nn.Identity())

        initial_shape = (4, 8, 8)

        part_a = torch.ones(initial_shape) * 2
        part_b = torch.ones(initial_shape) * -1
        input = torch.cat((part_a, part_b), dim=-1)

        output = activation(input)

        # ensure all dimensions match initial shape
        assert output.shape == initial_shape
        # ensure the gating was calculated correctly a * f(b)
        assert torch.all(output == -2).item()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, init_layer
def init_layer(m: nn.Module,
               method: str,
               init_depth: tp.Optional[int] = None,
               zero_bias_init: bool = False):
    """Wrapper around ``get_init_fn`` for proper initialization of LM modules.

    Args:
        m (nn.Module): Module to initialize.
        method (str): Method name for the init function.
        init_depth (int, optional): Optional init depth value used to rescale
            the standard deviation if defined.
        zero_bias_init (bool): Whether to initialize the bias to 0 or not.
    """
    if isinstance(m, nn.Linear):
        init_fn = get_init_fn(method, m.in_features, init_depth=init_depth)
        if m.weight.device.type == 'cpu' and m.weight.dtype == torch.float16:
            weight = m.weight.float()
            init_fn(weight)
            m.weight.data[:] = weight.half()
        else:
            init_fn(m.weight)
        if zero_bias_init and m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.Embedding):
        init_fn = get_init_fn(method, m.embedding_dim, init_depth=None)
        if m.weight.device.type == 'cpu' and m.weight.dtype == torch.float16:
            weight = m.weight.float()
            init_fn(weight)
            m.weight.data[:] = weight.half()
        else:
            init_fn(m.weight)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.__post_init__
def __post_init__(self):
        assert len(self.layout) > 0
        assert self.layout[0] == []
        self._validate_layout()
        self._build_reverted_sequence_scatter_indexes = lru_cache(100)(self._build_reverted_sequence_scatter_indexes)
        self._build_pattern_sequence_scatter_indexes = lru_cache(100)(self._build_pattern_sequence_scatter_indexes)
        logger.info("New pattern, time steps: %d, sequence steps: %d", self.timesteps, len(self.layout))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_dataset_return_info
def test_dataset_return_info(self):
        total_examples = 10
        num_samples = 20
        min_duration, max_duration = 1., 4.
        segment_duration = 1.
        sample_rate = 16_000
        channels = 1
        dataset = self._create_audio_dataset(
            'dset', total_examples, durations=(min_duration, max_duration), sample_rate=sample_rate,
            channels=channels, segment_duration=segment_duration, num_examples=num_samples, return_info=True)
        assert len(dataset) == num_samples
        assert dataset.sample_rate == sample_rate
        assert dataset.channels == channels
        for idx in range(len(dataset)):
            sample, segment_info = dataset[idx]
            assert sample.shape[0] == channels
            assert sample.shape[1] == int(segment_duration * sample_rate)
            assert segment_info.sample_rate == sample_rate
            assert segment_info.total_frames == int(segment_duration * sample_rate)
            assert segment_info.n_frames <= int(segment_duration * sample_rate)
            assert segment_info.seek_time >= 0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_audiogen.py
BlockTypes.METHOD, TestAudioGenModel.test_generate_continuation
def test_generate_continuation(self):
        ag = self.get_audiogen()
        prompt = torch.randn(3, 1, 16000)
        wav = ag.generate_continuation(prompt, 16000)
        assert list(wav.shape) == [3, 1, 32000]

        prompt = torch.randn(2, 1, 16000)
        wav = ag.generate_continuation(
            prompt, 16000, ['youpi', 'lapin dort'])
        assert list(wav.shape) == [2, 1, 32000]

        prompt = torch.randn(2, 1, 16000)
        with pytest.raises(AssertionError):
            wav = ag.generate_continuation(
                prompt, 16000, ['youpi', 'lapin dort', 'one too many'])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, TestNormConv1d.test_norm_conv1d_modules
def test_norm_conv1d_modules(self):
        N, C, T = 2, 2, random.randrange(1, 100_000)
        t0 = torch.randn(N, C, T)

        C_out, kernel_size, stride = 1, 4, 1
        expected_out_length = int((T - kernel_size) / stride + 1)
        wn_conv = NormConv1d(C, 1, kernel_size=4, norm='weight_norm')
        gn_conv = NormConv1d(C, 1, kernel_size=4, norm='time_group_norm')
        nn_conv = NormConv1d(C, 1, kernel_size=4, norm='none')

        assert isinstance(wn_conv.norm, nn.Identity)
        assert isinstance(wn_conv.conv, nn.Conv1d)

        assert isinstance(gn_conv.norm, nn.GroupNorm)
        assert isinstance(gn_conv.conv, nn.Conv1d)

        assert isinstance(nn_conv.norm, nn.Identity)
        assert isinstance(nn_conv.conv, nn.Conv1d)

        for conv_layer in [wn_conv, gn_conv, nn_conv]:
            out = conv_layer(t0)
            assert isinstance(out, torch.Tensor)
            assert list(out.shape) == [N, C_out, expected_out_length]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\deadlock.py
BlockTypes.METHOD, DeadlockDetect._detector_thread
def _detector_thread(self):
        logger.debug("Deadlock detector started")
        last_stage = "init"
        while True:
            try:
                stage = self._queue.get(timeout=self.timeout)
            except Empty:
                break
            if stage is None:
                logger.debug("Exiting deadlock detector thread")
                return
            else:
                last_stage = stage
        logger.error("Deadlock detector timed out, last stage was %s", last_stage)
        for th in threading.enumerate():
            print(th, file=sys.stderr)
            traceback.print_stack(sys._current_frames()[th.ident])
            print(file=sys.stderr)
        sys.stdout.flush()
        sys.stderr.flush()
        os.kill(os.getpid(), signal.SIGKILL)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.resolve_reference_path
def resolve_reference_path(cls, path: tp.Union[str, Path]) -> Path:
        """Converts reference placeholder in path with configured reference dir to resolve paths.

        Args:
            path (str or Path): Path to resolve.
        Returns:
            Path: Resolved path.
        """
        path = str(path)

        if path.startswith("//reference"):
            reference_dir = cls.get_reference_dir()
            logger.warn(f"Reference directory: {reference_dir}")
            assert (
                reference_dir.exists() and reference_dir.is_dir()
            ), f"Reference directory does not exist: {reference_dir}."
            path = re.sub("^//reference", str(reference_dir), path)

        return Path(path)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, expand_repeated_kv
def expand_repeated_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """torch.repeat_interleave(x, dim=2, repeats=n_rep) from xlformers."""
    if n_rep == 1:
        return x
    if _efficient_attention_backend == 'torch':
        bs, n_kv_heads, slen, head_dim = x.shape
        return (
            x[:, :, None, :, :]
            .expand(bs, n_kv_heads, n_rep, slen, head_dim)
            .reshape(bs, n_kv_heads * n_rep, slen, head_dim)
        )
    else:
        bs, slen, n_kv_heads, head_dim = x.shape
        return (
            x[:, :, :, None, :]
            .expand(bs, slen, n_kv_heads, n_rep, head_dim)
            .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
        )
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, MusicInfo.from_dict
def from_dict(cls, dictionary: dict, fields_required: bool = False):
        _dictionary: tp.Dict[str, tp.Any] = {}

        # allow a subset of attributes to not be loaded from the dictionary
        # these attributes may be populated later
        post_init_attributes = ['self_wav', 'joint_embed']
        optional_fields = ['keywords']

        for _field in fields(cls):
            if _field.name in post_init_attributes:
                continue
            elif _field.name not in dictionary:
                if fields_required and _field.name not in optional_fields:
                    raise KeyError(f"Unexpected missing key: {_field.name}")
            else:
                preprocess_func: tp.Optional[tp.Callable] = cls.attribute_getter(_field.name)
                value = dictionary[_field.name]
                if preprocess_func:
                    value = preprocess_func(value)
                _dictionary[_field.name] = value
        return cls(**_dictionary)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\rvm.py
BlockTypes.METHOD, scale_to_db
def scale_to_db(scale: torch.Tensor, min_volume: float = -120):
    min_scale = db_to_scale(min_volume)
    return 20 * torch.log10(scale.clamp(min=min_scale))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_condition_fuser
def get_condition_fuser(cfg: omegaconf.DictConfig) -> ConditionFuser:
    """Instantiate a condition fuser object."""
    fuser_cfg = getattr(cfg, 'fuser')
    fuser_methods = ['sum', 'cross', 'prepend', 'input_interpolate']
    fuse2cond = {k: fuser_cfg[k] for k in fuser_methods}
    kwargs = {k: v for k, v in fuser_cfg.items() if k not in fuser_methods}
    fuser = ConditionFuser(fuse2cond=fuse2cond, **kwargs)
    return fuser
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\loaders.py
BlockTypes.METHOD, load_compression_model
def load_compression_model(file_or_url_or_id: tp.Union[Path, str], device='cpu', cache_dir: tp.Optional[str] = None):
    pkg = load_compression_model_ckpt(file_or_url_or_id, cache_dir=cache_dir)
    if 'pretrained' in pkg:
        return CompressionModel.get_pretrained(pkg['pretrained'], device=device)
    cfg = OmegaConf.create(pkg['xp.cfg'])
    cfg.device = str(device)
    model = builders.get_compression_model(cfg)
    model.load_state_dict(pkg['best_state'])
    model.eval()
    return model
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner.__init__
def __init__(self, output_dim: int, sample_rate: int, n_chroma: int, radix2_exp: int,
                 duration: float, match_len_on_eval: bool = True, eval_wavs: tp.Optional[str] = None,
                 n_eval_wavs: int = 0, cache_path: tp.Optional[tp.Union[str, Path]] = None,
                 device: tp.Union[torch.device, str] = 'cpu', **kwargs):
        from demucs import pretrained
        super().__init__(dim=n_chroma, output_dim=output_dim, device=device)
        self.autocast = TorchAutocast(enabled=device != 'cpu', device_type=self.device, dtype=torch.float32)
        self.sample_rate = sample_rate
        self.match_len_on_eval = match_len_on_eval
        self.duration = duration
        self.__dict__['demucs'] = pretrained.get_model('htdemucs').to(device)
        stem_sources: list = self.demucs.sources  # type: ignore
        self.stem_indices = torch.LongTensor([stem_sources.index('vocals'), stem_sources.index('other')]).to(device)
        self.chroma = ChromaExtractor(sample_rate=sample_rate, n_chroma=n_chroma,
                                      radix2_exp=radix2_exp, **kwargs).to(device)
        self.chroma_len = self._get_chroma_len()
        self.eval_wavs: tp.Optional[torch.Tensor] = self._load_eval_wavs(eval_wavs, n_eval_wavs)
        self.cache = None
        if cache_path is not None:
            self.cache = EmbeddingCache(Path(cache_path) / 'wav', self.device,
                                        compute_embed_fn=self._get_full_chroma_for_cache,
                                        extract_embed_fn=self._extract_chroma_chunk)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\dadam.py
BlockTypes.METHOD, DAdaptAdam.supports_memory_efficient_fp16
def supports_memory_efficient_fp16(self):
        return False
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.build_dataloaders
def build_dataloaders(self):
        """Instantiate audio dataloaders for each stage."""
        self.dataloaders = builders.get_audio_datasets(self.cfg)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, AdversarialLoss._save_to_state_dict
def _save_to_state_dict(self, destination, prefix, keep_vars):
        # Add the optimizer state dict inside our own.
        super()._save_to_state_dict(destination, prefix, keep_vars)
        destination[prefix + 'optimizer'] = self.optimizer.state_dict()
        return destination
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\chroma_cosinesim.py
BlockTypes.METHOD, ChromaCosineSimilarityMetric.update
def update(self, preds: torch.Tensor, targets: torch.Tensor,
               sizes: torch.Tensor, sample_rates: torch.Tensor) -> None:
        """Compute cosine similarity between chromagrams and accumulate scores over the dataset."""
        if preds.size(0) == 0:
            return

        assert preds.shape == targets.shape, (
            f"Preds and target shapes mismatch: preds={preds.shape}, targets={targets.shape}")
        assert preds.size(0) == sizes.size(0), (
            f"Number of items in preds ({preds.shape}) mismatch ",
            f"with sizes ({sizes.shape})")
        assert preds.size(0) == sample_rates.size(0), (
            f"Number of items in preds ({preds.shape}) mismatch ",
            f"with sample_rates ({sample_rates.shape})")
        assert torch.all(sample_rates == sample_rates[0].item()), "All sample rates are not the same in the batch"

        device = self.weight.device
        preds, targets = preds.to(device), targets.to(device)  # type: ignore
        sample_rate = sample_rates[0].item()
        preds = convert_audio(preds, from_rate=sample_rate, to_rate=self.chroma_sample_rate, to_channels=1)
        targets = convert_audio(targets, from_rate=sample_rate, to_rate=self.chroma_sample_rate, to_channels=1)
        gt_chroma = self.chroma_extractor(targets)
        gen_chroma = self.chroma_extractor(preds)
        chroma_lens = (sizes / self.chroma_extractor.winhop).ceil().int()
        for i in range(len(gt_chroma)):
            t = int(chroma_lens[i].item())
            cosine_sim = torch.nn.functional.cosine_similarity(
                gt_chroma[i, :t], gen_chroma[i, :t], dim=1, eps=self.eps)
            self.cosine_sum += cosine_sim.sum(dim=0)  # type: ignore
            self.weight += torch.tensor(t)  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen.__init__
def __init__(self, name: str, compression_model: CompressionModel, lm: LMModel,
                 max_duration: tp.Optional[float] = None):
        self.name = name
        self.compression_model = compression_model
        self.lm = lm
        if max_duration is None:
            if hasattr(lm, 'cfg'):
                max_duration = lm.cfg.dataset.segment_duration  # type: ignore
            else:
                raise ValueError("You must provide max_duration when building directly AudioGen")
        assert max_duration is not None
        self.max_duration: float = max_duration
        self.device = next(iter(lm.parameters())).device
        self.generation_params: dict = {}
        self.set_generation_params(duration=5)  # 5 seconds by default
        self._progress_callback: tp.Optional[tp.Callable[[int, int], None]] = None
        if self.device.type == 'cpu':
            self.autocast = TorchAutocast(enabled=False)
        else:
            self.autocast = TorchAutocast(
                enabled=True, device_type=self.device.type, dtype=torch.float16)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, ResBlock.__init__
def __init__(self, channels: int, kernel: int = 3, norm_groups: int = 4,
                 dilation: int = 1, activation: tp.Type[nn.Module] = nn.ReLU,
                 dropout: float = 0.):
        super().__init__()
        stride = 1
        padding = dilation * (kernel - stride) // 2
        Conv = nn.Conv1d
        Drop = nn.Dropout1d
        self.norm1 = nn.GroupNorm(norm_groups, channels)
        self.conv1 = Conv(channels, channels, kernel, 1, padding, dilation=dilation)
        self.activation1 = activation()
        self.dropout1 = Drop(dropout)

        self.norm2 = nn.GroupNorm(norm_groups, channels)
        self.conv2 = Conv(channels, channels, kernel, 1, padding, dilation=dilation)
        self.activation2 = activation()
        self.dropout2 = Drop(dropout)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\checkpoint.py
BlockTypes.METHOD, load_checkpoint
def load_checkpoint(checkpoint_path: Path, is_sharded: bool = False) -> tp.Any:
    """Load state from checkpoints at the specified checkpoint path."""
    if is_sharded:
        rank0_checkpoint_path = checkpoint_path.parent / checkpoint_name(use_fsdp=False)
        if rank0_checkpoint_path.exists():
            check_sharded_checkpoint(checkpoint_path, rank0_checkpoint_path)
    state = torch.load(checkpoint_path, 'cpu')
    logger.info("Checkpoint loaded from %s", checkpoint_path)
    return state
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, SoundDataset._get_info_path
def _get_info_path(self, path: tp.Union[str, Path]) -> Path:
        """Get path of JSON with metadata (description, etc.).
        If there exists a JSON with the same name as 'path.name', then it will be used.
        Else, such JSON will be searched for in an external json source folder if it exists.
        """
        info_path = Path(path).with_suffix('.json')
        if Path(info_path).exists():
            return info_path
        elif self.external_metadata_source and (Path(self.external_metadata_source) / info_path.name).exists():
            return Path(self.external_metadata_source) / info_path.name
        else:
            raise Exception(f"Unable to find a metadata JSON for path: {path}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\audiogen\audiogen_pretrained_16khz_eval.py
BlockTypes.METHOD, eval
def eval(launcher, batch_size: int = 32):
    opts = {
        'dset': 'audio/audiocaps_16khz',
        'solver/audiogen/evaluation': 'objective_eval',
        'execute_only': 'evaluate',
        '+dataset.evaluate.batch_size': batch_size,
        '+metrics.fad.tf.batch_size': 32,
    }
    # binary for FAD computation: replace this path with your own path
    metrics_opts = {
        'metrics.fad.tf.bin': '/data/home/jadecopet/local/usr/opt/google-research'
    }
    opt1 = {'generate.lm.use_sampling': True, 'generate.lm.top_k': 250, 'generate.lm.top_p': 0.}
    opt2 = {'transformer_lm.two_step_cfg': True}

    sub = launcher.bind(opts)
    sub.bind_(metrics_opts)

    # base objective metrics
    sub(opt1, opt2)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\mos.py
BlockTypes.METHOD, normalize_path
def normalize_path(path: Path):
    """Just to make path a bit nicer, make them relative to the Dora root dir.
    """
    path = path.resolve()
    dora_dir = train.main.dora.dir.resolve() / 'xps'
    return path.relative_to(dora_dir)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, NormConv1d.__init__
def __init__(self, *args, causal: bool = False, norm: str = 'none',
                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
        super().__init__()
        self.conv = apply_parametrization_norm(nn.Conv1d(*args, **kwargs), norm)
        self.norm = get_norm_module(self.conv, causal, norm, **norm_kwargs)
        self.norm_type = norm
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\export_legacy.py
BlockTypes.METHOD, export_encodec
def export_encodec(checkpoint_path: tp.Union[Path, str], out_folder: tp.Union[Path, str]):
    sig = Path(checkpoint_path).parent.name
    assert len(sig) == 8, "Not a valid Dora signature"
    pkg = torch.load(checkpoint_path, 'cpu')
    new_pkg = {
        'best_state': pkg['ema']['state']['model'],
        'xp.cfg': OmegaConf.to_yaml(pkg['xp.cfg']),
    }
    out_file = Path(out_folder) / f'{sig}.th'
    torch.save(new_pkg, out_file)
    return out_file
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, LogSTFTMagnitudeLoss.__init__
def __init__(self, epsilon: float = torch.finfo(torch.float32).eps):
        super().__init__()
        self.epsilon = epsilon
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric.compute
def compute(self) -> float:
        """Compute metrics."""
        assert self.total_files.item() > 0, "No files dumped for FAD computation!"  # type: ignore
        fad_score = self._local_compute_frechet_audio_distance()
        logger.warning(f"FAD score = {fad_score}")
        fad_score = flashy.distrib.broadcast_object(fad_score, src=0)
        return fad_score
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, DiffusionUnet.forward
def forward(self, x: torch.Tensor, step: tp.Union[int, torch.Tensor], condition: tp.Optional[torch.Tensor] = None):
        skips = []
        bs = x.size(0)
        z = x
        view_args = [1]
        if type(step) is torch.Tensor:
            step_tensor = step
        else:
            step_tensor = torch.tensor([step], device=x.device, dtype=torch.long).expand(bs)

        for idx, encoder in enumerate(self.encoders):
            z = encoder(z)
            if idx == 0:
                z = z + self.embedding(step_tensor).view(bs, -1, *view_args).expand_as(z)
            elif self.embeddings is not None:
                z = z + self.embeddings[idx - 1](step_tensor).view(bs, -1, *view_args).expand_as(z)

            skips.append(z)

        if self.use_codec:  # insert condition in the bottleneck
            assert condition is not None, "Model defined for conditionnal generation"
            condition_emb = self.conv_codec(condition)  # reshape to the bottleneck dim
            assert condition_emb.size(-1) <= 2 * z.size(-1), \
                f"You are downsampling the conditionning with factor >=2 : {condition_emb.size(-1)=} and {z.size(-1)=}"
            if not self.cross_attention:

                condition_emb = torch.nn.functional.interpolate(condition_emb, z.size(-1))
                assert z.size() == condition_emb.size()
                z += condition_emb
                cross_attention_src = None
            else:
                cross_attention_src = condition_emb.permute(0, 2, 1)  # B, T, C
                B, T, C = cross_attention_src.shape
                positions = torch.arange(T, device=x.device).view(1, -1, 1)
                pos_emb = create_sin_embedding(positions, C, max_period=10_000, dtype=cross_attention_src.dtype)
                cross_attention_src = cross_attention_src + pos_emb
        if self.use_transformer:
            z = self.transformer(z.permute(0, 2, 1), cross_attention_src=cross_attention_src).permute(0, 2, 1)
        else:
            if self.bilstm is None:
                z = torch.zeros_like(z)
            else:
                z = self.bilstm(z)

        for decoder in self.decoders:
            s = skips.pop(-1)
            z = z[:, :, :s.shape[2]]
            z = z + s
            z = decoder(z)

        z = z[:, :, :x.shape[2]]
        return Output(z)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\discriminators\msd.py
BlockTypes.METHOD, MultiScaleDiscriminator.forward
def forward(self, x: torch.Tensor) -> MultiDiscriminatorOutputType:
        logits = []
        fmaps = []
        for i, disc in enumerate(self.discriminators):
            if i != 0:
                self.downsample(x)
            logit, fmap = disc(x)
            logits.append(logit)
            fmaps.append(fmap)
        return logits, fmaps
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._get_text_embedding_for_cache
def _get_text_embedding_for_cache(self, path: tp.Union[Path, str],
                                      x: JointEmbedCondition, idx: int) -> torch.Tensor:
        """Get text embedding function for the cache."""
        text = x.text[idx]
        text = text if text is not None else ""
        return self._compute_text_embedding([text])[0]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\linear_warmup_lr_scheduler.py
BlockTypes.METHOD, LinearWarmupLRScheduler.get_lr
def get_lr(self):
        return [self._get_sched_lr(base_lr, self.last_epoch) for base_lr in self.base_lrs]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_memory_efficient
def test_memory_efficient():
    for backend in ['torch', 'xformers']:
        torch.manual_seed(1234)
        set_efficient_attention_backend(backend)

        tr = StreamingTransformer(
            16, 4, 2, custom=True, dropout=0., layer_scale=0.1)
        tr_mem_efficient = StreamingTransformer(
            16, 4, 2, dropout=0., memory_efficient=True, layer_scale=0.1)
        tr_mem_efficient.load_state_dict(tr.state_dict())
        tr.eval()
        steps = 12
        x = torch.randn(3, steps, 16)

        with torch.no_grad():
            y = tr(x)
            y2 = tr_mem_efficient(x)
            assert torch.allclose(y, y2), ((y - y2).norm(), backend)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_codebooks_pattern_provider
def get_codebooks_pattern_provider(n_q: int, cfg: omegaconf.DictConfig) -> CodebooksPatternProvider:
    """Instantiate a codebooks pattern provider object."""
    pattern_providers = {
        'parallel': ParallelPatternProvider,
        'delay': DelayedPatternProvider,
        'unroll': UnrolledPatternProvider,
        'valle': VALLEPattern,
        'musiclm': MusicLMPattern,
    }
    name = cfg.modeling
    kwargs = dict_from_config(cfg.get(name)) if hasattr(cfg, name) else {}
    klass = pattern_providers[name]
    return klass(n_q, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.run_epoch
def run_epoch(self):
        # reset random seed at the beginning of the epoch
        self.rng = torch.Generator()
        self.rng.manual_seed(1234 + self.epoch)
        # run epoch
        super().run_epoch()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio.py
BlockTypes.METHOD, _av_read
def _av_read(filepath: tp.Union[str, Path], seek_time: float = 0, duration: float = -1.) -> tp.Tuple[torch.Tensor, int]:
    """FFMPEG-based audio file reading using PyAV bindings.
    Soundfile cannot read mp3 and av_read is more efficient than torchaudio.

    Args:
        filepath (str or Path): Path to audio file to read.
        seek_time (float): Time at which to start reading in the file.
        duration (float): Duration to read from the file. If set to -1, the whole file is read.
    Returns:
        tuple of torch.Tensor, int: Tuple containing audio data and sample rate
    """
    _init_av()
    with av.open(str(filepath)) as af:
        stream = af.streams.audio[0]
        sr = stream.codec_context.sample_rate
        num_frames = int(sr * duration) if duration >= 0 else -1
        frame_offset = int(sr * seek_time)
        # we need a small negative offset otherwise we get some edge artifact
        # from the mp3 decoder.
        af.seek(int(max(0, (seek_time - 0.1)) / stream.time_base), stream=stream)
        frames = []
        length = 0
        for frame in af.decode(streams=stream.index):
            current_offset = int(frame.rate * frame.pts * frame.time_base)
            strip = max(0, frame_offset - current_offset)
            buf = torch.from_numpy(frame.to_ndarray())
            if buf.shape[0] != stream.channels:
                buf = buf.view(-1, stream.channels).t()
            buf = buf[:, strip:]
            frames.append(buf)
            length += buf.shape[1]
            if num_frames > 0 and length >= num_frames:
                break
        assert frames
        # If the above assert fails, it is likely because we seeked past the end of file point,
        # in which case ffmpeg returns a single frame with only zeros, and a weird timestamp.
        # This will need proper debugging, in due time.
        wav = torch.cat(frames, dim=1)
        assert wav.shape[0] == stream.channels
        if num_frames > 0:
            wav = wav[:, :num_frames]
        return f32_pcm(wav), sr
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.decode
def decode(self, codes: torch.Tensor, scale: tp.Optional[torch.Tensor] = None):
        """Decode the given codes to a reconstructed representation, using the scale to perform
        audio denormalization if needed.

        Args:
            codes (torch.Tensor): Int tensor of shape [B, K, T]
            scale (torch.Tensor, optional): Float tensor containing the scale value.

        Returns:
            out (torch.Tensor): Float tensor of shape [B, C, T], the reconstructed audio.
        """
        emb = self.decode_latent(codes)
        out = self.decoder(emb)
        out = self.postprocess(out, scale)
        # out contains extra padding added by the encoder and decoder
        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, make_waveform
def make_waveform(*args, **kwargs):
    # Further remove some warnings.
    be = time.time()
    with warnings.catch_warnings():
        warnings.simplefilter('ignore')
        out = gr.make_waveform(*args, **kwargs)
        print("Make a video took", time.time() - be)
        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, load_audio_meta
def load_audio_meta(path: tp.Union[str, Path],
                    resolve: bool = True, fast: bool = True) -> tp.List[AudioMeta]:
    """Load list of AudioMeta from an optionally compressed json file.

    Args:
        path (str or Path): Path to JSON file.
        resolve (bool): Whether to resolve the path from AudioMeta (default=True).
        fast (bool): activates some tricks to make things faster.
    Returns:
        list of AudioMeta: List of audio file path and its total duration.
    """
    open_fn = gzip.open if str(path).lower().endswith('.gz') else open
    with open_fn(path, 'rb') as fp:  # type: ignore
        lines = fp.readlines()
    meta = []
    for line in lines:
        d = json.loads(line)
        m = AudioMeta.from_dict(d)
        if resolve:
            m = _resolve_audio_meta(m, fast=fast)
        meta.append(m)
    return meta
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, LMModel._init_weights
def _init_weights(self, weight_init: tp.Optional[str], depthwise_init: tp.Optional[str], zero_bias_init: bool):
        """Initialization of the transformer module weights.

        Args:
            weight_init (str, optional): Weight initialization strategy. See ``get_init_fn`` for valid options.
            depthwise_init (str, optional): Depthwise initialization strategy. The following options are valid:
                'current' where the depth corresponds to the current layer index or 'global' where the total number
                of layer is used as depth. If not set, no depthwise initialization strategy is used.
            zero_bias_init (bool): Whether to initialize bias to zero or not.
        """
        assert depthwise_init is None or depthwise_init in ['current', 'global']
        assert depthwise_init is None or weight_init is not None, \
            "If 'depthwise_init' is defined, a 'weight_init' method should be provided."
        assert not zero_bias_init or weight_init is not None, \
            "If 'zero_bias_init', a 'weight_init' method should be provided"

        if weight_init is None:
            return

        for emb_layer in self.emb:
            init_layer(emb_layer, method=weight_init, init_depth=None, zero_bias_init=zero_bias_init)

        for layer_idx, tr_layer in enumerate(self.transformer.layers):
            depth = None
            if depthwise_init == 'current':
                depth = layer_idx + 1
            elif depthwise_init == 'global':
                depth = len(self.transformer.layers)
            init_fn = partial(init_layer, method=weight_init, init_depth=depth, zero_bias_init=zero_bias_init)
            tr_layer.apply(init_fn)

        for linear in self.linears:
            init_layer(linear, method=weight_init, init_depth=None, zero_bias_init=zero_bias_init)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\polynomial_decay_lr_scheduler.py
BlockTypes.METHOD, PolynomialDecayLRScheduler.get_lr
def get_lr(self):
        return [self._get_sched_lr(base_lr, self.last_epoch) for base_lr in self.base_lrs]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\rvm.py
BlockTypes.METHOD, RelativeVolumeMel.__init__
def __init__(self, sample_rate: int = 24000, n_mels: int = 80, n_fft: int = 512,
                 hop_length: int = 128, min_relative_volume: float = -25,
                 max_relative_volume: float = 25, max_initial_gain: float = 25,
                 min_activity_volume: float = -25,
                 num_aggregated_bands: int = 4) -> None:
        super().__init__()
        self.melspec = torchaudio.transforms.MelSpectrogram(
            n_mels=n_mels, n_fft=n_fft, hop_length=hop_length,
            normalized=True, sample_rate=sample_rate, power=2)
        self.min_relative_volume = min_relative_volume
        self.max_relative_volume = max_relative_volume
        self.max_initial_gain = max_initial_gain
        self.min_activity_volume = min_activity_volume
        self.num_aggregated_bands = num_aggregated_bands
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, LMModel.generate
def generate(self,
                 prompt: tp.Optional[torch.Tensor] = None,
                 conditions: tp.List[ConditioningAttributes] = [],
                 num_samples: tp.Optional[int] = None,
                 max_gen_len: int = 256,
                 use_sampling: bool = True,
                 temp: float = 1.0,
                 top_k: int = 250,
                 top_p: float = 0.0,
                 cfg_coef: tp.Optional[float] = None,
                 two_step_cfg: tp.Optional[bool] = None,
                 remove_prompts: bool = False,
                 check: bool = False,
                 callback: tp.Optional[tp.Callable[[int, int], None]] = None) -> torch.Tensor:
        """Generate tokens sampling from the model given a prompt or unconditionally. Generation can
        be perform in a greedy fashion or using sampling with top K and top P strategies.

        Args:
            prompt (torch.Tensor, optional): Prompt tokens of shape [B, K, T].
            conditions_tensors (list of ConditioningAttributes, optional): List of conditions.
            num_samples (int, optional): Number of samples to generate when no prompt and no conditions are given.
            max_gen_len (int): Maximum generation length.
            use_sampling (bool): Whether to use a sampling strategy or not.
            temp (float): Sampling temperature.
            top_k (int): K for "top-k" sampling.
            top_p (float): P for "top-p" sampling.
            cfg_coeff (float, optional): Classifier-free guidance coefficient.
            two_step_cfg (bool, optional): Whether to perform classifier-free guidance with two steps generation.
            remove_prompts (bool): Whether to remove prompts from generation or not.
            check (bool): Whether to apply further checks on generated sequence.
            callback (Callback, optional): Callback function to report generation progress.
        Returns:
            torch.Tensor: Generated tokens.
        """
        assert not self.training, "generation shouldn't be used in training mode."
        first_param = next(iter(self.parameters()))
        device = first_param.device

        # Checking all input shapes are consistent.
        possible_num_samples = []
        if num_samples is not None:
            possible_num_samples.append(num_samples)
        elif prompt is not None:
            possible_num_samples.append(prompt.shape[0])
        elif conditions:
            possible_num_samples.append(len(conditions))
        else:
            possible_num_samples.append(1)
        assert [x == possible_num_samples[0] for x in possible_num_samples], "Inconsistent inputs shapes"
        num_samples = possible_num_samples[0]

        # below we create set of conditions: one conditional and one unconditional
        # to do that we merge the regular condition together with the null condition
        # we then do 1 forward pass instead of 2.
        # the reason for that is two-fold:
        # 1. it is about x2 faster than doing 2 forward passes
        # 2. avoid the streaming API treating the 2 passes as part of different time steps
        # We also support doing two different passes, in particular to ensure that
        # the padding structure is exactly the same between train and test.
        # With a batch size of 1, this can be slower though.
        cfg_conditions: CFGConditions
        two_step_cfg = self.two_step_cfg if two_step_cfg is None else two_step_cfg
        if conditions:
            null_conditions = ClassifierFreeGuidanceDropout(p=1.0)(conditions)
            if two_step_cfg:
                cfg_conditions = (
                    self.condition_provider(self.condition_provider.tokenize(conditions)),
                    self.condition_provider(self.condition_provider.tokenize(null_conditions)),
                )
            else:
                conditions = conditions + null_conditions
                tokenized = self.condition_provider.tokenize(conditions)
                cfg_conditions = self.condition_provider(tokenized)
        else:
            cfg_conditions = {}

        if prompt is None:
            assert num_samples > 0
            prompt = torch.zeros((num_samples, self.num_codebooks, 0), dtype=torch.long, device=device)

        B, K, T = prompt.shape
        start_offset = T
        assert start_offset < max_gen_len

        pattern = self.pattern_provider.get_pattern(max_gen_len)
        # this token is used as default value for codes that are not generated yet
        unknown_token = -1

        # we generate codes up to the max_gen_len that will be mapped to the pattern sequence
        gen_codes = torch.full((B, K, max_gen_len), unknown_token, dtype=torch.long, device=device)
        # filling the gen_codes with the prompt if needed
        gen_codes[..., :start_offset] = prompt
        # create the gen_sequence with proper interleaving from the pattern: [B, K, S]
        gen_sequence, indexes, mask = pattern.build_pattern_sequence(gen_codes, self.special_token_id)
        # retrieve the start_offset in the sequence:
        # it is the first sequence step that contains the `start_offset` timestep
        start_offset_sequence = pattern.get_first_step_with_timesteps(start_offset)
        assert start_offset_sequence is not None

        with self.streaming():
            unconditional_state = self.get_streaming_state()
            prev_offset = 0
            gen_sequence_len = gen_sequence.shape[-1]  # gen_sequence shape is [B, K, S]
            for offset in range(start_offset_sequence, gen_sequence_len):
                # get current sequence (note that the streaming API is providing the caching over previous offsets)
                curr_sequence = gen_sequence[..., prev_offset:offset]
                curr_mask = mask[None, ..., prev_offset:offset].expand(B, -1, -1)
                if check:
                    # check coherence between mask and sequence
                    assert (curr_sequence == torch.where(curr_mask, curr_sequence, self.special_token_id)).all()
                    # should never happen as gen_sequence is filled progressively
                    assert not (curr_sequence == unknown_token).any()
                # sample next token from the model, next token shape is [B, K, 1]
                next_token = self._sample_next_token(
                    curr_sequence, cfg_conditions, unconditional_state, use_sampling, temp, top_k, top_p,
                    cfg_coef=cfg_coef)
                # ensure the tokens that should be masked are properly set to special_token_id
                # as the model never output special_token_id
                valid_mask = mask[..., offset:offset+1].expand(B, -1, -1)
                next_token[~valid_mask] = self.special_token_id
                # ensure we don't overwrite prompt tokens, we only write over unknown tokens
                # (then mask tokens should be left as is as well, which is correct)
                gen_sequence[..., offset:offset+1] = torch.where(
                    gen_sequence[..., offset:offset+1] == unknown_token,
                    next_token, gen_sequence[..., offset:offset+1]
                )
                prev_offset = offset
                if callback is not None:
                    callback(1 + offset - start_offset_sequence, gen_sequence_len - start_offset_sequence)
        unconditional_state.clear()

        # ensure sequence has been entirely filled
        assert not (gen_sequence == unknown_token).any()
        # ensure gen_sequence pattern and mask are matching
        # which means the gen_sequence is valid according to the pattern
        assert (
            gen_sequence == torch.where(mask[None, ...].expand(B, -1, -1), gen_sequence, self.special_token_id)
        ).all()
        # get back the codes, trimming the prompt if needed and cutting potentially incomplete timesteps
        out_codes, out_indexes, out_mask = pattern.revert_pattern_sequence(gen_sequence, special_token=unknown_token)

        # sanity checks over the returned codes and corresponding masks
        assert (out_codes[..., :max_gen_len] != unknown_token).all()
        assert (out_mask[..., :max_gen_len] == 1).all()

        out_start_offset = start_offset if remove_prompts else 0
        out_codes = out_codes[..., out_start_offset:max_gen_len]

        # ensure the returned codes are all valid
        assert (out_codes >= 0).all() and (out_codes <= self.card).all()
        return out_codes
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, StreamableConv1d.forward
def forward(self, x):
        B, C, T = x.shape
        kernel_size = self.conv.conv.kernel_size[0]
        stride = self.conv.conv.stride[0]
        dilation = self.conv.conv.dilation[0]
        kernel_size = (kernel_size - 1) * dilation + 1  # effective kernel size with dilations
        padding_total = kernel_size - stride
        extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)
        if self.causal:
            # Left padding for causal
            x = pad1d(x, (padding_total, extra_padding), mode=self.pad_mode)
        else:
            # Asymmetric padding required for odd strides
            padding_right = padding_total // 2
            padding_left = padding_total - padding_right
            x = pad1d(x, (padding_left, padding_right + extra_padding), mode=self.pad_mode)
        return self.conv(x)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, VectorQuantization._postprocess
def _postprocess(self, quantize):
        if not self.channels_last:
            quantize = rearrange(quantize, "b n d -> b d n")
        return quantize
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, AdversarialLoss._load_from_state_dict
def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):
        # Load optimizer state.
        self.optimizer.load_state_dict(state_dict.pop(prefix + 'optimizer'))
        super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, EmbeddingCache.get_embed_from_cache
def get_embed_from_cache(self, paths: tp.List[Path], x: tp.Any) -> torch.Tensor:
        """Get embedding from cache, computing and storing it to cache if not already cached.
        The EmbeddingCache first tries to load the embedding from the in-memory cache
        containing the pre-computed chunks populated through `populate_embed_cache`.
        If not found, the full embedding is computed and stored on disk to be later accessed
        to populate the in-memory cache, and the desired embedding chunk is extracted and returned.

        Args:
            paths (list[Path or str]): List of paths from where the embeddings can be loaded.
            x (any): Object from which the embedding is extracted.
        """
        embeds = []
        for idx, path in enumerate(paths):
            cache = self._get_cache_path(path)
            if cache in self._current_batch_cache:
                embed = self._current_batch_cache[cache]
            else:
                full_embed = self._compute_embed_fn(path, x, idx)
                try:
                    with flashy.utils.write_and_rename(cache, pid=True) as f:
                        torch.save(full_embed.cpu(), f)
                except Exception as exc:
                    logger.error('Error saving embed %s (%s): %r', cache, full_embed.shape, exc)
                else:
                    logger.info('New embed cache saved: %s (%s)', cache, full_embed.shape)
                    embed = self._extract_embed_fn(full_embed, x, idx)
            embeds.append(embed)
        embed = torch.stack(embeds, dim=0)
        return embed
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.restore
def restore(self, load_best: bool = False, replay_metrics: bool = False,
                ignore_state_keys: tp.List[str] = []) -> bool:
        """Restore the status of a solver for a given xp.

        Args:
            load_best (bool): if `True`, load the best state from the checkpoint.
            replay_metrics (bool): if `True`, logs all the metrics from past epochs.
            ignore_state_keys (list of str): list of sources to ignore when loading the state, e.g. `optimizer`.
        """
        self.logger.info("Restoring weights and history.")
        restored_checkpoints = self.load_checkpoints(load_best, ignore_state_keys)

        self.logger.info("Model hash: %s", model_hash(self.model))

        if replay_metrics and len(self.history) > 0:
            self.logger.info("Replaying past metrics...")
            for epoch, stages in enumerate(self.history):
                for stage_name, metrics in stages.items():
                    # We manually log the metrics summary to the result logger
                    # as we don't want to add them to the pending metrics
                    self.result_logger._log_summary(stage_name, metrics, step=epoch + 1, step_name='epoch',
                                                    formatter=self.get_formatter(stage_name))
        return restored_checkpoints is not None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, LMModel.compute_predictions
def compute_predictions(
            self, codes: torch.Tensor,
            conditions: tp.List[ConditioningAttributes],
            condition_tensors: tp.Optional[ConditionTensors] = None) -> LMOutput:
        """Given an input tensor of codes [B, K, T] and list of conditions, runs the model
        forward using the specified codes interleaving pattern.

        Args:
            codes (torch.Tensor): Input codes of shape [B, K, T] with B the batch size,
                K the number of codebooks and T the number of timesteps.
            conditions (list of ConditioningAttributes): conditionings to use when modeling
                the given codes. Note that when evaluating multiple time with the same conditioning
                you should pre-compute those and pass them as `condition_tensors`.
            condition_tensors (dict[str, ConditionType], optional): pre-computed conditioning
                tensors, see `conditions`.
        Returns:
            LMOutput: Language model outputs
                logits (torch.Tensor) of shape [B, K, T, card] corresponding to the provided codes,
                    i.e. the first item corresponds to logits to predict the first code, meaning that
                    no additional shifting of codes and logits is required.
                mask (torch.Tensor) of shape [B, K, T], mask over valid and invalid positions.
                    Given the specified interleaving strategies, parts of the logits and codes should
                    not be considered as valid predictions because of invalid context.
        """
        B, K, T = codes.shape
        codes = codes.contiguous()
        # map codes [B, K, T] into pattern sequence [B, K, S] using special_token_id for masked tokens
        pattern = self.pattern_provider.get_pattern(T)
        sequence_codes, sequence_indexes, sequence_mask = pattern.build_pattern_sequence(
            codes, self.special_token_id, keep_only_valid_steps=True
        )
        # apply model on pattern sequence
        model = self if self._fsdp is None else self._fsdp
        logits = model(sequence_codes, conditions, condition_tensors)  # [B, K, S, card]
        # map back the logits on pattern sequence to logits on original codes: [B, K, S, card] -> [B, K, T, card]
        # and provide the corresponding mask over invalid positions of tokens
        logits = logits.permute(0, 3, 1, 2)  # [B, card, K, S]
        # note: we use nans as special token to make it obvious if we feed unexpected logits
        logits, logits_indexes, logits_mask = pattern.revert_pattern_logits(
            logits, float('nan'), keep_only_valid_steps=True
        )
        logits = logits.permute(0, 2, 3, 1)  # [B, K, T, card]
        logits_mask = logits_mask[None, :, :].expand(B, -1, -1)  # [K, T] -> [B, K, T]
        return LMOutput(logits, logits_mask)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, LogSTFTMagnitudeLoss.forward
def forward(self, x_mag: torch.Tensor, y_mag: torch.Tensor):
        """Calculate forward propagation.

        Args:
            x_mag (torch.Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).
            y_mag (torch.Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).
        Returns:
            torch.Tensor: Log STFT magnitude loss value.
        """
        return F.l1_loss(torch.log(self.epsilon + y_mag), torch.log(self.epsilon + x_mag))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, UnrolledPatternProvider._build_flattened_codebooks
def _build_flattened_codebooks(self, delays: tp.List[int], flattening: tp.List[int]):
        """Build a flattened codebooks representation as a dictionary of inner step
        and the actual codebook indices corresponding to the flattened codebook. For convenience, we
        also store the delay associated to the flattened codebook to avoid maintaining an extra mapping.
        """
        flattened_codebooks: dict = {}
        for q, (inner_step, delay) in enumerate(zip(flattening, delays)):
            if inner_step not in flattened_codebooks:
                flat_codebook = UnrolledPatternProvider.FlattenedCodebook(codebooks=[q], delay=delay)
            else:
                flat_codebook = flattened_codebooks[inner_step]
                assert flat_codebook.delay == delay, (
                    "Delay and flattening between codebooks is inconsistent: ",
                    "two codebooks flattened to the same position should have the same delay."
                )
                flat_codebook.codebooks.append(q)
            flattened_codebooks[inner_step] = flat_codebook
        return flattened_codebooks
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_musicgen.py
BlockTypes.METHOD, TestMusicGenModel.get_musicgen
def get_musicgen(self):
        mg = MusicGen.get_pretrained(name='debug', device='cpu')
        mg.set_generation_params(duration=2.0, extend_stride=2.)
        return mg
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, model_hash
def model_hash(model: torch.nn.Module) -> str:
    """Return a model hash. This should allow us to track regressions in model init
    from the logs of past experiments.
    """
    hasher = hashlib.sha1()
    for p in model.parameters():
        hasher.update(p.data.cpu().numpy().tobytes())
    return hasher.hexdigest()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\musicgen_pretrained_32khz_eval.py
BlockTypes.METHOD, eval
def eval(launcher, batch_size: int = 32, eval_melody: bool = False):
    opts = {
        'dset': 'audio/musiccaps_32khz',
        'solver/musicgen/evaluation': 'objective_eval',
        'execute_only': 'evaluate',
        '+dataset.evaluate.batch_size': batch_size,
        '+metrics.fad.tf.batch_size': 16,
    }
    # chroma-specific evaluation
    chroma_opts = {
        'dset': 'internal/music_400k_32khz',
        'dataset.evaluate.segment_duration': 30,
        'dataset.evaluate.num_samples': 1000,
        'evaluate.metrics.chroma_cosine': True,
        'evaluate.metrics.fad': False,
        'evaluate.metrics.kld': False,
        'evaluate.metrics.text_consistency': False,
    }
    # binary for FAD computation: replace this path with your own path
    metrics_opts = {
        'metrics.fad.tf.bin': '/data/home/jadecopet/local/usr/opt/google-research'
    }
    opt1 = {'generate.lm.use_sampling': True, 'generate.lm.top_k': 250, 'generate.lm.top_p': 0.}
    opt2 = {'transformer_lm.two_step_cfg': True}

    sub = launcher.bind(opts)
    sub.bind_(metrics_opts)

    # base objective metrics
    sub(opt1, opt2)

    if eval_melody:
        # chroma-specific metrics
        sub(opt1, opt2, chroma_opts)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\loaders.py
BlockTypes.METHOD, load_lm_model_ckpt
def load_lm_model_ckpt(file_or_url_or_id: tp.Union[Path, str], cache_dir: tp.Optional[str] = None):
    return _get_state_dict(file_or_url_or_id, filename="state_dict.bin", cache_dir=cache_dir)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_codebooks_patterns.py
BlockTypes.METHOD, TestPattern.ref_build_pattern_sequence
def ref_build_pattern_sequence(self, z: torch.Tensor, pattern: Pattern, special_token: int):
        """Reference method to build the sequence from the pattern without using fancy scatter."""
        bs, n_q, T = z.shape
        z = z.cpu().numpy()
        assert n_q == pattern.n_q
        assert T <= pattern.timesteps
        inp = torch.full((bs, n_q, len(pattern.layout)), special_token, dtype=torch.long).numpy()
        inp[:] = special_token
        for s, v in enumerate(pattern.layout):
            for (t, q) in v:
                if t < T:
                    inp[:, q, s] = z[:, q, t]
        return torch.from_numpy(inp)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\dadam.py
BlockTypes.METHOD, DAdaptAdam.supports_flat_params
def supports_flat_params(self):
        return True
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, AdversarialLoss.get_adversary_pred
def get_adversary_pred(self, x):
        """Run adversary model, validating expected output format."""
        logits, fmaps = self.adversary(x)
        assert isinstance(logits, list) and all([isinstance(t, torch.Tensor) for t in logits]), \
            f'Expecting a list of tensors as logits but {type(logits)} found.'
        assert isinstance(fmaps, list), f'Expecting a list of features maps but {type(fmaps)} found.'
        for fmap in fmaps:
            assert isinstance(fmap, list) and all([isinstance(f, torch.Tensor) for f in fmap]), \
                f'Expecting a list of tensors as feature maps but {type(fmap)} found.'
        return logits, fmaps
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset._audio_read
def _audio_read(self, path: str, seek_time: float = 0, duration: float = -1):
        # Override this method in subclass if needed.
        if self.load_wav:
            return audio_read(path, seek_time, duration, pad=False)
        else:
            assert self.segment_duration is not None
            n_frames = int(self.sample_rate * self.segment_duration)
            return torch.zeros(self.channels, n_frames), self.sample_rate
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\compression\encodec_audiogen_16khz.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=8, partition=partitions)
    # use configuration for AudioGen's EnCodec model trained on monophonic audio sampled at 16 kHz
    # AudioGen's EnCodec is trained with a total stride of 320 leading to a frame rate of 50 hz
    launcher.bind_(solver='compression/encodec_audiogen_16khz')
    # replace this by the desired sound dataset
    launcher.bind_(dset='internal/sounds_16khz')
    # launch xp
    launcher()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, DiffusionProcess.__init__
def __init__(self, model: DiffusionUnet, noise_schedule: NoiseSchedule) -> None:
        """
        """
        self.model = model
        self.schedule = noise_schedule
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\mos.py
BlockTypes.METHOD, get_full_path
def get_full_path(normalized_path: Path):
    """Revert `normalize_path`.
    """
    return train.main.dora.dir.resolve() / 'xps' / normalized_path
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._get_text_embedding
def _get_text_embedding(self, x: JointEmbedCondition) -> torch.Tensor:
        """Get CLAP embedding from a batch of text descriptions."""
        no_nullified_cond = x.wav.shape[-1] > 1  # we don't want to read from cache when condition dropout
        if self.text_cache is not None and no_nullified_cond:
            assert all(p is not None for p in x.path), "Cache requires all JointEmbedCondition paths to be provided"
            paths = [Path(p) for p in x.path if p is not None]
            embed = self.text_cache.get_embed_from_cache(paths, x)
        else:
            text = [xi if xi is not None else "" for xi in x.text]
            embed = self._compute_text_embedding(text)
        if self.normalize:
            embed = torch.nn.functional.normalize(embed, p=2.0, dim=-1)
        return embed
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\fsdp.py
BlockTypes.METHOD, is_fsdp_used
def is_fsdp_used() -> bool:
    """Return whether we are using FSDP."""
    # A bit of a hack but should work from anywhere.
    if dora.is_xp():
        cfg = dora.get_xp().cfg
        if hasattr(cfg, 'fsdp'):
            return cfg.fsdp.use
    return False
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cluster.py
BlockTypes.METHOD, _guess_cluster_type
def _guess_cluster_type() -> ClusterType:
    uname = os.uname()
    fqdn = socket.getfqdn()
    if uname.sysname == "Linux" and (uname.release.endswith("-aws") or ".ec2" in fqdn):
        return ClusterType.AWS

    if fqdn.endswith(".fair"):
        return ClusterType.FAIR

    if fqdn.endswith(".facebook.com"):
        return ClusterType.RSC

    if uname.sysname == "Darwin":
        return ClusterType.LOCAL_DARWIN

    return ClusterType.DEFAULT
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, SoundInfo.has_sound_meta
def has_sound_meta(self) -> bool:
        return self.description is not None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, NormConv1d.forward
def forward(self, x):
        x = self.conv(x)
        x = self.norm(x)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._preprocess_wav
def _preprocess_wav(self, wav: torch.Tensor, length: torch.Tensor, sample_rates: tp.List[int]) -> torch.Tensor:
        """Preprocess wav to expected format by CLAP model.

        Args:
            wav (torch.Tensor): Audio wav, of shape [B, C, T].
            length (torch.Tensor): Actual length of the audio for each item in the batch, of shape [B].
            sample_rates (list[int]): Sample rates for each sample in the batch
        Returns:
            torch.Tensor: Audio wav of shape [B, T].
        """
        assert wav.dim() == 3, "Expecting wav to be [B, C, T]"
        if sample_rates is not None:
            _wav = []
            for i, audio in enumerate(wav):
                sr = sample_rates[i]
                audio = convert_audio(audio, from_rate=sr, to_rate=self.clap_sample_rate, to_channels=1)
                _wav.append(audio)
            wav = torch.stack(_wav, dim=0)
        wav = wav.mean(dim=1)
        return wav
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, SoundDataset.__getitem__
def __getitem__(self, index):
        wav, info = super().__getitem__(index)
        info_data = info.to_dict()
        info_path = self._get_info_path(info.meta.path)
        if Path(info_path).exists():
            with open(info_path, 'r') as json_file:
                sound_data = json.load(json_file)
                sound_data.update(info_data)
                sound_info = SoundInfo.from_dict(sound_data, fields_required=self.info_fields_required)
                # if there are multiple descriptions, sample one randomly
                if isinstance(sound_info.description, list):
                    sound_info.description = random.choice(sound_info.description)
        else:
            sound_info = SoundInfo.from_dict(info_data, fields_required=False)

        sound_info.self_wav = WavCondition(
            wav=wav[None], length=torch.tensor([info.n_frames]),
            sample_rate=[sound_info.sample_rate], path=[info.meta.path], seek_time=[info.seek_time])

        return wav, sound_info
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.show
def show(self):
        """Show the compression model and employed adversarial loss."""
        self.logger.info(f"Compression model with {self.model.quantizer.total_codebooks} codebooks:")
        self.log_model_summary(self.model)
        self.logger.info("Adversarial loss:")
        self.log_model_summary(self.adv_losses)
        self.logger.info("Auxiliary losses:")
        self.logger.info(self.aux_losses)
        self.logger.info("Info losses:")
        self.logger.info(self.info_losses)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\dadam.py
BlockTypes.METHOD, DAdaptAdam.step
def step(self, closure=None):
        """Performs a single optimization step.

        Args:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            loss = closure()

        g_sq = 0.0
        sksq_weighted = 0.0
        sk_l1 = 0.0

        lr = max(group['lr'] for group in self.param_groups)

        group = self.param_groups[0]
        gsq_weighted = group['gsq_weighted']
        d = group['d']
        dlr = d*lr

        growth_rate = group['growth_rate']
        decouple = group['decouple']
        fsdp_in_use = group['fsdp_in_use']
        log_every = group['log_every']

        beta1, beta2 = group['betas']

        for group in self.param_groups:
            group_lr = group['lr']
            decay = group['weight_decay']
            k = group['k']
            eps = group['eps']

            if group_lr not in [lr, 0.0]:
                raise RuntimeError("Setting different lr values in different parameter "
                                   "groups is only supported for values of 0")

            for p in group['params']:
                if p.grad is None:
                    continue
                if hasattr(p, "_fsdp_flattened"):
                    fsdp_in_use = True
                grad = p.grad.data

                # Apply weight decay (coupled variant)
                if decay != 0 and not decouple:
                    grad.add_(p.data, alpha=decay)

                state = self.state[p]

                # State initialization
                if 'step' not in state:
                    state['step'] = 0
                    state['s'] = torch.zeros_like(p.data, memory_format=torch.preserve_format).detach()
                    # Exponential moving average of gradient values
                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format).detach()
                    # Exponential moving average of squared gradient values
                    state['exp_avg_sq'] = torch.zeros_like(
                        to_real(p.data), memory_format=torch.preserve_format).detach()

                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']

                grad_grad = to_real(grad * grad.conj())

                # Adam EMA updates
                if group_lr > 0:
                    exp_avg.mul_(beta1).add_(grad, alpha=dlr*(1-beta1))
                    exp_avg_sq.mul_(beta2).add_(grad_grad, alpha=1-beta2)

                    denom = exp_avg_sq.sqrt().add_(eps)

                    g_sq += grad_grad.div_(denom).sum().item()

                    s = state['s']
                    s.mul_(beta2).add_(grad, alpha=dlr*(1-beta2))
                    sksq_weighted += to_real(s * s.conj()).div_(denom).sum().item()
                    sk_l1 += s.abs().sum().item()

            ######

        gsq_weighted = beta2*gsq_weighted + g_sq*(dlr**2)*(1-beta2)
        d_hat = d

        # if we have not done any progres, return
        # if we have any gradients available, will have sk_l1 > 0 (unless \|g\|=0)
        if sk_l1 == 0:
            return loss

        if lr > 0.0:
            if fsdp_in_use:
                dist_tensor = torch.zeros(3, device='cuda')
                dist_tensor[0] = sksq_weighted
                dist_tensor[1] = gsq_weighted
                dist_tensor[2] = sk_l1
                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)
                global_sksq_weighted = dist_tensor[0]
                global_gsq_weighted = dist_tensor[1]
                global_sk_l1 = dist_tensor[2]
            else:
                global_sksq_weighted = sksq_weighted
                global_gsq_weighted = gsq_weighted
                global_sk_l1 = sk_l1

            d_hat = (global_sksq_weighted/(1-beta2) - global_gsq_weighted)/global_sk_l1
            d = max(d, min(d_hat, d*growth_rate))

        if log_every > 0 and k % log_every == 0:
            logger.info(
                f"(k={k}) dlr: {dlr:1.1e} d_hat: {d_hat:1.1e}, d: {d:1.8}. "
                f"sksq_weighted={global_sksq_weighted:1.1e} gsq_weighted={global_gsq_weighted:1.1e} "
                f"sk_l1={global_sk_l1:1.1e}{' (FSDP)' if fsdp_in_use else ''}")

        for group in self.param_groups:
            group['gsq_weighted'] = gsq_weighted
            group['d'] = d

            group_lr = group['lr']
            decay = group['weight_decay']
            k = group['k']
            eps = group['eps']

            for p in group['params']:
                if p.grad is None:
                    continue
                grad = p.grad.data

                state = self.state[p]

                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']

                state['step'] += 1

                denom = exp_avg_sq.sqrt().add_(eps)
                denom = denom.type(p.type())

                # Apply weight decay (decoupled variant)
                if decay != 0 and decouple and group_lr > 0:
                    p.data.add_(p.data, alpha=-decay * dlr)

                # Take step
                p.data.addcdiv_(exp_avg, denom, value=-1)

            group['k'] = k + 1

        return loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.evaluate
def evaluate(self):
        """Evaluate stage. Runs audio reconstruction evaluation."""
        self.model.eval()
        evaluate_stage_name = str(self.current_stage)

        loader = self.dataloaders['evaluate']
        updates = len(loader)
        lp = self.log_progress(f'{evaluate_stage_name} inference', loader, total=updates, updates=self.log_updates)
        average = flashy.averager()

        pendings = []
        ctx = multiprocessing.get_context('spawn')
        with get_pool_executor(self.cfg.evaluate.num_workers, mp_context=ctx) as pool:
            for idx, batch in enumerate(lp):
                x = batch.to(self.device)
                with torch.no_grad():
                    qres = self.model(x)

                y_pred = qres.x.cpu()
                y = batch.cpu()  # should already be on CPU but just in case
                pendings.append(pool.submit(evaluate_audio_reconstruction, y_pred, y, self.cfg))

            metrics_lp = self.log_progress(f'{evaluate_stage_name} metrics', pendings, updates=self.log_updates)
            for pending in metrics_lp:
                metrics = pending.result()
                metrics = average(metrics)

        metrics = flashy.distrib.average_metrics(metrics, len(loader))
        return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio.py
BlockTypes.METHOD, audio_write
def audio_write(stem_name: tp.Union[str, Path],
                wav: torch.Tensor, sample_rate: int,
                format: str = 'wav', mp3_rate: int = 320, normalize: bool = True,
                strategy: str = 'peak', peak_clip_headroom_db: float = 1,
                rms_headroom_db: float = 18, loudness_headroom_db: float = 14,
                loudness_compressor: bool = False,
                log_clipping: bool = True, make_parent_dir: bool = True,
                add_suffix: bool = True) -> Path:
    """Convenience function for saving audio to disk. Returns the filename the audio was written to.

    Args:
        stem_name (str or Path): Filename without extension which will be added automatically.
        wav (torch.Tensor): Audio data to save.
        sample_rate (int): Sample rate of audio data.
        format (str): Either "wav" or "mp3".
        mp3_rate (int): kbps when using mp3s.
        normalize (bool): if `True` (default), normalizes according to the prescribed
            strategy (see after). If `False`, the strategy is only used in case clipping
            would happen.
        strategy (str): Can be either 'clip', 'peak', or 'rms'. Default is 'peak',
            i.e. audio is normalized by its largest value. RMS normalizes by root-mean-square
            with extra headroom to avoid clipping. 'clip' just clips.
        peak_clip_headroom_db (float): Headroom in dB when doing 'peak' or 'clip' strategy.
        rms_headroom_db (float): Headroom in dB when doing 'rms' strategy. This must be much larger
            than the `peak_clip` one to avoid further clipping.
        loudness_headroom_db (float): Target loudness for loudness normalization.
        loudness_compressor (bool): Uses tanh for soft clipping when strategy is 'loudness'.
         when strategy is 'loudness' log_clipping (bool): If True, basic logging on stderr when clipping still
            occurs despite strategy (only for 'rms').
        make_parent_dir (bool): Make parent directory if it doesn't exist.
    Returns:
        Path: Path of the saved audio.
    """
    assert wav.dtype.is_floating_point, "wav is not floating point"
    if wav.dim() == 1:
        wav = wav[None]
    elif wav.dim() > 2:
        raise ValueError("Input wav should be at most 2 dimension.")
    assert wav.isfinite().all()
    wav = normalize_audio(wav, normalize, strategy, peak_clip_headroom_db,
                          rms_headroom_db, loudness_headroom_db, loudness_compressor,
                          log_clipping=log_clipping, sample_rate=sample_rate,
                          stem_name=str(stem_name))
    kwargs: dict = {}
    if format == 'mp3':
        suffix = '.mp3'
        kwargs.update({"compression": mp3_rate})
    elif format == 'wav':
        wav = i16_pcm(wav)
        suffix = '.wav'
        kwargs.update({"encoding": "PCM_S", "bits_per_sample": 16})
    else:
        raise RuntimeError(f"Invalid format {format}. Only wav or mp3 are supported.")
    if not add_suffix:
        suffix = ''
    path = Path(str(stem_name) + suffix)
    if make_parent_dir:
        path.parent.mkdir(exist_ok=True, parents=True)
    try:
        ta.save(path, wav, sample_rate, **kwargs)
    except Exception:
        if path.exists():
            # we do not want to leave half written files around.
            path.unlink()
        raise
    return path
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\info_audio_dataset.py
BlockTypes.METHOD, clusterify_all_meta
def clusterify_all_meta(meta: tp.List[AudioMeta]) -> tp.List[AudioMeta]:
    """Monkey-patch all meta to match cluster specificities."""
    return [_clusterify_meta(m) for m in meta]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\compression\_explorers.py
BlockTypes.METHOD, CompressionExplorer.stages
def stages(self):
        return ["train", "valid", "evaluate"]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningAttributes.__getitem__
def __getitem__(self, item):
        return getattr(self, item)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, get_musical_key
def get_musical_key(value: tp.Optional[str]) -> tp.Optional[str]:
    """Preprocess key keywords, discarding them if there are multiple key defined."""
    if value is None or (not isinstance(value, str)) or len(value) == 0 or value == 'None':
        return None
    elif ',' in value:
        # For now, we discard when multiple keys are defined separated with comas
        return None
    else:
        return value.strip().lower()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, exists
def exists(val: tp.Optional[tp.Any]) -> bool:
    return val is not None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestAdversarialLoss.test_adversarial_feat_loss
def test_adversarial_feat_loss(self):
        adv = MultiScaleDiscriminator()
        optimizer = torch.optim.Adam(
            adv.parameters(),
            lr=1e-4,
        )
        loss, loss_real, loss_fake = get_adv_criterion('mse'), get_real_criterion('mse'), get_fake_criterion('mse')
        feat_loss = FeatureMatchingLoss()
        adv_loss = AdversarialLoss(adv, optimizer, loss, loss_real, loss_fake, feat_loss)

        B, C, T = 4, 1, random.randint(1000, 5000)
        real = torch.randn(B, C, T)
        fake = torch.randn(B, C, T)

        loss, loss_feat = adv_loss(fake, real)

        assert isinstance(loss, torch.Tensor) and isinstance(loss.item(), float)
        assert isinstance(loss_feat, torch.Tensor) and isinstance(loss.item(), float)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, VectorQuantization.encode
def encode(self, x):
        x = self._preprocess(x)
        x = self.project_in(x)
        embed_in = self._codebook.encode(x)
        return embed_in
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, SoundInfo.to_condition_attributes
def to_condition_attributes(self) -> ConditioningAttributes:
        out = ConditioningAttributes()

        for _field in fields(self):
            key, value = _field.name, getattr(self, _field.name)
            if key == 'self_wav':
                out.wav[key] = value
            else:
                out.text[key] = value
        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\streaming.py
BlockTypes.METHOD, StreamingModule.__init__
def __init__(self) -> None:
        super().__init__()
        self._streaming_state: State = {}
        self._is_streaming = False
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.set_generation_params
def set_generation_params(self, use_sampling: bool = True, top_k: int = 250,
                              top_p: float = 0.0, temperature: float = 1.0,
                              duration: float = 30.0, cfg_coef: float = 3.0,
                              two_step_cfg: bool = False, extend_stride: float = 18):
        """Set the generation parameters for MusicGen.

        Args:
            use_sampling (bool, optional): Use sampling if True, else do argmax decoding. Defaults to True.
            top_k (int, optional): top_k used for sampling. Defaults to 250.
            top_p (float, optional): top_p used for sampling, when set to 0 top_k is used. Defaults to 0.0.
            temperature (float, optional): Softmax temperature parameter. Defaults to 1.0.
            duration (float, optional): Duration of the generated waveform. Defaults to 30.0.
            cfg_coef (float, optional): Coefficient used for classifier free guidance. Defaults to 3.0.
            two_step_cfg (bool, optional): If True, performs 2 forward for Classifier Free Guidance,
                instead of batching together the two. This has some impact on how things
                are padded but seems to have little impact in practice.
            extend_stride: when doing extended generation (i.e. more than 30 seconds), by how much
                should we extend the audio each time. Larger values will mean less context is
                preserved, and shorter value will require extra computations.
        """
        assert extend_stride < self.max_duration, "Cannot stride by more than max generation duration."
        self.extend_stride = extend_stride
        self.duration = duration
        self.generation_params = {
            'use_sampling': use_sampling,
            'temp': temperature,
            'top_k': top_k,
            'top_p': top_p,
            'cfg_coef': cfg_coef,
            'two_step_cfg': two_step_cfg,
        }
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, T5Conditioner.forward
def forward(self, inputs: tp.Dict[str, torch.Tensor]) -> ConditionType:
        mask = inputs['attention_mask']
        with torch.set_grad_enabled(self.finetune), self.autocast:
            embeds = self.t5(**inputs).last_hidden_state
        embeds = self.output_proj(embeds.to(self.output_proj.weight))
        embeds = (embeds * mask.unsqueeze(-1))
        return embeds, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_ema
def get_ema(module_dict: nn.ModuleDict, cfg: omegaconf.DictConfig) -> tp.Optional[optim.ModuleDictEMA]:
    """Initialize Exponential Moving Average.

    Args:
        module_dict (nn.ModuleDict): ModuleDict for which to compute the EMA.
        cfg (omegaconf.DictConfig): Optim EMA configuration.
    Returns:
        optim.ModuleDictEMA: EMA version of the ModuleDict.
    """
    kw: tp.Dict[str, tp.Any] = dict(cfg)
    use = kw.pop('use', False)
    decay = kw.pop('decay', None)
    device = kw.pop('device', None)
    if not use:
        return None
    if len(module_dict) == 0:
        raise ValueError("Trying to build EMA but an empty module_dict source is provided!")
    ema_module = optim.ModuleDictEMA(module_dict, decay=decay, device=device)
    return ema_module
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\mos.py
BlockTypes.METHOD, get_signature
def get_signature(xps: tp.List[str]):
    """Return a signature for a list of XP signatures.
    """
    return sha1(json.dumps(xps).encode()).hexdigest()[:10]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_musicgen.py
BlockTypes.METHOD, TestMusicGenModel.test_base
def test_base(self):
        mg = self.get_musicgen()
        assert mg.frame_rate == 25
        assert mg.sample_rate == 32000
        assert mg.audio_channels == 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, LMModel.special_token_id
def special_token_id(self) -> int:
        return self.card
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, LayerScale.__init__
def __init__(self, channels: int, init: float = 1e-4, channel_last: bool = True,
                 device=None, dtype=None):
        super().__init__()
        self.channel_last = channel_last
        self.scale = nn.Parameter(
            torch.full((channels,), init,
                       requires_grad=True, device=device, dtype=dtype))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\export_legacy.py
BlockTypes.METHOD, export_lm
def export_lm(checkpoint_path: tp.Union[Path, str], out_folder: tp.Union[Path, str]):
    sig = Path(checkpoint_path).parent.name
    assert len(sig) == 8, "Not a valid Dora signature"
    pkg = torch.load(checkpoint_path, 'cpu')
    new_pkg = {
        'best_state': pkg['fsdp_best_state']['model'],
        'xp.cfg': OmegaConf.to_yaml(_clean_lm_cfg(pkg['xp.cfg']))
    }
    out_file = Path(out_folder) / f'{sig}.th'
    torch.save(new_pkg, out_file)
    return out_file
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\compression\_explorers.py
BlockTypes.METHOD, CompressionExplorer.get_grid_meta
def get_grid_meta(self):
        """Returns the list of Meta information to display for each XP/job.
        """
        return [
            tt.leaf("index", align=">"),
            tt.leaf("name", wrap=140),
            tt.leaf("state"),
            tt.leaf("sig", align=">"),
        ]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningAttributes.text_attributes
def text_attributes(self):
        return self.text.keys()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\ema.py
BlockTypes.METHOD, ModuleDictEMA.state_dict
def state_dict(self):
        return {'state': self.state, 'count': self.count}
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, default
def default(val: tp.Any, d: tp.Any) -> tp.Any:
    return val if exists(val) else d
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset.__getitem__
def __getitem__(self, index: int) -> tp.Union[torch.Tensor, tp.Tuple[torch.Tensor, SegmentInfo]]:
        if self.segment_duration is None:
            file_meta = self.meta[index]
            out, sr = audio_read(file_meta.path)
            out = convert_audio(out, sr, self.sample_rate, self.channels)
            n_frames = out.shape[-1]
            segment_info = SegmentInfo(file_meta, seek_time=0., n_frames=n_frames, total_frames=n_frames,
                                       sample_rate=self.sample_rate, channels=out.shape[0])
        else:
            rng = torch.Generator()
            if self.shuffle:
                # We use index, plus extra randomness, either totally random if we don't know the epoch.
                # otherwise we make use of the epoch number and optional shuffle_seed.
                if self.current_epoch is None:
                    rng.manual_seed(index + self.num_samples * random.randint(0, 2**24))
                else:
                    rng.manual_seed(index + self.num_samples * (self.current_epoch + self.shuffle_seed))
            else:
                # We only use index
                rng.manual_seed(index)

            for retry in range(self.max_read_retry):
                file_meta = self.sample_file(index, rng)
                # We add some variance in the file position even if audio file is smaller than segment
                # without ending up with empty segments
                max_seek = max(0, file_meta.duration - self.segment_duration * self.min_segment_ratio)
                seek_time = torch.rand(1, generator=rng).item() * max_seek
                try:
                    out, sr = audio_read(file_meta.path, seek_time, self.segment_duration, pad=False)
                    out = convert_audio(out, sr, self.sample_rate, self.channels)
                    n_frames = out.shape[-1]
                    target_frames = int(self.segment_duration * self.sample_rate)
                    if self.pad:
                        out = F.pad(out, (0, target_frames - n_frames))
                    segment_info = SegmentInfo(file_meta, seek_time, n_frames=n_frames, total_frames=target_frames,
                                               sample_rate=self.sample_rate, channels=out.shape[0])
                except Exception as exc:
                    logger.warning("Error opening file %s: %r", file_meta.path, exc)
                    if retry == self.max_read_retry - 1:
                        raise
                else:
                    break

        if self.return_info:
            # Returns the wav and additional information on the wave segment
            return out, segment_info
        else:
            return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, STFTLosses.__init__
def __init__(self, n_fft: int = 1024, hop_length: int = 120, win_length: int = 600,
                 window: str = "hann_window", normalized: bool = False,
                 epsilon: float = torch.finfo(torch.float32).eps):
        super().__init__()
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.win_length = win_length
        self.normalized = normalized
        self.register_buffer("window", getattr(torch, window)(win_length))
        self.spectral_convergenge_loss = SpectralConvergenceLoss(epsilon)
        self.log_stft_magnitude_loss = LogSTFTMagnitudeLoss(epsilon)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, EncodecModel.decode_latent
def decode_latent(self, codes: torch.Tensor):
        """Decode from the discrete codes to continuous latent space."""
        return self.quantizer.decode(codes)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, DiffusionProcess.generate
def generate(self, condition: torch.Tensor, initial_noise: torch.Tensor,
                 step_list: tp.Optional[tp.List[int]] = None):
        """Perform one diffusion process to generate one of the bands.

        Args:
            condition (tensor): The embeddings form the compression model.
            initial_noise (tensor): The initial noise to start the process/
        """
        return self.schedule.generate_subsampled(model=self.model, initial=initial_noise, step_list=step_list,
                                                 condition=condition)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_visqol
def get_visqol(cfg: omegaconf.DictConfig) -> metrics.ViSQOL:
    """Instantiate ViSQOL metric from config."""
    kwargs = dict_from_config(cfg)
    return metrics.ViSQOL(**kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\musicgen.py
BlockTypes.METHOD, MusicGenSolver.run_step
def run_step(self, idx: int, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]], metrics: dict) -> dict:
        """Perform one training or valid step on a given batch."""
        check_synchronization_points = idx == 1 and self.device == 'cuda'

        condition_tensors, audio_tokens, padding_mask = self._prepare_tokens_and_attributes(
            batch, check_synchronization_points)

        self.deadlock_detect.update('tokens_and_conditions')

        if check_synchronization_points:
            torch.cuda.set_sync_debug_mode('warn')

        with self.autocast:
            model_output = self.model.compute_predictions(audio_tokens, [], condition_tensors)  # type: ignore
            logits = model_output.logits
            mask = padding_mask & model_output.mask
            ce, ce_per_codebook = self._compute_cross_entropy(logits, audio_tokens, mask)
            loss = ce
        self.deadlock_detect.update('loss')

        if check_synchronization_points:
            torch.cuda.set_sync_debug_mode('default')

        if self.is_training:
            metrics['lr'] = self.optimizer.param_groups[0]['lr']
            if self.scaler is not None:
                loss = self.scaler.scale(loss)
            self.deadlock_detect.update('scale')
            if self.cfg.fsdp.use:
                loss.backward()
                flashy.distrib.average_tensors(self.model.buffers())
            elif self.cfg.optim.eager_sync:
                with flashy.distrib.eager_sync_model(self.model):
                    loss.backward()
            else:
                # this should always be slower but can be useful
                # for weird use cases like multiple backwards.
                loss.backward()
                flashy.distrib.sync_model(self.model)
            self.deadlock_detect.update('backward')

            if self.scaler is not None:
                self.scaler.unscale_(self.optimizer)
            if self.cfg.optim.max_norm:
                if self.cfg.fsdp.use:
                    metrics['grad_norm'] = self.model.clip_grad_norm_(self.cfg.optim.max_norm)  # type: ignore
                else:
                    metrics['grad_norm'] = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), self.cfg.optim.max_norm
                    )
            if self.scaler is None:
                self.optimizer.step()
            else:
                self.scaler.step(self.optimizer)
                self.scaler.update()
            if self.lr_scheduler:
                self.lr_scheduler.step()
            self.optimizer.zero_grad()
            self.deadlock_detect.update('optim')
            if self.scaler is not None:
                scale = self.scaler.get_scale()
                metrics['grad_scale'] = scale
            if not loss.isfinite().all():
                raise RuntimeError("Model probably diverged.")

        metrics['ce'] = ce
        metrics['ppl'] = torch.exp(ce)
        for k, ce_q in enumerate(ce_per_codebook):
            metrics[f'ce_q{k + 1}'] = ce_q
            metrics[f'ppl_q{k + 1}'] = torch.exp(ce_q)

        return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_lm_model
def get_lm_model(cfg: omegaconf.DictConfig) -> LMModel:
    """Instantiate a transformer LM."""
    if cfg.lm_model == 'transformer_lm':
        kwargs = dict_from_config(getattr(cfg, 'transformer_lm'))
        n_q = kwargs['n_q']
        q_modeling = kwargs.pop('q_modeling', None)
        codebooks_pattern_cfg = getattr(cfg, 'codebooks_pattern')
        attribute_dropout = dict_from_config(getattr(cfg, 'attribute_dropout'))
        cls_free_guidance = dict_from_config(getattr(cfg, 'classifier_free_guidance'))
        cfg_prob, cfg_coef = cls_free_guidance['training_dropout'], cls_free_guidance['inference_coef']
        fuser = get_condition_fuser(cfg)
        condition_provider = get_conditioner_provider(kwargs["dim"], cfg).to(cfg.device)
        if len(fuser.fuse2cond['cross']) > 0:  # enforce cross-att programmatically
            kwargs['cross_attention'] = True
        if codebooks_pattern_cfg.modeling is None:
            assert q_modeling is not None, \
                "LM model should either have a codebook pattern defined or transformer_lm.q_modeling"
            codebooks_pattern_cfg = omegaconf.OmegaConf.create(
                {'modeling': q_modeling, 'delay': {'delays': list(range(n_q))}}
            )
        pattern_provider = get_codebooks_pattern_provider(n_q, codebooks_pattern_cfg)
        return LMModel(
            pattern_provider=pattern_provider,
            condition_provider=condition_provider,
            fuser=fuser,
            cfg_dropout=cfg_prob,
            cfg_coef=cfg_coef,
            attribute_dropout=attribute_dropout,
            dtype=getattr(torch, cfg.dtype),
            device=cfg.device,
            **kwargs
        ).to(cfg.device)
    else:
        raise KeyError(f"Unexpected LM model {cfg.lm_model}")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, EuclideanCodebook.init_embed_
def init_embed_(self, data):
        if self.inited:
            return

        embed, cluster_size = kmeans(data, self.codebook_size, self.kmeans_iters)
        self.embed.data.copy_(embed)
        self.embed_avg.data.copy_(embed.clone())
        self.cluster_size.data.copy_(cluster_size)
        self.inited.data.copy_(torch.Tensor([True]))
        # Make sure all buffers across workers are in sync after initialization
        flashy.distrib.broadcast_tensors(self.buffers())
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, load_model
def load_model(version='facebook/musicgen-melody'):
    global MODEL
    print("Loading model", version)
    if MODEL is None or MODEL.name != version:
        MODEL = MusicGen.get_pretrained(version)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\builders.py
BlockTypes.METHOD, get_debug_compression_model
def get_debug_compression_model(device='cpu', sample_rate: int = 32000):
    """Instantiate a debug compression model to be used for unit tests."""
    assert sample_rate in [16000, 32000], "unsupported sample rate for debug compression model"
    model_ratios = {
        16000: [10, 8, 8],  # 25 Hz at 16kHz
        32000: [10, 8, 16]  # 25 Hz at 32kHz
    }
    ratios: tp.List[int] = model_ratios[sample_rate]
    frame_rate = 25
    seanet_kwargs: dict = {
        'n_filters': 4,
        'n_residual_layers': 1,
        'dimension': 32,
        'ratios': ratios,
    }
    print(seanet_kwargs)
    encoder = audiocraft.modules.SEANetEncoder(**seanet_kwargs)
    decoder = audiocraft.modules.SEANetDecoder(**seanet_kwargs)
    quantizer = qt.ResidualVectorQuantizer(dimension=32, bins=400, n_q=4)
    init_x = torch.randn(8, 32, 128)
    quantizer(init_x, 1)  # initialize kmeans etc.
    compression_model = EncodecModel(
        encoder, decoder, quantizer,
        frame_rate=frame_rate, sample_rate=sample_rate, channels=1).to(device)
    return compression_model.eval()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\audiogen\audiogen_pretrained_16khz_eval.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=4, partition=partitions)

    if 'REGEN' not in os.environ:
        folder = train.main.dora.dir / 'grids' / __name__.split('.', 2)[-1]
        with launcher.job_array():
            for sig in folder.iterdir():
                if not sig.is_symlink():
                    continue
                xp = train.main.get_xp_from_sig(sig.name)
                launcher(xp.argv)
        return

    audiogen_base = launcher.bind(solver="audiogen/audiogen_base_16khz")
    audiogen_base.bind_({'autocast': False, 'fsdp.use': True})

    audiogen_base_medium = audiogen_base.bind({'continue_from': '//pretrained/facebook/audiogen-medium'})
    audiogen_base_medium.bind_({'model/lm/model_scale': 'medium'})
    eval(audiogen_base_medium, batch_size=128)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\environment.py
BlockTypes.METHOD, AudioCraftEnvironment.apply_dataset_mappers
def apply_dataset_mappers(cls, path: str) -> str:
        """Applies dataset mapping regex rules as defined in the configuration.
        If no rules are defined, the path is returned as-is.
        """
        instance = cls.instance()

        for pattern, repl in instance._dataset_mappers:
            path = pattern.sub(repl, path)

        return path
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\checkpoint.py
BlockTypes.METHOD, save_checkpoint
def save_checkpoint(state: tp.Any, checkpoint_path: Path, is_sharded: bool = False) -> None:
    """Save state to disk to the specified checkpoint_path."""
    _safe_save_checkpoint(state, checkpoint_path, is_sharded)
    logger.info("Checkpoint saved to %s", checkpoint_path)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestInfo.test_info_wav
def test_info_wav(self):
        self._test_info_format('.wav')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_audiogen.py
BlockTypes.METHOD, TestAudioGenModel.test_generate
def test_generate(self):
        ag = self.get_audiogen()
        wav = ag.generate(
            ['youpi', 'lapin dort'])
        assert list(wav.shape) == [2, 1, 32000]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\ema.py
BlockTypes.METHOD, ModuleDictEMA.load_state_dict
def load_state_dict(self, state):
        self.count = state['count']
        for module_name, module in state['state'].items():
            for key, val in module.items():
                self.state[module_name][key].copy_(val)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\rvm.py
BlockTypes.METHOD, RelativeVolumeMel.forward
def forward(self, estimate: torch.Tensor, ground_truth: torch.Tensor) -> tp.Dict[str, torch.Tensor]:
        """Compute RVM metric between estimate and reference samples.

        Args:
            estimate (torch.Tensor): Estimate sample.
            ground_truth (torch.Tensor): Reference sample.

        Returns:
            dict[str, torch.Tensor]: Metrics with keys `rvm` for the overall average, and `rvm_{k}`
            for the RVM over the k-th band (k=0..num_aggregated_bands - 1).
        """
        min_scale = db_to_scale(-self.max_initial_gain)
        std = ground_truth.pow(2).mean().sqrt().clamp(min=min_scale)
        z_gt = self.melspec(ground_truth / std).sqrt()
        z_est = self.melspec(estimate / std).sqrt()

        delta = z_gt - z_est
        ref_db = scale_to_db(z_gt, self.min_activity_volume)
        delta_db = scale_to_db(delta.abs(), min_volume=-120)
        relative_db = (delta_db - ref_db).clamp(self.min_relative_volume, self.max_relative_volume)
        dims = list(range(relative_db.dim()))
        dims.remove(dims[-2])
        losses_per_band = relative_db.mean(dim=dims)
        aggregated = [chunk.mean() for chunk in losses_per_band.chunk(self.num_aggregated_bands, dim=0)]
        metrics = {f'rvm_{index}': value for index, value in enumerate(aggregated)}
        metrics['rvm'] = losses_per_band.mean()
        return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, l2norm
def l2norm(t):
    return F.normalize(t, p=2, dim=-1)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, get_adv_criterion
def get_adv_criterion(loss_type: str) -> tp.Callable:
    assert loss_type in ADVERSARIAL_LOSSES
    if loss_type == 'mse':
        return mse_loss
    elif loss_type == 'hinge':
        return hinge_loss
    elif loss_type == 'hinge2':
        return hinge2_loss
    raise ValueError('Unsupported loss')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningAttributes.wav_attributes
def wav_attributes(self):
        return self.wav.keys()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\mos.py
BlockTypes.METHOD, ensure_logged
def ensure_logged(func):
    """Ensure user is logged in.
    """
    @wraps(func)
    def _wrapped(*args, **kwargs):
        user = session.get('user')
        if user is None:
            return redirect(url_for('login', redirect_to=request.url))
        return func(*args, **kwargs)
    return _wrapped
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, dict_from_config
def dict_from_config(cfg: omegaconf.DictConfig) -> dict:
    """Convenience function to map an omegaconf configuration to a dictionary.

    Args:
        cfg (omegaconf.DictConfig): Original configuration to map to dict.
    Returns:
        dict: Config as dictionary object.
    """
    dct = omegaconf.OmegaConf.to_container(cfg, resolve=True)
    assert isinstance(dct, dict)
    return dct
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\loaders.py
BlockTypes.METHOD, _delete_param
def _delete_param(cfg: DictConfig, full_name: str):
    parts = full_name.split('.')
    for part in parts[:-1]:
        if part in cfg:
            cfg = cfg[part]
        else:
            return
    OmegaConf.set_struct(cfg, False)
    if parts[-1] in cfg:
        del cfg[parts[-1]]
    OmegaConf.set_struct(cfg, True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, EmbeddingCache._get_full_embed_from_cache
def _get_full_embed_from_cache(cache: Path):
        """Loads full pre-computed embedding from the cache."""
        try:
            embed = torch.load(cache, 'cpu')
        except Exception as exc:
            logger.error("Error loading %s: %r", cache, exc)
            embed = None
        return embed
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestAvRead.test_avread_seek_partial
def test_avread_seek_partial(self):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 1.
        for sample_rate, ch in product(sample_rates, channels):
            n_frames = int(sample_rate * duration)
            wav = get_white_noise(ch, n_frames)
            path = self.get_temp_path(f'reference_b_{sample_rate}_{ch}.wav')
            save_wav(path, wav, sample_rate)
            for _ in range(100):
                # seek will always load a partial segment
                seek_time = random.uniform(0.5, 1.)
                seek_duration = 1.
                expected_num_frames = n_frames - int(seek_time * sample_rate)
                read_wav, read_sr = _av_read(path, seek_time, seek_duration)
                assert read_sr == sample_rate
                assert read_wav.shape[0] == wav.shape[0]
                assert read_wav.shape[-1] == expected_num_frames
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, test_pad1d_reflect
def test_pad1d_reflect():
    x = torch.randn(1, 1, 20)

    xp1 = pad1d(x, (0, 5), mode='reflect', value=0.)
    assert xp1.shape[-1] == 25
    xp2 = pad1d(x, (5, 5), mode='reflect', value=0.)
    assert xp2.shape[-1] == 30
    xp3 = pad1d(x, (0, 0), mode='reflect', value=0.)
    assert xp3.shape[-1] == 20
    xp4 = pad1d(x, (10, 30), mode='reflect', value=0.)
    assert xp4.shape[-1] == 60

    with pytest.raises(AssertionError):
        pad1d(x, (-1, 0), mode='reflect', value=0.)

    with pytest.raises(AssertionError):
        pad1d(x, (0, -1), mode='reflect', value=0.)

    with pytest.raises(AssertionError):
        pad1d(x, (-1, -1), mode='reflect', value=0.)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\compression\_explorers.py
BlockTypes.METHOD, CompressionExplorer.get_grid_metrics
def get_grid_metrics(self):
        """Return the metrics that should be displayed in the tracking table.
        """
        return [
            tt.group(
                "train",
                [
                    tt.leaf("epoch"),
                    tt.leaf("bandwidth", ".2f"),
                    tt.leaf("adv", ".4f"),
                    tt.leaf("d_loss", ".4f"),
                ],
                align=">",
            ),
            tt.group(
                "valid",
                [
                    tt.leaf("bandwidth", ".2f"),
                    tt.leaf("adv", ".4f"),
                    tt.leaf("msspec", ".4f"),
                    tt.leaf("sisnr", ".2f"),
                ],
                align=">",
            ),
            tt.group(
                "evaluate", [tt.leaf(name, ".3f") for name in self.eval_metrics], align=">"
            ),
        ]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\streaming.py
BlockTypes.METHOD, StreamingModule._apply_named_streaming
def _apply_named_streaming(self, fn: tp.Any):
        for name, module in self.named_modules():
            if isinstance(module, StreamingModule):
                fn(name, module)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestInfo.test_info_flac
def test_info_flac(self):
        self._test_info_format('.flac')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\info_audio_dataset.py
BlockTypes.METHOD, AudioInfo.to_condition_attributes
def to_condition_attributes(self) -> ConditioningAttributes:
        return ConditioningAttributes()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_sample_with_weight
def test_sample_with_weight(self, segment_duration, sample_on_weight, sample_on_duration, a_hist, b_hist, c_hist):
        random.seed(1234)
        rng = torch.Generator()
        rng.manual_seed(1234)

        def _get_histogram(dataset, repetitions=20_000):
            counts = {file_meta.path: 0. for file_meta in meta}
            for _ in range(repetitions):
                file_meta = dataset.sample_file(0, rng)
                counts[file_meta.path] += 1
            return {name: count / repetitions for name, count in counts.items()}

        meta = [
           AudioMeta(path='a', duration=5, sample_rate=1, weight=2),
           AudioMeta(path='b', duration=10, sample_rate=1, weight=None),
           AudioMeta(path='c', duration=5, sample_rate=1, weight=0),
        ]
        dataset = AudioDataset(
            meta, segment_duration=segment_duration, sample_on_weight=sample_on_weight,
            sample_on_duration=sample_on_duration)
        hist = _get_histogram(dataset)
        assert math.isclose(hist['a'], a_hist, abs_tol=0.01)
        assert math.isclose(hist['b'], b_hist, abs_tol=0.01)
        assert math.isclose(hist['c'], c_hist, abs_tol=0.01)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_lstm.py
BlockTypes.METHOD, TestStreamableLSTM.test_lstm
def test_lstm(self):
        B, C, T = 4, 2, random.randint(1, 100)

        lstm = StreamableLSTM(C, 3, skip=False)
        x = torch.randn(B, C, T)
        y = lstm(x)

        print(y.shape)
        assert y.shape == torch.Size([B, C, T])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, get_bpm
def get_bpm(value: tp.Optional[str]) -> tp.Optional[float]:
    """Preprocess to a float."""
    if value is None:
        return None
    try:
        return float(value)
    except ValueError:
        return None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, VectorQuantization.decode
def decode(self, embed_ind):
        quantize = self._codebook.decode(embed_ind)
        quantize = self.project_out(quantize)
        quantize = self._postprocess(quantize)
        return quantize
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, ScaledEmbedding.__init__
def __init__(self, *args, lr=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.lr = lr
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, NormConv2d.__init__
def __init__(self, *args, norm: str = 'none', norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
        super().__init__()
        self.conv = apply_parametrization_norm(nn.Conv2d(*args, **kwargs), norm)
        self.norm = get_norm_module(self.conv, causal=False, norm=norm, **norm_kwargs)
        self.norm_type = norm
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, Sample.__hash__
def __hash__(self):
        return hash(self.id)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_repeat_kv
def test_repeat_kv():
    torch.manual_seed(1234)
    num_heads = 8
    kv_repeat = 4
    dim = num_heads * 64
    with pytest.raises(AssertionError):
        mha = StreamingMultiheadAttention(
            dim, num_heads, causal=True, kv_repeat=kv_repeat, cross_attention=True)
        mha = StreamingMultiheadAttention(
            dim, num_heads, causal=True, kv_repeat=kv_repeat)
    mha = StreamingMultiheadAttention(
        dim, num_heads, causal=True, kv_repeat=kv_repeat, custom=True)
    x = torch.randn(4, 18, dim)
    y = mha(x, x, x)[0]
    assert x.shape == y.shape
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningAttributes.joint_embed_attributes
def joint_embed_attributes(self):
        return self.joint_embed.keys()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, ema_inplace
def ema_inplace(moving_avg, new, decay: float):
    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_dataset_equal_audio_and_segment_durations
def test_dataset_equal_audio_and_segment_durations(self):
        total_examples = 1
        num_samples = 2
        audio_duration = 1.
        segment_duration = 1.
        sample_rate = 16_000
        channels = 1
        dataset = self._create_audio_dataset(
            'dset', total_examples, durations=audio_duration, sample_rate=sample_rate,
            channels=channels, segment_duration=segment_duration, num_examples=num_samples)
        assert len(dataset) == num_samples
        assert dataset.sample_rate == sample_rate
        assert dataset.channels == channels
        for idx in range(len(dataset)):
            sample = dataset[idx]
            assert sample.shape[0] == channels
            assert sample.shape[1] == int(segment_duration * sample_rate)
        # the random seek_time adds variability on audio read
        sample_1 = dataset[0]
        sample_2 = dataset[1]
        assert not torch.allclose(sample_1, sample_2)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\visqol.py
BlockTypes.METHOD, ViSQOL._flush_files
def _flush_files(self, tmp_dir: tp.Union[Path, str]):
        # flush tmp files used to compute ViSQOL.
        shutil.rmtree(str(tmp_dir))
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, LayerScale.forward
def forward(self, x: torch.Tensor):
        if self.channel_last:
            return self.scale * x
        else:
            return self.scale[:, None] * x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_fad
def get_fad(cfg: omegaconf.DictConfig) -> metrics.FrechetAudioDistanceMetric:
    """Instantiate Frechet Audio Distance metric from config."""
    kwargs = dict_from_config(cfg.tf)
    xp = dora.get_xp()
    kwargs['log_folder'] = xp.folder
    return metrics.FrechetAudioDistanceMetric(**kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_musicgen.py
BlockTypes.METHOD, TestMusicGenModel.test_generate_unconditional
def test_generate_unconditional(self):
        mg = self.get_musicgen()
        wav = mg.generate_unconditional(3)
        assert list(wav.shape) == [3, 1, 64000]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, AttributeDropout.__repr__
def __repr__(self):
        return f"AttributeDropout({dict(self.p)})"
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningProvider._collate_joint_embeds
def _collate_joint_embeds(self, samples: tp.List[ConditioningAttributes]) -> tp.Dict[str, JointEmbedCondition]:
        """Generate a dict where the keys are attributes by which we compute joint embeddings,
        and the values are Tensors of pre-computed embeddings and the corresponding text attributes.

        Args:
            samples (list[ConditioningAttributes]): List of ConditioningAttributes samples.
        Returns:
            A dictionary mapping an attribute name to joint embeddings.
        """
        texts = defaultdict(list)
        wavs = defaultdict(list)
        lengths = defaultdict(list)
        sample_rates = defaultdict(list)
        paths = defaultdict(list)
        seek_times = defaultdict(list)
        channels: int = 0

        out = {}
        for sample in samples:
            for attribute in self.joint_embed_conditions:
                wav, text, length, sample_rate, path, seek_time = sample.joint_embed[attribute]
                assert wav.dim() == 3
                if channels == 0:
                    channels = wav.size(1)
                else:
                    assert channels == wav.size(1), "not all audio has same number of channels in batch"
                assert wav.size(0) == 1, "Expecting single-wav batch in the collate method"
                wav = einops.rearrange(wav, "b c t -> (b c t)")  # [1, C, T] => [C * T]
                wavs[attribute].append(wav)
                texts[attribute].extend(text)
                lengths[attribute].append(length)
                sample_rates[attribute].extend(sample_rate)
                paths[attribute].extend(path)
                seek_times[attribute].extend(seek_time)

        for attribute in self.joint_embed_conditions:
            stacked_texts = texts[attribute]
            stacked_paths = paths[attribute]
            stacked_seek_times = seek_times[attribute]
            stacked_wavs = pad_sequence(wavs[attribute]).to(self.device)
            stacked_wavs = einops.rearrange(stacked_wavs, "(c t) b -> b c t", c=channels)
            stacked_sample_rates = sample_rates[attribute]
            stacked_lengths = torch.cat(lengths[attribute]).to(self.device)
            assert stacked_lengths.size(0) == stacked_wavs.size(0)
            assert len(stacked_sample_rates) == stacked_wavs.size(0)
            assert len(stacked_texts) == stacked_wavs.size(0)
            out[attribute] = JointEmbedCondition(
                text=stacked_texts, wav=stacked_wavs,
                length=stacked_lengths, sample_rate=stacked_sample_rates,
                path=stacked_paths, seek_time=stacked_seek_times)

        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, ResBlock.forward
def forward(self, x):
        h = self.dropout1(self.conv1(self.activation1(self.norm1(x))))
        h = self.dropout2(self.conv2(self.activation2(self.norm2(h))))
        return x + h
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, SampleManager._store_audio
def _store_audio(self, wav: torch.Tensor, stem_path: Path, overwrite: bool = False) -> Path:
        """Stores the audio with the given stem path using the XP's configuration.

        Args:
            wav (torch.Tensor): Audio to store.
            stem_path (Path): Path in sample output directory with file stem to use.
            overwrite (bool): When False (default), skips storing an existing audio file.
        Returns:
            Path: The path at which the audio is stored.
        """
        existing_paths = [
            path for path in stem_path.parent.glob(stem_path.stem + '.*')
            if path.suffix != '.json'
        ]
        exists = len(existing_paths) > 0
        if exists and overwrite:
            logger.warning(f"Overwriting existing audio file with stem path {stem_path}")
        elif exists:
            return existing_paths[0]

        audio_path = audio_write(stem_path, wav, **self.xp.cfg.generate.audio)
        return audio_path
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestInfo.test_info_ogg
def test_info_ogg(self):
        self._test_info_format('.ogg')
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\checkpoint.py
BlockTypes.METHOD, flush_stale_checkpoints
def flush_stale_checkpoints(checkpoint_path: Path, keep_last: tp.Optional[int] = None) -> None:
    """Flush checkpoints to only keep last N checkpoints."""
    if keep_last is None or keep_last <= 0:
        return
    checkpoint_dir = checkpoint_path.parent
    suffix = ''
    if flashy.distrib.rank() > 0:
        suffix = f'.{flashy.distrib.rank()}'
    checkpoint_files_with_epoch = []
    for path in Path(checkpoint_dir).glob(f'checkpoint_*.th{suffix}'):
        epoch_part = path.name.split('.', 1)[0].split('_', 1)[1]
        if epoch_part.isdigit():
            checkpoint_files_with_epoch.append((path, int(epoch_part)))
    checkpoint_files = [path for path, _ in list(sorted(checkpoint_files_with_epoch, key=lambda t: t[1]))]
    total_to_flush = max(0, len(checkpoint_files) - keep_last)
    files_to_flush = checkpoint_files[:total_to_flush]
    for path in files_to_flush:
        logger.debug("Removing checkpoint: %s", str(path))
        path.unlink(missing_ok=True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_audiogen.py
BlockTypes.METHOD, TestAudioGenModel.test_generate_long
def test_generate_long(self):
        ag = self.get_audiogen()
        ag.max_duration = 3.
        ag.set_generation_params(duration=4., extend_stride=2.)
        wav = ag.generate(
            ['youpi', 'lapin dort'])
        assert list(wav.shape) == [2, 1, 16000 * 4]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\diffusion.py
BlockTypes.METHOD, DiffusionSolver.best_metric_name
def best_metric_name(self) -> tp.Optional[str]:
        if self._current_stage == "evaluate":
            return 'rvm'
        else:
            return 'loss'
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\train.py
BlockTypes.METHOD, get_solver_from_sig
def get_solver_from_sig(sig: str, *args, **kwargs):
    """Return Solver object from Dora signature, i.e. to play with it from a notebook.
    See `get_solver_from_xp` for more information.
    """
    xp = main.get_xp_from_sig(sig)
    return get_solver_from_xp(xp, *args, **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\visqol.py
BlockTypes.METHOD, ViSQOL._collect_moslqo_score
def _collect_moslqo_score(self, results_csv_path: tp.Union[Path, str]) -> float:
        # collect results for each evaluated pair and return averaged moslqo score.
        with open(results_csv_path, "r") as csv_file:
            reader = csv.DictReader(csv_file)
            moslqo_scores = [float(row["moslqo"]) for row in reader]
            if len(moslqo_scores) > 0:
                return sum(moslqo_scores) / len(moslqo_scores)
            else:
                return 0.0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, UnrolledPatternProvider._num_inner_steps
def _num_inner_steps(self):
        """Number of inner steps to unroll between timesteps in order to flatten the codebooks.
        """
        return max([inner_step for inner_step in self._flattened_codebooks.keys()]) + 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, Sample.audio
def audio(self) -> tp.Tuple[torch.Tensor, int]:
        return audio_read(self.path)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, MultiBandDiffusion.__init__
def __init__(self, DPs: tp.List[DiffusionProcess], codec_model: CompressionModel) -> None:
        self.DPs = DPs
        self.codec_model = codec_model
        self.device = next(self.codec_model.parameters()).device
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, laplace_smoothing
def laplace_smoothing(x, n_categories: int, epsilon: float = 1e-5):
    return (x + epsilon) / (x.sum() + n_categories * epsilon)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\adversarial\losses.py
BlockTypes.METHOD, AdversarialLoss.train_adv
def train_adv(self, fake: torch.Tensor, real: torch.Tensor) -> torch.Tensor:
        """Train the adversary with the given fake and real example.

        We assume the adversary output is the following format: Tuple[List[torch.Tensor], List[List[torch.Tensor]]].
        The first item being the logits and second item being a list of feature maps for each sub-discriminator.

        This will automatically synchronize gradients (with `flashy.distrib.eager_sync_model`)
        and call the optimizer.
        """
        loss = torch.tensor(0., device=fake.device)
        all_logits_fake_is_fake, _ = self.get_adversary_pred(fake.detach())
        all_logits_real_is_fake, _ = self.get_adversary_pred(real.detach())
        n_sub_adversaries = len(all_logits_fake_is_fake)
        for logit_fake_is_fake, logit_real_is_fake in zip(all_logits_fake_is_fake, all_logits_real_is_fake):
            loss += self.loss_fake(logit_fake_is_fake) + self.loss_real(logit_real_is_fake)

        if self.normalize:
            loss /= n_sub_adversaries

        self.optimizer.zero_grad()
        with flashy.distrib.eager_sync_model(self.adversary):
            loss.backward()
        self.optimizer.step()

        return loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningAttributes.attributes
def attributes(self):
        return {
            "text": self.text_attributes,
            "wav": self.wav_attributes,
            "joint_embed": self.joint_embed_attributes,
        }
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\lm.py
BlockTypes.METHOD, ScaledEmbedding.make_optim_group
def make_optim_group(self):
        group = {"params": list(self.parameters())}
        if self.lr is not None:
            group["lr"] = self.lr
        return group
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, Pattern.revert_pattern_sequence
def revert_pattern_sequence(self, s: torch.Tensor, special_token: int, keep_only_valid_steps: bool = False):
        """Revert a sequence built from the pattern back to the original multi-codebook sequence without interleaving.
        The sequence is reverted using up to timesteps if specified, and non-pattern coordinates
        are filled with the special token.

        Args:
            s (torch.Tensor): Interleaved sequence tensor obtained from the pattern, of shape [B, K, S].
            special_token (int or float): Special token used to fill non-pattern coordinates in the new sequence.
        Returns:
            values (torch.Tensor): Interleaved sequence matching the pattern, of shape [B, K, T] with T
                corresponding either to the timesteps if provided, or the total timesteps in pattern otherwise.
            indexes (torch.Tensor): Indexes corresponding to the interleaved sequence, of shape [K, T].
            mask (torch.Tensor): Mask corresponding to indexes that matches valid indexes of shape [K, T].
        """
        B, K, S = s.shape
        indexes, mask = self._build_reverted_sequence_scatter_indexes(
            S, K, keep_only_valid_steps, is_model_output=False, device=str(s.device)
        )
        s = s.view(B, -1)
        # we append the special token as the last index of our flattened z tensor
        s = torch.cat([s, torch.zeros_like(s[:, :1]) + special_token], dim=1)
        values = s[:, indexes.view(-1)]
        values = values.view(B, K, indexes.shape[-1])
        return values, indexes, mask
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner.tokenize
def tokenize(self, x: WavCondition) -> WavCondition:
        """Apply WavConditioner tokenization and populate cache if needed."""
        x = super().tokenize(x)
        no_undefined_paths = all(p is not None for p in x.path)
        if self.cache is not None and no_undefined_paths:
            paths = [Path(p) for p in x.path if p is not None]
            self.cache.populate_embed_cache(paths, x)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\fsdp.py
BlockTypes.METHOD, purge_fsdp
def purge_fsdp(model: FSDP):
    """Purge the FSDP cached shard inside the model. This should
    allow setting the best state or switching to the EMA.
    """
    from torch.distributed.fsdp._runtime_utils import _reshard  # type: ignore
    for module in FSDP.fsdp_modules(model):
        handles = module._handles
        if not handles:
            continue
        handle = handles[0]
        unsharded_flat_param = handle._get_padded_unsharded_flat_param()
        storage_size: int = unsharded_flat_param._typed_storage()._size()  # type: ignore
        if storage_size == 0:
            continue
        true_list = [True for h in handles]
        _reshard(module, handles, true_list)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\cache.py
BlockTypes.METHOD, EmbeddingCache.populate_embed_cache
def populate_embed_cache(self, paths: tp.List[Path], x: tp.Any) -> None:
        """Populate in-memory caches for embeddings reading from the embeddings stored on disk.
        The in-memory caches consist in a cache for the full embedding and another cache for the
        final embedding chunk. Such caches are used to limit the IO access when computing the actual embeddings
        and reduce the IO footprint and synchronization points during forward passes.

        Args:
            paths (list[Path]): List of paths from where the embeddings can be loaded.
            x (any): Object from which the embedding is extracted.
        """
        self._current_batch_cache.clear()
        if self.cache_path is not None:
            futures: list = []
            for path in paths:
                assert path is not None, "Path is required for computation from cache"
                cache = self._get_cache_path(path)
                if cache in self._memory_cache or not cache.exists():
                    futures.append(None)
                else:
                    futures.append(self.pool.submit(EmbeddingCache._get_full_embed_from_cache, cache))
            for idx, (path, future) in enumerate(zip(paths, futures)):
                assert path is not None
                cache = self._get_cache_path(path)
                full_embed = None
                if future is None:
                    if cache in self._memory_cache:
                        full_embed = self._memory_cache[cache]
                else:
                    full_embed = future.result()
                    if full_embed is not None:
                        self._memory_cache[cache] = full_embed
                        full_embed = full_embed.to(self.device)
                if full_embed is not None:
                    embed = self._extract_embed_fn(full_embed, x, idx)
                    self._current_batch_cache[cache] = embed
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\metrics\fad.py
BlockTypes.METHOD, FrechetAudioDistanceMetric._local_compute_frechet_audio_distance
def _local_compute_frechet_audio_distance(self):
        """Compute Frechet Audio Distance score calling TensorFlow API."""
        num_of_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
        if num_of_gpus > 1:
            self._parallel_create_embedding_beams(num_of_gpus)
        else:
            self._sequential_create_embedding_beams()
        fad_score = self._compute_fad_score(gpu_index=0)
        return fad_score
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\demos\musicgen_app.py
BlockTypes.METHOD, load_diffusion
def load_diffusion():
    global MBD
    if MBD is None:
        print("loading MBD")
        MBD = MultiBandDiffusion.get_mbd_musicgen()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestInfo.test_info_m4a
def test_info_m4a(self):
        # TODO: generate m4a file programmatically
        # self._test_info_format('.m4a')
        pass
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, uniform_init
def uniform_init(*shape: int):
    t = torch.empty(shape)
    nn.init.kaiming_uniform_(t)
    return t
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\cosine_lr_scheduler.py
BlockTypes.METHOD, CosineLRScheduler.__init__
def __init__(self, optimizer: Optimizer, total_steps: int, warmup_steps: int,
                 lr_min_ratio: float = 0.0, cycle_length: float = 1.0):
        self.warmup_steps = warmup_steps
        assert self.warmup_steps >= 0
        self.total_steps = total_steps
        assert self.total_steps >= 0
        self.lr_min_ratio = lr_min_ratio
        self.cycle_length = cycle_length
        super().__init__(optimizer)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\utils.py
BlockTypes.METHOD, random_subset
def random_subset(dataset, max_samples: int, seed: int = 42) -> torch.utils.data.Subset:
    if max_samples >= len(dataset):
        return dataset

    generator = torch.Generator().manual_seed(seed)
    perm = torch.randperm(len(dataset), generator=generator)
    return torch.utils.data.Subset(dataset, perm[:max_samples].tolist())
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, Sample.audio_prompt
def audio_prompt(self) -> tp.Optional[tp.Tuple[torch.Tensor, int]]:
        return audio_read(self.prompt.path) if self.prompt is not None else None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\fsdp.py
BlockTypes.METHOD, is_sharded_tensor
def is_sharded_tensor(x: tp.Any) -> bool:
    return isinstance(x, ShardedTensor)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\fsdp.py
BlockTypes.METHOD, _fix_post_backward_hook
def _fix_post_backward_hook():
    global _hook_fixed
    if _hook_fixed:
        return
    _hook_fixed = True

    from torch.distributed.fsdp import _runtime_utils
    from torch.distributed.fsdp._common_utils import TrainingState, HandleTrainingState
    old_hook = _runtime_utils._post_backward_hook

    def _post_backward_hook(state, handle, *args, **kwargs):
        checkpointed = getattr(state._fsdp_wrapped_module, '_audiocraft_checkpointed', False)
        if checkpointed:
            # there will be one more forward in the backward with checkpointing and that will
            # massively confuse FSDP, so we have to make it think everything
            # is going according to the plan.
            state.training_state = TrainingState.FORWARD_BACKWARD
            handle._training_state = HandleTrainingState.BACKWARD_PRE
        old_hook(state, handle, *args, **kwargs)

    _runtime_utils._post_backward_hook = _post_backward_hook
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\compression.py
BlockTypes.METHOD, CompressionSolver.run_step
def run_step(self, idx: int, batch: torch.Tensor, metrics: dict):
        """Perform one training or valid step on a given batch."""
        x = batch.to(self.device)
        y = x.clone()

        qres = self.model(x)
        assert isinstance(qres, quantization.QuantizedResult)
        y_pred = qres.x
        # Log bandwidth in kb/s
        metrics['bandwidth'] = qres.bandwidth.mean()

        if self.is_training:
            d_losses: dict = {}
            if len(self.adv_losses) > 0 and torch.rand(1, generator=self.rng).item() <= 1 / self.cfg.adversarial.every:
                for adv_name, adversary in self.adv_losses.items():
                    disc_loss = adversary.train_adv(y_pred, y)
                    d_losses[f'd_{adv_name}'] = disc_loss
                metrics['d_loss'] = torch.sum(torch.stack(list(d_losses.values())))
            metrics.update(d_losses)

        balanced_losses: dict = {}
        other_losses: dict = {}

        # penalty from quantization
        if qres.penalty is not None and qres.penalty.requires_grad:
            other_losses['penalty'] = qres.penalty  # penalty term from the quantizer

        # adversarial losses
        for adv_name, adversary in self.adv_losses.items():
            adv_loss, feat_loss = adversary(y_pred, y)
            balanced_losses[f'adv_{adv_name}'] = adv_loss
            balanced_losses[f'feat_{adv_name}'] = feat_loss

        # auxiliary losses
        for loss_name, criterion in self.aux_losses.items():
            loss = criterion(y_pred, y)
            balanced_losses[loss_name] = loss

        # weighted losses
        metrics.update(balanced_losses)
        metrics.update(other_losses)
        metrics.update(qres.metrics)

        if self.is_training:
            # backprop losses that are not handled by balancer
            other_loss = torch.tensor(0., device=self.device)
            if 'penalty' in other_losses:
                other_loss += other_losses['penalty']
            if other_loss.requires_grad:
                other_loss.backward(retain_graph=True)
                ratio1 = sum(p.grad.data.norm(p=2).pow(2)
                             for p in self.model.parameters() if p.grad is not None)
                assert isinstance(ratio1, torch.Tensor)
                metrics['ratio1'] = ratio1.sqrt()

            # balancer losses backward, returns effective training loss
            # with effective weights at the current batch.
            metrics['g_loss'] = self.balancer.backward(balanced_losses, y_pred)
            # add metrics corresponding to weight ratios
            metrics.update(self.balancer.metrics)
            ratio2 = sum(p.grad.data.norm(p=2).pow(2)
                         for p in self.model.parameters() if p.grad is not None)
            assert isinstance(ratio2, torch.Tensor)
            metrics['ratio2'] = ratio2.sqrt()

            # optim
            flashy.distrib.sync_model(self.model)
            if self.cfg.optim.max_norm:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), self.cfg.optim.max_norm
                )
            self.optimizer.step()
            self.optimizer.zero_grad()

        # informative losses only
        info_losses: dict = {}
        with torch.no_grad():
            for loss_name, criterion in self.info_losses.items():
                loss = criterion(y_pred, y)
                info_losses[loss_name] = loss

        metrics.update(info_losses)

        # aggregated GAN losses: this is useful to report adv and feat across different adversarial loss setups
        adv_losses = [loss for loss_name, loss in metrics.items() if loss_name.startswith('adv')]
        if len(adv_losses) > 0:
            metrics['adv'] = torch.sum(torch.stack(adv_losses))
        feat_losses = [loss for loss_name, loss in metrics.items() if loss_name.startswith('feat')]
        if len(feat_losses) > 0:
            metrics['feat'] = torch.sum(torch.stack(feat_losses))

        return metrics
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\info_audio_dataset.py
BlockTypes.METHOD, InfoAudioDataset.__init__
def __init__(self, meta: tp.List[AudioMeta], **kwargs):
        super().__init__(clusterify_all_meta(meta), **kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningAttributes.to_flat_dict
def to_flat_dict(self):
        return {
            **{f"text.{k}": v for k, v in self.text.items()},
            **{f"wav.{k}": v for k, v in self.wav.items()},
            **{f"joint_embed.{k}": v for k, v in self.joint_embed.items()}
        }
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, UnrolledPatternProvider.num_virtual_steps
def num_virtual_steps(self, timesteps: int) -> int:
        return timesteps * self._num_inner_steps + 1
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\models\test_musicgen.py
BlockTypes.METHOD, TestMusicGenModel.test_generate_continuation
def test_generate_continuation(self):
        mg = self.get_musicgen()
        prompt = torch.randn(3, 1, 32000)
        wav = mg.generate_continuation(prompt, 32000)
        assert list(wav.shape) == [3, 1, 64000]

        prompt = torch.randn(2, 1, 32000)
        wav = mg.generate_continuation(
            prompt, 32000, ['youpi', 'lapin dort'])
        assert list(wav.shape) == [2, 1, 64000]

        prompt = torch.randn(2, 1, 32000)
        with pytest.raises(AssertionError):
            wav = mg.generate_continuation(
                prompt, 32000, ['youpi', 'lapin dort', 'one too many'])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.generate_audio
def generate_audio(self, gen_tokens: torch.Tensor):
        """Generate Audio from tokens"""
        assert gen_tokens.dim() == 3
        with torch.no_grad():
            gen_audio = self.compression_model.decode(gen_tokens, None)
        return gen_audio
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ClassifierFreeGuidanceDropout.__init__
def __init__(self, p: float, seed: int = 1234):
        super().__init__(seed=seed)
        self.p = p
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\optim\fsdp.py
BlockTypes.METHOD, switch_to_full_state_dict
def switch_to_full_state_dict(models: tp.List[FSDP]):
    # Another bug in FSDP makes it that we cannot use the `state_dict_type` API,
    # so let's do thing manually.
    for model in models:
        FSDP.set_state_dict_type(  # type: ignore
            model, StateDictType.FULL_STATE_DICT,
            FullStateDictConfig(offload_to_cpu=True, rank0_only=True))
    try:
        yield
    finally:
        for model in models:
            FSDP.set_state_dict_type(model, StateDictType.LOCAL_STATE_DICT)  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, VectorQuantization.forward
def forward(self, x):
        device = x.device
        x = self._preprocess(x)

        x = self.project_in(x)
        quantize, embed_ind = self._codebook(x)

        if self.training:
            quantize = x + (quantize - x).detach()

        loss = torch.tensor([0.0], device=device, requires_grad=self.training)

        if self.training:
            if self.commitment_weight > 0:
                commit_loss = F.mse_loss(quantize.detach(), x)
                loss = loss + commit_loss * self.commitment_weight

            if self.orthogonal_reg_weight > 0:
                codebook = self.codebook

                if self.orthogonal_reg_active_codes_only:
                    # only calculate orthogonal loss for the activated codes for this batch
                    unique_code_ids = torch.unique(embed_ind)
                    codebook = codebook[unique_code_ids]

                num_codes = codebook.shape[0]
                if exists(self.orthogonal_reg_max_codes) and num_codes > self.orthogonal_reg_max_codes:
                    rand_ids = torch.randperm(num_codes, device=device)[:self.orthogonal_reg_max_codes]
                    codebook = codebook[rand_ids]

                orthogonal_reg_loss = orthogonal_loss_fn(codebook)
                loss = loss + orthogonal_reg_loss * self.orthogonal_reg_weight

        quantize = self.project_out(quantize)
        quantize = self._postprocess(quantize)

        return quantize, embed_ind, loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\sound_dataset.py
BlockTypes.METHOD, SoundInfo.attribute_getter
def attribute_getter(attribute):
        if attribute == 'description':
            preprocess_func = get_keyword_or_keyword_list
        else:
            preprocess_func = None
        return preprocess_func
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._downsampling_factor
def _downsampling_factor(self) -> int:
        return self.chroma.winhop
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\base.py
BlockTypes.METHOD, StandardSolver.commit
def commit(self, save_checkpoints: bool = True):
        """Commit metrics to dora and save checkpoints at the end of an epoch."""
        # we override commit to introduce more complex checkpoint saving behaviors
        self.history.append(self._pending_metrics)  # This will increase self.epoch
        if save_checkpoints:
            self.save_checkpoints()
        self._start_epoch()
        if flashy.distrib.is_rank_zero():
            self.xp.link.update_history(self.history)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, NormConv2d.forward
def forward(self, x):
        x = self.conv(x)
        x = self.norm(x)
        return x
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\musicgen_melody_32khz.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=32, partition=partitions)
    launcher.bind_(solver='musicgen/musicgen_melody_32khz')
    # replace this by the desired music dataset
    launcher.bind_(dset='internal/music_400k_32khz')

    fsdp = {'autocast': False, 'fsdp.use': True}
    medium = {'model/lm/model_scale': 'medium'}
    large = {'model/lm/model_scale': 'large'}

    cfg_low = {'classifier_free_guidance.training_dropout': 0.2}
    wd_low = {'conditioners.description.t5.word_dropout': 0.2}

    adam = {'optim.optimizer': 'adamw', 'optim.lr': 1e-4}

    cache_path = {'conditioners.self_wav.chroma_stem.cache_path':
                  '/fsx-audio-craft-llm/jadecopet/experiments/audiocraft/caches/chroma_stem'}

    # CACHE GENERATION JOBS
    n_cache_gen_jobs = 4
    gen_sub = launcher.slurm(gpus=1)
    gen_sub.bind_(
        cache_path, {
            # the cache is always computed over the whole file, so duration doesn't matter here.
            'dataset.segment_duration': 2.,
            'dataset.batch_size': 8,
            'dataset.train.permutation_on_files': True,  # try to not repeat files.
            'optim.epochs': 10,
            'model/lm/model_scale': 'xsmall',

        })
    with gen_sub.job_array():
        for gen_job in range(n_cache_gen_jobs):
            gen_sub({'dataset.train.shuffle_seed': gen_job})

    # ACTUAL TRAINING JOBS.
    launcher.bind_(fsdp)

    launcher.slurm_(gpus=32).bind_(label='32gpus')
    with launcher.job_array():
        sub = launcher.bind()
        sub()
        sub(cache_path)

    launcher.slurm_(gpus=64).bind_(label='64gpus')
    with launcher.job_array():
        sub = launcher.bind()
        sub(medium, adam)

    launcher.slurm_(gpus=96).bind_(label='96gpus')
    with launcher.job_array():
        sub = launcher.bind()
        sub(large, cfg_low, wd_low, adam, {'optim.max_norm': 3})
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conv.py
BlockTypes.METHOD, StreamableConvTranspose1d.__init__
def __init__(self, in_channels: int, out_channels: int,
                 kernel_size: int, stride: int = 1, causal: bool = False,
                 norm: str = 'none', trim_right_ratio: float = 1.,
                 norm_kwargs: tp.Dict[str, tp.Any] = {}):
        super().__init__()
        self.convtr = NormConvTranspose1d(in_channels, out_channels, kernel_size, stride,
                                          causal=causal, norm=norm, norm_kwargs=norm_kwargs)
        self.causal = causal
        self.trim_right_ratio = trim_right_ratio
        assert self.causal or self.trim_right_ratio == 1., \
            "`trim_right_ratio` != 1.0 only makes sense for causal convolutions"
        assert self.trim_right_ratio >= 0. and self.trim_right_ratio <= 1.
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\samples\manager.py
BlockTypes.METHOD, Sample.audio_reference
def audio_reference(self) -> tp.Optional[tp.Tuple[torch.Tensor, int]]:
        return audio_read(self.reference.path) if self.reference is not None else None
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ConditioningAttributes.from_flat_dict
def from_flat_dict(cls, x):
        out = cls()
        for k, v in x.items():
            kind, att = k.split(".")
            out[kind][att] = v
        return out
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, CLAPEmbeddingConditioner._compute_wav_embedding
def _compute_wav_embedding(self, wav: torch.Tensor, length: torch.Tensor,
                               sample_rates: tp.List[int], reduce_mean: bool = False) -> torch.Tensor:
        """Compute audio wave embedding from CLAP model.

        Since CLAP operates on a fixed sequence length audio inputs and we need to process longer audio sequences,
        we calculate the wav embeddings on `clap_max_frames` windows with `clap_stride`-second stride and
        average the resulting embeddings.

        Args:
            wav (torch.Tensor): Audio wav, of shape [B, C, T].
            length (torch.Tensor): Actual length of the audio for each item in the batch, of shape [B].
            sample_rates (list[int]): Sample rates for each sample in the batch.
            reduce_mean (bool): Whether to get the average tensor.
        Returns:
            torch.Tensor: Audio embedding of shape [B, F, D], F being the number of chunks, D the dimension.
        """
        with torch.no_grad():
            wav = self._preprocess_wav(wav, length, sample_rates)
            B, T = wav.shape
            if T >= self.clap_max_frames:
                wav = wav.unfold(-1, self.clap_max_frames, self.clap_stride)  # [B, F, T]
            else:
                wav = wav.view(-1, 1, T)  # [B, F, T] with F=1
            wav = einops.rearrange(wav, 'b f t -> (b f) t')
            embed_list = []
            for i in range(0, wav.size(0), self.batch_size):
                _wav = wav[i:i+self.batch_size, ...]
                _embed = self.clap.get_audio_embedding_from_data(_wav, use_tensor=True)
                embed_list.append(_embed)
            embed = torch.cat(embed_list, dim=0)
            embed = einops.rearrange(embed, '(b f) d -> b f d', b=B)
            if reduce_mean:
                embed = embed.mean(dim=1, keepdim=True)
            return embed  # [B, F, D] with F=1 if reduce_mean is True
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\multibanddiffusion.py
BlockTypes.METHOD, MultiBandDiffusion.sample_rate
def sample_rate(self) -> int:
        return self.codec_model.sample_rate
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_conv.py
BlockTypes.METHOD, TestNormConvTranspose1d.test_normalizations
def test_normalizations(self):
        N, C, T = 2, 2, random.randrange(1, 100_000)
        t0 = torch.randn(N, C, T)

        C_out, kernel_size, stride = 1, 4, 1
        expected_out_length = (T - 1) * stride + (kernel_size - 1) + 1

        wn_convtr = NormConvTranspose1d(C, C_out, kernel_size=kernel_size, stride=stride, norm='weight_norm')
        gn_convtr = NormConvTranspose1d(C, C_out, kernel_size=kernel_size, stride=stride, norm='time_group_norm')
        nn_convtr = NormConvTranspose1d(C, C_out, kernel_size=kernel_size, stride=stride, norm='none')

        assert isinstance(wn_convtr.norm, nn.Identity)
        assert isinstance(wn_convtr.convtr, nn.ConvTranspose1d)

        assert isinstance(gn_convtr.norm, nn.GroupNorm)
        assert isinstance(gn_convtr.convtr, nn.ConvTranspose1d)

        assert isinstance(nn_convtr.norm, nn.Identity)
        assert isinstance(nn_convtr.convtr, nn.ConvTranspose1d)

        for convtr_layer in [wn_convtr, gn_convtr, nn_convtr]:
            out = convtr_layer(t0)
            assert isinstance(out, torch.Tensor)
            assert list(out.shape) == [N, C_out, expected_out_length]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, save_audio_meta
def save_audio_meta(path: tp.Union[str, Path], meta: tp.List[AudioMeta]):
    """Save the audio metadata to the file pointer as json.

    Args:
        path (str or Path): Path to JSON file.
        metadata (list of BaseAudioMeta): List of audio meta to save.
    """
    Path(path).parent.mkdir(exist_ok=True, parents=True)
    open_fn = gzip.open if str(path).lower().endswith('.gz') else open
    with open_fn(path, 'wb') as fp:  # type: ignore
        for m in meta:
            json_str = json.dumps(m.to_dict()) + '\n'
            json_bytes = json_str.encode('utf-8')
            fp.write(json_bytes)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\info_audio_dataset.py
BlockTypes.METHOD, InfoAudioDataset.__getitem__
def __getitem__(self, index: int) -> tp.Union[torch.Tensor, tp.Tuple[torch.Tensor, SegmentWithAttributes]]:
        if not self.return_info:
            wav = super().__getitem__(index)
            assert isinstance(wav, torch.Tensor)
            return wav
        wav, meta = super().__getitem__(index)
        return wav, AudioInfo(**meta.to_dict())
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\codebooks_patterns.py
BlockTypes.METHOD, UnrolledPatternProvider.get_pattern
def get_pattern(self, timesteps: int) -> Pattern:
        """Builds pattern for delay across codebooks.

        Args:
            timesteps (int): Total number of timesteps.
        """
        # the PatternLayout is built as a tuple of sequence position and list of coordinates
        # so that it can be reordered properly given the required delay between codebooks of given timesteps
        indexed_out: list = [(-1, [])]
        max_timesteps = timesteps + self.max_delay
        for t in range(max_timesteps):
            # for each timestep, we unroll the flattened codebooks,
            # emitting the sequence step with the corresponding delay
            for step in range(self._num_inner_steps):
                if step in self._flattened_codebooks:
                    # we have codebooks at this virtual step to emit
                    step_codebooks = self._flattened_codebooks[step]
                    t_for_q = t + step_codebooks.delay
                    coords = [LayoutCoord(t, q) for q in step_codebooks.codebooks]
                    if t_for_q < max_timesteps and t < max_timesteps:
                        indexed_out.append((t_for_q, coords))
                else:
                    # there is no codebook in this virtual step so we emit an empty list
                    indexed_out.append((t, []))
        out = [coords for _, coords in sorted(indexed_out)]
        return Pattern(out, n_q=self.n_q, timesteps=timesteps)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\streaming.py
BlockTypes.METHOD, StreamingModule._set_streaming
def _set_streaming(self, streaming: bool):
        def _set_streaming(name, module):
            module._is_streaming = streaming
        self._apply_named_streaming(_set_streaming)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\losses\stftloss.py
BlockTypes.METHOD, STFTLosses.forward
def forward(self, x: torch.Tensor, y: torch.Tensor) -> tp.Tuple[torch.Tensor, torch.Tensor]:
        """Calculate forward propagation.

        Args:
            x (torch.Tensor): Predicted signal (B, T).
            y (torch.Tensor): Groundtruth signal (B, T).
        Returns:
            torch.Tensor: Spectral convergence loss value.
            torch.Tensor: Log STFT magnitude loss value.
        """
        x_mag = _stft(x, self.n_fft, self.hop_length,
                      self.win_length, self.window, self.normalized)  # type: ignore
        y_mag = _stft(y, self.n_fft, self.hop_length,
                      self.win_length, self.window, self.normalized)  # type: ignore
        sc_loss = self.spectral_convergenge_loss(x_mag, y_mag)
        mag_loss = self.log_stft_magnitude_loss(x_mag, y_mag)

        return sc_loss, mag_loss
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\unet.py
BlockTypes.METHOD, DecoderLayer.__init__
def __init__(self, chin: int, chout: int, kernel: int = 4, stride: int = 2,
                 norm_groups: int = 4, res_blocks: int = 1, activation: tp.Type[nn.Module] = nn.ReLU,
                 dropout: float = 0.):
        super().__init__()
        padding = (kernel - stride) // 2
        self.res_blocks = nn.Sequential(
            *[ResBlock(chin, norm_groups=norm_groups, dilation=2**idx, dropout=dropout)
              for idx in range(res_blocks)])
        self.norm = nn.GroupNorm(norm_groups, chin)
        ConvTr = nn.ConvTranspose1d
        self.convtr = ConvTr(chin, chout, kernel, stride, padding, bias=False)
        self.activation = activation()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ChromaStemConditioner._load_eval_wavs
def _load_eval_wavs(self, path: tp.Optional[str], num_samples: int) -> tp.Optional[torch.Tensor]:
        """Load pre-defined waveforms from a json.
        These waveforms will be used for chroma extraction during evaluation.
        This is done to make the evaluation on MusicCaps fair (we shouldn't see the chromas of MusicCaps).
        """
        if path is None:
            return None

        logger.info(f"Loading evaluation wavs from {path}")
        from audiocraft.data.audio_dataset import AudioDataset
        dataset: AudioDataset = AudioDataset.from_meta(
            path, segment_duration=self.duration, min_audio_duration=self.duration,
            sample_rate=self.sample_rate, channels=1)

        if len(dataset) > 0:
            eval_wavs = dataset.collater([dataset[i] for i in range(num_samples)]).to(self.device)
            logger.info(f"Using {len(eval_wavs)} evaluation wavs for chroma-stem conditioner")
            return eval_wavs
        else:
            raise ValueError("Could not find evaluation wavs, check lengths of wavs")
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_dataset_return_info_no_segment_duration
def test_dataset_return_info_no_segment_duration(self):
        total_examples = 10
        num_samples = 20
        min_duration, max_duration = 1., 4.
        segment_duration = None
        sample_rate = 16_000
        channels = 1
        dataset = self._create_audio_dataset(
            'dset', total_examples, durations=(min_duration, max_duration), sample_rate=sample_rate,
            channels=channels, segment_duration=segment_duration, num_examples=num_samples, return_info=True)
        assert len(dataset) == total_examples
        assert dataset.sample_rate == sample_rate
        assert dataset.channels == channels
        for idx in range(len(dataset)):
            sample, segment_info = dataset[idx]
            assert sample.shape[0] == channels
            assert sample.shape[1] == segment_info.total_frames
            assert segment_info.sample_rate == sample_rate
            assert segment_info.n_frames <= segment_info.total_frames
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\musicgen.py
BlockTypes.METHOD, MusicGen.set_custom_progress_callback
def set_custom_progress_callback(self, progress_callback: tp.Optional[tp.Callable[[int, int], None]] = None):
        """Override the default progress callback."""
        self._progress_callback = progress_callback
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\utils\checkpoint.py
BlockTypes.METHOD, checkpoint_name
def checkpoint_name(name: tp.Optional[str] = None, rank: tp.Optional[int] = None, use_fsdp: bool = False) -> str:
    """Checkpoint name formatted for all use in AudioCraft codebase and has the following format:
    `checkpoint_<name>.th(.<rank>)`. By convention, name is expected to be empty for last checkpoint,
    'best' for the best checkpoint or the epoch number.

    Args:
        name (str, optional): Name suffix for the checkpoint file stem.
        rank (optional, int): Rank for distributed processing, retrieved with flashy if not provided.
        use_fsdp (bool): Whether the calling solver relies on FSDP.
    Returns:
        str: The checkpoint name.
    """
    suffix = ''
    if rank is None:
        rank = flashy.distrib.rank()
    if rank > 0 and use_fsdp:
        suffix = '.' + str(rank)
    name_part = ''
    if name is not None:
        name_part = f'_{name}'
    return f'checkpoint{name_part}.th{suffix}'
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio_dataset.py
BlockTypes.METHOD, TestAudioDataset.test_dataset_with_meta_collate_fn
def test_dataset_with_meta_collate_fn(self, segment_duration):
        total_examples = 10
        num_samples = 20
        min_duration, max_duration = 1., 4.
        segment_duration = 1.
        sample_rate = 16_000
        channels = 1
        dataset = self._create_audio_dataset(
            'dset', total_examples, durations=(min_duration, max_duration), sample_rate=sample_rate,
            channels=channels, segment_duration=segment_duration, num_examples=num_samples, return_info=True)
        batch_size = 4
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            collate_fn=dataset.collater,
            num_workers=0
        )
        for idx, batch in enumerate(dataloader):
            wav, infos = batch
            assert wav.shape[0] == batch_size
            assert len(infos) == batch_size
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\chroma.py
BlockTypes.METHOD, ChromaExtractor.__init__
def __init__(self, sample_rate: int, n_chroma: int = 12, radix2_exp: int = 12, nfft: tp.Optional[int] = None,
                 winlen: tp.Optional[int] = None, winhop: tp.Optional[int] = None, argmax: bool = False,
                 norm: float = torch.inf):
        super().__init__()
        self.winlen = winlen or 2 ** radix2_exp
        self.nfft = nfft or self.winlen
        self.winhop = winhop or (self.winlen // 4)
        self.sample_rate = sample_rate
        self.n_chroma = n_chroma
        self.norm = norm
        self.argmax = argmax
        self.register_buffer('fbanks', torch.from_numpy(filters.chroma(sr=sample_rate, n_fft=self.nfft, tuning=0,
                                                                       n_chroma=self.n_chroma)), persistent=False)
        self.spec = torchaudio.transforms.Spectrogram(n_fft=self.nfft, win_length=self.winlen,
                                                      hop_length=self.winhop, power=2, center=True,
                                                      pad=0, normalized=True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\solvers\builders.py
BlockTypes.METHOD, get_kldiv
def get_kldiv(cfg: omegaconf.DictConfig) -> metrics.KLDivergenceMetric:
    """Instantiate KL-Divergence metric from config."""
    kld_metrics = {
        'passt': metrics.PasstKLDivergenceMetric,
    }
    klass = kld_metrics[cfg.model]
    kwargs = dict_from_config(cfg.get(cfg.model))
    return klass(**kwargs)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\compression\encodec_base_24khz.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=8, partition=partitions)
    # base causal EnCodec trained on monophonic audio sampled at 24 kHz
    launcher.bind_(solver='compression/encodec_base_24khz')
    # replace this by the desired dataset
    launcher.bind_(dset='audio/example')
    # launch xp
    launcher()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, ClassifierFreeGuidanceDropout.forward
def forward(self, samples: tp.List[ConditioningAttributes]) -> tp.List[ConditioningAttributes]:
        """
        Args:
            samples (list[ConditioningAttributes]): List of conditions.
        Returns:
            list[ConditioningAttributes]: List of conditions after all attributes were set to None.
        """
        if not self.training:
            return samples

        # decide on which attributes to drop in a batched fashion
        drop = torch.rand(1, generator=self.rng).item() < self.p
        if not drop:
            return samples

        # nullify conditions of all attributes
        samples = deepcopy(samples)
        for condition_type in ["wav", "text"]:
            for sample in samples:
                for condition in sample.attributes[condition_type]:
                    dropout_condition(sample, condition_type, condition)
        return samples
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingMultiheadAttention.__init__
def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.0, bias: bool = True,
                 causal: bool = False, past_context: tp.Optional[int] = None, custom: bool = False,
                 memory_efficient: bool = False, attention_as_float32: bool = False,
                 rope: tp.Optional[RotaryEmbedding] = None, cross_attention: bool = False,
                 safe_streaming: bool = True, qk_layer_norm: bool = False, kv_repeat: int = 1,
                 device=None, dtype=None):
        super().__init__()
        factory_kwargs = {'device': device, 'dtype': dtype}
        if past_context is not None:
            assert causal

        self.embed_dim = embed_dim
        self.causal = causal
        self.past_context = past_context
        self.memory_efficient = memory_efficient
        self.attention_as_float32 = attention_as_float32
        self.rope = rope
        self.cross_attention = cross_attention
        self.safe_streaming = safe_streaming
        self.num_heads = num_heads
        self.dropout = dropout
        self.kv_repeat = kv_repeat
        if cross_attention:
            assert not causal, "Causal cannot work with cross attention."
            assert rope is None, "Rope cannot work with cross attention."

        if memory_efficient:
            _verify_xformers_memory_efficient_compat()

        self.custom = _is_custom(custom, memory_efficient)
        if self.custom:
            out_dim = embed_dim
            assert num_heads % kv_repeat == 0
            assert not cross_attention or kv_repeat == 1
            num_kv = num_heads // kv_repeat
            kv_dim = (embed_dim // num_heads) * num_kv
            out_dim += 2 * kv_dim
            in_proj = nn.Linear(embed_dim, out_dim, bias=bias, **factory_kwargs)
            # We try to follow the default PyTorch MHA convention, to easily compare results.
            self.in_proj_weight = in_proj.weight
            self.in_proj_bias = in_proj.bias
            if bias:
                self.in_proj_bias.data.zero_()  # Following Pytorch convention
            self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias, **factory_kwargs)
            if bias:
                self.out_proj.bias.data.zero_()
        else:
            assert not qk_layer_norm
            assert kv_repeat == 1
            self.mha = nn.MultiheadAttention(
                embed_dim, num_heads, dropout=dropout, bias=bias, batch_first=True,
                **factory_kwargs)
        self.qk_layer_norm = qk_layer_norm
        if qk_layer_norm:
            assert self.custom
            assert kv_repeat == 1
            ln_dim = embed_dim
            self.q_layer_norm = nn.LayerNorm(ln_dim)
            self.k_layer_norm = nn.LayerNorm(ln_dim)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\encodec.py
BlockTypes.METHOD, DAC.__init__
def __init__(self, model_type: str = "44khz"):
        super().__init__()
        try:
            import dac.utils
        except ImportError:
            raise RuntimeError("Could not import dac, make sure it is installed, "
                               "please run `pip install descript-audio-codec`")
        self.model = dac.utils.load_model(model_type=model_type)
        self.n_quantizers = self.total_codebooks
        self.model.eval()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\seanet.py
BlockTypes.METHOD, SEANetResnetBlock.__init__
def __init__(self, dim: int, kernel_sizes: tp.List[int] = [3, 1], dilations: tp.List[int] = [1, 1],
                 activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},
                 norm: str = 'none', norm_params: tp.Dict[str, tp.Any] = {}, causal: bool = False,
                 pad_mode: str = 'reflect', compress: int = 2, true_skip: bool = True):
        super().__init__()
        assert len(kernel_sizes) == len(dilations), 'Number of kernel sizes should match number of dilations'
        act = getattr(nn, activation)
        hidden = dim // compress
        block = []
        for i, (kernel_size, dilation) in enumerate(zip(kernel_sizes, dilations)):
            in_chs = dim if i == 0 else hidden
            out_chs = dim if i == len(kernel_sizes) - 1 else hidden
            block += [
                act(**activation_params),
                StreamableConv1d(in_chs, out_chs, kernel_size=kernel_size, dilation=dilation,
                                 norm=norm, norm_kwargs=norm_params,
                                 causal=causal, pad_mode=pad_mode),
            ]
        self.block = nn.Sequential(*block)
        self.shortcut: nn.Module
        if true_skip:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = StreamableConv1d(dim, dim, kernel_size=1, norm=norm, norm_kwargs=norm_params,
                                             causal=causal, pad_mode=pad_mode)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_lstm.py
BlockTypes.METHOD, TestStreamableLSTM.test_lstm_skip
def test_lstm_skip(self):
        B, C, T = 4, 2, random.randint(1, 100)

        lstm = StreamableLSTM(C, 3, skip=True)
        x = torch.randn(B, C, T)
        y = lstm(x)

        assert y.shape == torch.Size([B, C, T])
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\music_dataset.py
BlockTypes.METHOD, augment_music_info_description
def augment_music_info_description(music_info: MusicInfo, merge_text_p: float = 0.,
                                   drop_desc_p: float = 0., drop_other_p: float = 0.) -> MusicInfo:
    """Augment MusicInfo description with additional metadata fields and potential dropout.
    Additional textual attributes are added given probability 'merge_text_conditions_p' and
    the original textual description is dropped from the augmented description given probability drop_desc_p.

    Args:
        music_info (MusicInfo): The music metadata to augment.
        merge_text_p (float): Probability of merging additional metadata to the description.
            If provided value is 0, then no merging is performed.
        drop_desc_p (float): Probability of dropping the original description on text merge.
            if provided value is 0, then no drop out is performed.
        drop_other_p (float): Probability of dropping the other fields used for text augmentation.
    Returns:
        MusicInfo: The MusicInfo with augmented textual description.
    """
    def is_valid_field(field_name: str, field_value: tp.Any) -> bool:
        valid_field_name = field_name in ['key', 'bpm', 'genre', 'moods', 'instrument', 'keywords']
        valid_field_value = field_value is not None and isinstance(field_value, (int, float, str, list))
        keep_field = random.uniform(0, 1) < drop_other_p
        return valid_field_name and valid_field_value and keep_field

    def process_value(v: tp.Any) -> str:
        if isinstance(v, (int, float, str)):
            return str(v)
        if isinstance(v, list):
            return ", ".join(v)
        else:
            raise ValueError(f"Unknown type for text value! ({type(v), v})")

    description = music_info.description

    metadata_text = ""
    if random.uniform(0, 1) < merge_text_p:
        meta_pairs = [f'{_field.name}: {process_value(getattr(music_info, _field.name))}'
                      for _field in fields(music_info) if is_valid_field(_field.name, getattr(music_info, _field.name))]
        random.shuffle(meta_pairs)
        metadata_text = ". ".join(meta_pairs)
        description = description if not random.uniform(0, 1) < drop_desc_p else None
        logger.debug(f"Applying text augmentation on MMI info. description: {description}, metadata: {metadata_text}")

    if description is None:
        description = metadata_text if len(metadata_text) > 1 else None
    else:
        description = ". ".join([description.rstrip('.'), metadata_text])
    description = description.strip() if description else None

    music_info = replace(music_info)
    music_info.description = description
    return music_info
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\models\audiogen.py
BlockTypes.METHOD, AudioGen.frame_rate
def frame_rate(self) -> float:
        """Roughly the number of AR steps per seconds."""
        return self.compression_model.frame_rate
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\conditioners.py
BlockTypes.METHOD, WaveformConditioner.__init__
def __init__(self, dim: int, output_dim: int, device: tp.Union[torch.device, str]):
        super().__init__(dim, output_dim)
        self.device = device
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\quantization\core_vq.py
BlockTypes.METHOD, sample_vectors
def sample_vectors(samples, num: int):
    num_samples, device = samples.shape[0], samples.device

    if num_samples >= num:
        indices = torch.randperm(num_samples, device=device)[:num]
    else:
        indices = torch.randint(0, num_samples, (num,), device=device)

    return samples[indices]
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\grids\musicgen\musicgen_pretrained_32khz_eval.py
BlockTypes.METHOD, explorer
def explorer(launcher):
    partitions = AudioCraftEnvironment.get_slurm_partitions(['team', 'global'])
    launcher.slurm_(gpus=4, partition=partitions)

    if 'REGEN' not in os.environ:
        folder = train.main.dora.dir / 'grids' / __name__.split('.', 2)[-1]
        with launcher.job_array():
            for sig in folder.iterdir():
                if not sig.is_symlink():
                    continue
                xp = train.main.get_xp_from_sig(sig.name)
                launcher(xp.argv)
        return

    with launcher.job_array():
        musicgen_base = launcher.bind(solver="musicgen/musicgen_base_32khz")
        musicgen_base.bind_({'autocast': False, 'fsdp.use': True})

        # base musicgen models
        musicgen_base_small = musicgen_base.bind({'continue_from': '//pretrained/facebook/musicgen-small'})
        eval(musicgen_base_small, batch_size=128)

        musicgen_base_medium = musicgen_base.bind({'continue_from': '//pretrained/facebook/musicgen-medium'})
        musicgen_base_medium.bind_({'model/lm/model_scale': 'medium'})
        eval(musicgen_base_medium, batch_size=128)

        musicgen_base_large = musicgen_base.bind({'continue_from': '//pretrained/facebook/musicgen-large'})
        musicgen_base_large.bind_({'model/lm/model_scale': 'large'})
        eval(musicgen_base_large, batch_size=128)

        # melody musicgen model
        musicgen_melody = launcher.bind(solver="musicgen/musicgen_melody_32khz")
        musicgen_melody.bind_({'autocast': False, 'fsdp.use': True})

        musicgen_melody_medium = musicgen_melody.bind({'continue_from': '//pretrained/facebook/musicgen-melody'})
        musicgen_melody_medium.bind_({'model/lm/model_scale': 'medium'})
        eval(musicgen_melody_medium, batch_size=128, eval_melody=True)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\modules\test_transformer.py
BlockTypes.METHOD, test_attention_as_float32
def test_attention_as_float32():
    torch.manual_seed(1234)
    cases = [
        {'custom': True},
        {'custom': False},
    ]
    for case in cases:
        tr = StreamingTransformer(16, 4, 2, dropout=0., dtype=torch.bfloat16, **case)
        tr_float32 = StreamingTransformer(
            16, 4, 2, dropout=0., attention_as_float32=True, dtype=torch.bfloat16, **case)
        if not case['custom']:
            # we are not using autocast here because it doesn't really
            # work as expected on CPU, so we have to manually cast the weights of the MHA.
            for layer in tr_float32.layers:
                layer.self_attn.mha.to(torch.float32)
        tr_float32.load_state_dict(tr.state_dict())
        steps = 12
        x = torch.randn(3, steps, 16, dtype=torch.bfloat16)

        with torch.no_grad():
            y = tr(x)
            y2 = tr_float32(x)
            assert not torch.allclose(y, y2), (y - y2).norm()
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\losses\test_losses.py
BlockTypes.METHOD, test_msspec_loss
def test_msspec_loss():
    N, C, T = 2, 2, random.randrange(1000, 100_000)
    t1 = torch.randn(N, C, T)
    t2 = torch.randn(N, C, T)

    msspec = MultiScaleMelSpectrogramLoss(sample_rate=22_050)
    loss = msspec(t1, t2)
    loss_same = msspec(t1, t1)

    assert isinstance(loss, torch.Tensor)
    assert isinstance(loss_same, torch.Tensor)
    assert loss_same.item() == 0.0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\data\audio_dataset.py
BlockTypes.METHOD, AudioDataset.collater
def collater(self, samples):
        """The collater function has to be provided to the dataloader
        if AudioDataset has return_info=True in order to properly collate
        the samples of a batch.
        """
        if self.segment_duration is None and len(samples) > 1:
            assert self.pad, "Must allow padding when batching examples of different durations."

        # In this case the audio reaching the collater is of variable length as segment_duration=None.
        to_pad = self.segment_duration is None and self.pad
        if to_pad:
            max_len = max([wav.shape[-1] for wav, _ in samples])

            def _pad_wav(wav):
                return F.pad(wav, (0, max_len - wav.shape[-1]))

        if self.return_info:
            if len(samples) > 0:
                assert len(samples[0]) == 2
                assert isinstance(samples[0][0], torch.Tensor)
                assert isinstance(samples[0][1], SegmentInfo)

            wavs = [wav for wav, _ in samples]
            segment_infos = [copy.deepcopy(info) for _, info in samples]

            if to_pad:
                # Each wav could be of a different duration as they are not segmented.
                for i in range(len(samples)):
                    # Determines the total length of the signal with padding, so we update here as we pad.
                    segment_infos[i].total_frames = max_len
                    wavs[i] = _pad_wav(wavs[i])

            wav = torch.stack(wavs)
            return wav, segment_infos
        else:
            assert isinstance(samples[0], torch.Tensor)
            if to_pad:
                samples = [_pad_wav(s) for s in samples]
            return torch.stack(samples)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\scripts\mos.py
BlockTypes.METHOD, login
def login():
    """Login user if not already, then redirect.
    """
    user = session.get('user')
    if user is None:
        error = None
        if request.method == 'POST':
            user = request.form['user']
            if not user:
                error = 'User cannot be empty'
        if user is None or error:
            return render_template('login.html', error=error)
    assert user
    session['user'] = user
    redirect_to = request.args.get('redirect_to')
    if redirect_to is None:
        redirect_to = url_for('index')
    return redirect(redirect_to)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\adversarial\test_losses.py
BlockTypes.METHOD, TestGeneratorAdversarialLoss.test_hinge_generator_adv_loss
def test_hinge_generator_adv_loss(self):
        adv_loss = get_adv_criterion(loss_type='hinge')

        t0 = torch.randn(1, 2, 0)
        t1 = torch.FloatTensor([1.0, 2.0, 3.0])

        assert adv_loss(t0).item() == 0.0
        assert adv_loss(t1).item() == -2.0
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\transformer.py
BlockTypes.METHOD, StreamingTransformerLayer._cross_attention_block
def _cross_attention_block(self, src: torch.Tensor,
                               cross_attention_src: torch.Tensor) -> torch.Tensor:
        assert self.cross_attention is not None
        # queries are from src, keys and values from cross_attention_src.
        x = self.cross_attention(
            src, cross_attention_src, cross_attention_src, need_weights=False)[0]
        return self.dropout_cross(x)  # type: ignore
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\tests\data\test_audio.py
BlockTypes.METHOD, TestRead.test_read_full_wav
def test_read_full_wav(self):
        sample_rates = [8000, 16_000]
        channels = [1, 2]
        duration = 1.
        for sample_rate, ch in product(sample_rates, channels):
            n_frames = int(sample_rate * duration)
            wav = get_white_noise(ch, n_frames).clamp(-0.99, 0.99)
            path = self.get_temp_path('sample_wav.wav')
            save_wav(path, wav, sample_rate)
            read_wav, read_sr = audio_read(path)
            assert read_sr == sample_rate
            assert read_wav.shape[0] == wav.shape[0]
            assert read_wav.shape[1] == wav.shape[1]
            assert torch.allclose(read_wav, wav, rtol=1e-03, atol=1e-04)
--------------------------------------------------
facebookresearch__audiocraft\coeditor_codeplan\audiocraft\modules\seanet.py
BlockTypes.METHOD, SEANetEncoder.forward
def forward(self, x):
        return self.model(x)
--------------------------------------------------
